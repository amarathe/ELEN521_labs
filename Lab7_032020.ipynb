{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbGiiytYBcQgeZg6aoCe+d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amarathe/ELEN521_labs/blob/master/Lab7_032020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP_PNTIO2wX2",
        "colab_type": "code",
        "outputId": "ec379e45-6738-4dfb-9e2e-c0df45bfb69d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "# install gensim\n",
        "!pip install gensim --upgrade\n",
        "#   conda install gensim\n",
        "\n",
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "king = wv[\"king\"]\n",
        "\n",
        "print(king.shape)\n",
        "\n",
        "print(wv.most_similar(positive=[\"king\", \"queen\"], topn=10))\n",
        "\n",
        "print(wv.similarity(\"king\", \"queen\" ))\n",
        "\n",
        "print(wv.most_similar(positive=[\"man\", \"rule\"], negative=[\"woman\"]))\n",
        "\n",
        "# What happened here?\n",
        "# \n",
        "# Do some research on how to resolve this problem. A few sites you may \n",
        "# want to look at are:\n",
        "# \n",
        "# https://cacm.acm.org/videos/2018/6\n",
        "#\n",
        "# Ricardo Baeza-Yates and Jeanna Neefe Matthews,\n",
        "# Handling Web Bias 2019: Chairs' Welcome and Workshop Summary,\n",
        "# Communications of the ACM\n",
        "# \n",
        "# https://arxiv.org/pdf/1906.08976.pdf\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.11.15)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.14.15)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim) (2.6.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.1\n",
            "[=================================================-] 98.3% 1634.5/1662.8MB downloaded\n",
            "(300,)\n",
            "[('monarch', 0.7042065858840942), ('kings', 0.6780861020088196), ('princess', 0.6731551289558411), ('queens', 0.6679497957229614), ('prince', 0.6435247659683228), ('royal', 0.5985592603683472), ('princes', 0.5942345261573792), ('crown_prince', 0.5906674265861511), ('NYC_anglophiles_aflutter', 0.5811060070991516), ('Queen_Consort', 0.5735104084014893)]\n",
            "0.6510957\n",
            "[('rules', 0.5407542586326599), ('ruled', 0.44207143783569336), ('Fifa_statutes', 0.4358542859554291), ('Rule', 0.42790597677230835), ('rulebook', 0.40312427282333374), ('Gensym_flagship_G2', 0.39585864543914795), ('unwritten_pact', 0.3956116735935211), ('edict', 0.3861566483974457), ('dictum', 0.3820774257183075), ('Bosman_Ruling', 0.37687504291534424)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZWleiNBfnuL",
        "colab_type": "text"
      },
      "source": [
        "There is bias learned by the model due to the training data.  By learning stereotypes about a group from text, the model becomes inherently biased.  This bias can be counteracted by reducing the weights that contribute to the stereotypes, by flipping the polarity of loss function for predictions marked as incorrect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNAhxzXFK-8_",
        "colab_type": "text"
      },
      "source": [
        "#7.2 IMDB dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcFvA0g9K-J5",
        "colab_type": "code",
        "outputId": "02362008-6f97-493f-a5d7-3126622cc321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras import preprocessing\n",
        "from keras.datasets import imdb\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 100\n",
        "embedded_size = 8\n",
        "\n",
        "# Load imdb dataset and print a few samples to check.\n",
        "#\n",
        "# IMDB: sentence (x) -> positive/negative (y)\n",
        "#\n",
        "# “The food was really good” \t\t\t\t -> pos\n",
        "# “The chicken crossed the road because it was uncooked” -> neg\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# x_train has a size (training_size, ). Because the sentences have variable size,\n",
        "# we cannot represent this in matrix format.\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# The first step is to make the column size constant.\n",
        "#\n",
        "# We do that by \"padding\" the sentences. If the sentences are bigger, we clip them.\n",
        "# If they are smaller, we insert a \"NO_WORD\" token to the sentence.\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "# Let's see the first sentence\n",
        "\n",
        "print(x_train[0])\n",
        "\n",
        "# Input shape should be now (training_size, maxlen)\n",
        "\n",
        "# Let's use an embedding to try to help to estimate \n",
        "\n",
        "xi = Input(x_train.shape[1:])\n",
        "\n",
        "# Embedding input is (training_size, maxlen)\n",
        "# Embedding output is (training_size, maxlen, embedded_size)\n",
        "\n",
        "x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
        "x = Flatten()(x)\n",
        "x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=xi, outputs=x)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "        x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "p = model.predict(x_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 100, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 80,801\n",
            "Trainable params: 80,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "20000/20000 [==============================] - 11s 546us/step - loss: 0.5749 - acc: 0.7171 - val_loss: 0.4079 - val_acc: 0.8248\n",
            "Epoch 2/20\n",
            "20000/20000 [==============================] - 2s 106us/step - loss: 0.3156 - acc: 0.8742 - val_loss: 0.3341 - val_acc: 0.8544\n",
            "Epoch 3/20\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.2330 - acc: 0.9134 - val_loss: 0.3311 - val_acc: 0.8534\n",
            "Epoch 4/20\n",
            "20000/20000 [==============================] - 2s 109us/step - loss: 0.1794 - acc: 0.9387 - val_loss: 0.3368 - val_acc: 0.8530\n",
            "Epoch 5/20\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.1356 - acc: 0.9613 - val_loss: 0.3587 - val_acc: 0.8456\n",
            "Epoch 6/20\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.0996 - acc: 0.9766 - val_loss: 0.3760 - val_acc: 0.8462\n",
            "Epoch 7/20\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.0706 - acc: 0.9869 - val_loss: 0.3981 - val_acc: 0.8456\n",
            "Epoch 8/20\n",
            "20000/20000 [==============================] - 2s 109us/step - loss: 0.0485 - acc: 0.9937 - val_loss: 0.4260 - val_acc: 0.8410\n",
            "Epoch 9/20\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.0328 - acc: 0.9981 - val_loss: 0.4546 - val_acc: 0.8396\n",
            "Epoch 10/20\n",
            "20000/20000 [==============================] - 2s 108us/step - loss: 0.0220 - acc: 0.9991 - val_loss: 0.4822 - val_acc: 0.8390\n",
            "Epoch 11/20\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.0148 - acc: 0.9997 - val_loss: 0.5120 - val_acc: 0.8370\n",
            "Epoch 12/20\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.0101 - acc: 0.9998 - val_loss: 0.5415 - val_acc: 0.8346\n",
            "Epoch 13/20\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.0068 - acc: 0.9999 - val_loss: 0.5698 - val_acc: 0.8340\n",
            "Epoch 14/20\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.5994 - val_acc: 0.8338\n",
            "Epoch 15/20\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.6250 - val_acc: 0.8350\n",
            "Epoch 16/20\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.6556 - val_acc: 0.8336\n",
            "Epoch 17/20\n",
            "20000/20000 [==============================] - 2s 107us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.6814 - val_acc: 0.8332\n",
            "Epoch 18/20\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7089 - val_acc: 0.8316\n",
            "Epoch 19/20\n",
            "20000/20000 [==============================] - 2s 104us/step - loss: 7.7047e-04 - acc: 1.0000 - val_loss: 0.7365 - val_acc: 0.8312\n",
            "Epoch 20/20\n",
            "20000/20000 [==============================] - 2s 111us/step - loss: 5.4646e-04 - acc: 1.0000 - val_loss: 0.7668 - val_acc: 0.8294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcEX5EX4k9wm",
        "colab_type": "text"
      },
      "source": [
        "Print out the first sentence in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b0mF0gxk6yf",
        "colab_type": "code",
        "outputId": "95f86414-0cc7-4c3d-9b98-111d1b2cf0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# From https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
        "NUM_WORDS=1000 # only use top 1000 words\n",
        "INDEX_FROM=3   # word index offset\n",
        "\n",
        "train,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
        "train_x,train_y = train\n",
        "test_x,test_y = test\n",
        "\n",
        "word_to_id = keras.datasets.imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "word_to_id[\"<UNUSED>\"] = 3\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in train_x[0] ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "<START> this film was just brilliant casting <UNK> <UNK> story direction <UNK> really <UNK> the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> <UNK> as myself so i loved the fact there was a real <UNK> with this film the <UNK> <UNK> throughout the film were great it was just brilliant so much that i <UNK> the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the <UNK> <UNK> was amazing really <UNK> at the end it was so sad and you know what they say if you <UNK> at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of <UNK> and paul they were just brilliant children are often left out of the <UNK> <UNK> i think because the stars that play them all <UNK> up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so <UNK> because it was true and was <UNK> life after all that was <UNK> with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eCVmZuzLi6s",
        "colab_type": "code",
        "outputId": "1b8849d0-991b-43d2-9b01-e7addbb3e22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#\n",
        "# What's the current accuracy for this model?\n",
        "# \n",
        "print (\"Loss: %f, Accuracy: %f\" % tuple(model.evaluate(x_test,y_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 41us/step\n",
            "Loss: 0.765734, Accuracy: 0.829680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CjZJMwFMq_t",
        "colab_type": "code",
        "outputId": "1058f71b-6d01-452b-ed9b-053cabdd1eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Try to add a preloaded embedded from Glove from this model, see the\n",
        "# suggestion in\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "# \n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip\n",
        "!ls\n",
        "!pwd\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# Also try Conv1D + MaxPooling1D to improve results\n",
        "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.htm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-19 22:25:17--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-03-19 22:25:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-03-19 22:25:18--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.05MB/s    in 6m 30s  \n",
            "\n",
            "2020-03-19 22:31:49 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n",
            "/content\n",
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI8p8yX61K2i",
        "colab_type": "text"
      },
      "source": [
        "# Question: why does this model may help you get better accuracy?\n",
        "\n",
        "\n",
        "This model should give better accuracy because the embeddings have been pre-trained with the relationships between words.  So the IMDB predictor can leverage the relationships between words during training the final layer (trainable) of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yWP4uc6nfvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88zdx3Ui3Ppx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "def runembedding(printme=False, embedded_size=8, maxlen=100):\n",
        "  print (\"DEBUG: maxlen\", maxlen)\n",
        "\n",
        "  embedding_matrix = np.zeros((len(word_to_id) + 1, maxlen))\n",
        "  for word, i in word_to_id.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "      print (\"Embedding_vector:\", embedding_vector)\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  max_features = 10000\n",
        "  embedded_size = 8\n",
        "\n",
        "  # Load imdb dataset and print a few samples to check.\n",
        "  #\n",
        "  # IMDB: sentence (x) -> positive/negative (y)\n",
        "  #\n",
        "  # “The food was really good” \t\t\t\t -> pos\n",
        "  # “The chicken crossed the road because it was uncooked” -> neg\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "  # x_train has a size (training_size, ). Because the sentences have variable size,\n",
        "  # we cannot represent this in matrix format.\n",
        "\n",
        "  print(\"Initial shape x_train:\", x_train.shape)\n",
        "\n",
        "  # The first step is to make the column size constant.\n",
        "  #\n",
        "  # We do that by \"padding\" the sentences. If the sentences are bigger, we clip them.\n",
        "  # If they are smaller, we insert a \"NO_WORD\" token to the sentence.\n",
        "\n",
        "  x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "  x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "  print(\"x_train shape:\", x_train.shape)\n",
        "\n",
        "  # Let's see the first sentence\n",
        "\n",
        "  print(x_train[0])\n",
        "\n",
        "  # Input shape should be now (training_size, maxlen)\n",
        "\n",
        "  # Let's use an embedding to try to help to estimate \n",
        "\n",
        "  xi = Input(x_train.shape[1:])\n",
        "\n",
        "  # Embedding input is (training_size, maxlen)\n",
        "  # Embedding output is (training_size, maxlen, embedded_size)\n",
        "\n",
        "  x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  model = Model(inputs=xi, outputs=x)\n",
        "\n",
        "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "        x_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
        "\n",
        "  p = model.predict(x_test)\n",
        "\n",
        "#print (\"len(word_to_id)\", len(word_to_id))\n",
        "#print (\"maxlen\", maxlen)\n",
        "  #print (\"x_train.shape:\", x_train.shape)\n",
        "  xi = Input(x_train.shape[1:])\n",
        "\n",
        "\n",
        "  embedding_layer = Embedding(max_features,\n",
        "                            embedded_size,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=maxlen,\n",
        "                            trainable=False)\n",
        "\n",
        "  x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "#print (\"sequence input\")\n",
        "#sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
        "#embedded_sequences = embedding_layer(sequence_input)\n",
        "#print (\"embedded done\")\n",
        "#x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
        "#print (\"conv1 done\")\n",
        "#x = MaxPooling1D(5)(x)\n",
        "#x = Conv1D(128, 5, activation='relu')(x)\n",
        "#x = MaxPooling1D(5)(x)\n",
        "#x = Conv1D(128, 5, activation='relu')(x)\n",
        "#x = MaxPooling1D(35)(x)  # global max pooling\n",
        "#x = Flatten()(x)\n",
        "#x = Dense(128, activation='relu')(x)\n",
        "#print (\"dense\")\n",
        "#preds = Dense(len(labels_index), activation='softmax')(x)\n",
        "\n",
        "#print (\"building model\")\n",
        "  model = Model(xi, x)\n",
        "#print (\"model compile\")\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "  if printme: model.summary()\n",
        "\n",
        "# happy learning!\n",
        "#model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
        "#          epochs=2, batch_size=128)\n",
        "\n",
        "  history = model.fit(\n",
        "        x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "  \n",
        "  res = model.evaluate(x_test,y_test)\n",
        "  if printme: print (\"Loss: %f, Accuracy: %f\" % tuple(res))\n",
        "  return res[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xc4StZLT3w_k",
        "colab_type": "code",
        "outputId": "65bcd654-2327-4701-8aeb-dabefd2a6363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "runembedding(True);\n",
        "#print (\"Loss: %f, Accuracy: %f\" % tuple(model.evaluate(x_test,y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 100, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 80,801\n",
            "Trainable params: 80,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "20000/20000 [==============================] - 2s 124us/step - loss: 0.6100 - acc: 0.6871 - val_loss: 0.4485 - val_acc: 0.8120\n",
            "Epoch 2/20\n",
            "20000/20000 [==============================] - 2s 107us/step - loss: 0.3401 - acc: 0.8674 - val_loss: 0.3448 - val_acc: 0.8492\n",
            "Epoch 3/20\n",
            "20000/20000 [==============================] - 2s 111us/step - loss: 0.2356 - acc: 0.9154 - val_loss: 0.3279 - val_acc: 0.8528\n",
            "Epoch 4/20\n",
            "20000/20000 [==============================] - 2s 109us/step - loss: 0.1701 - acc: 0.9473 - val_loss: 0.3333 - val_acc: 0.8500\n",
            "Epoch 5/20\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.1199 - acc: 0.9705 - val_loss: 0.3484 - val_acc: 0.8442\n",
            "Epoch 6/20\n",
            "20000/20000 [==============================] - 2s 101us/step - loss: 0.0823 - acc: 0.9844 - val_loss: 0.3687 - val_acc: 0.8456\n",
            "Epoch 7/20\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.0550 - acc: 0.9933 - val_loss: 0.3921 - val_acc: 0.8434\n",
            "Epoch 8/20\n",
            "20000/20000 [==============================] - 2s 111us/step - loss: 0.0362 - acc: 0.9971 - val_loss: 0.4172 - val_acc: 0.8408\n",
            "Epoch 9/20\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.0237 - acc: 0.9990 - val_loss: 0.4441 - val_acc: 0.8400\n",
            "Epoch 10/20\n",
            "20000/20000 [==============================] - 2s 105us/step - loss: 0.0157 - acc: 0.9996 - val_loss: 0.4735 - val_acc: 0.8380\n",
            "Epoch 11/20\n",
            "20000/20000 [==============================] - 2s 106us/step - loss: 0.0104 - acc: 0.9999 - val_loss: 0.5009 - val_acc: 0.8384\n",
            "Epoch 12/20\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.5269 - val_acc: 0.8364\n",
            "Epoch 13/20\n",
            "20000/20000 [==============================] - 2s 108us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.5561 - val_acc: 0.8370\n",
            "Epoch 14/20\n",
            "20000/20000 [==============================] - 2s 106us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.5850 - val_acc: 0.8364\n",
            "Epoch 15/20\n",
            "20000/20000 [==============================] - 2s 111us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.6103 - val_acc: 0.8358\n",
            "Epoch 16/20\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.6376 - val_acc: 0.8358\n",
            "Epoch 17/20\n",
            "20000/20000 [==============================] - 2s 104us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6651 - val_acc: 0.8342\n",
            "Epoch 18/20\n",
            "20000/20000 [==============================] - 2s 109us/step - loss: 7.8167e-04 - acc: 1.0000 - val_loss: 0.6924 - val_acc: 0.8334\n",
            "Epoch 19/20\n",
            "20000/20000 [==============================] - 2s 105us/step - loss: 5.5390e-04 - acc: 1.0000 - val_loss: 0.7223 - val_acc: 0.8334\n",
            "Epoch 20/20\n",
            "20000/20000 [==============================] - 2s 109us/step - loss: 3.9106e-04 - acc: 1.0000 - val_loss: 0.7456 - val_acc: 0.8326\n",
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 100, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 80,801\n",
            "Trainable params: 80,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.6168 - acc: 0.6648 - val_loss: 0.4373 - val_acc: 0.8184\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3318 - acc: 0.8688 - val_loss: 0.3374 - val_acc: 0.8514\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 106us/step - loss: 0.2349 - acc: 0.9129 - val_loss: 0.3299 - val_acc: 0.8538\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 105us/step - loss: 0.1773 - acc: 0.9414 - val_loss: 0.3381 - val_acc: 0.8536\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.1317 - acc: 0.9634 - val_loss: 0.3545 - val_acc: 0.8498\n",
            "25000/25000 [==============================] - 1s 44us/step\n",
            "Loss: 0.352226, Accuracy: 0.849960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo2WbNyjACjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Glove has fixed maxlen=100. To sweep maxlength run without glove\n",
        "def runembeddingNOGLOVE(printme=False, embedded_size=8, maxlen=100):\n",
        "\n",
        "  max_features = 10000\n",
        "\n",
        "  # Load imdb dataset and print a few samples to check.\n",
        "  #\n",
        "  #IMDB: sentence (x) -> positive/negative (y)\n",
        "  #\n",
        "  # “The food was really good” \t\t\t\t -> pos\n",
        "  # “The chicken crossed the road because it was uncooked” -> neg\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "  # x_train has a size (training_size, ). Because the sentences have variable size,\n",
        "  # we cannot represent this in matrix format.\n",
        "\n",
        "  print(x_train.shape)\n",
        "\n",
        "  # The first step is to make the column size constant.\n",
        "  #\n",
        "  # We do that by \"padding\" the sentences. If the sentences are bigger, we clip them.\n",
        "  # If they are smaller, we insert a \"NO_WORD\" token to the sentence.\n",
        "\n",
        "  x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "  x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "  print(x_train.shape)\n",
        "\n",
        "  # Let's see the first sentence\n",
        "\n",
        "  print(x_train[0])\n",
        "\n",
        "  # Input shape should be now (training_size, maxlen)\n",
        "\n",
        "  # Let's use an embedding to try to help to estimate \n",
        "\n",
        "  xi = Input(x_train.shape[1:])\n",
        "\n",
        "  # Embedding input is (training_size, maxlen)\n",
        "  # Embedding output is (training_size, maxlen, embedded_size)\n",
        "\n",
        "  x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  model = Model(inputs=xi, outputs=x)\n",
        "\n",
        "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "        x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "  p = model.predict(x_test)\n",
        "  res = model.evaluate(x_test,y_test)\n",
        "\n",
        "  return res[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXHfGa2YWBg2",
        "colab_type": "code",
        "outputId": "52bbc5e8-a05e-4792-b6a9-10948e8bb4a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Try to change the maxlen or the embedded_size and plot a 3D graph with\n",
        "# embedded_size x maxlen x accuracy in a python jupyter notebook.\n",
        "\n",
        "#NEED TO CHANGE INPUT LENGTH EVEN BEFORE MODEL\n",
        "\n",
        "%matplotlib notebook\n",
        "import numpy as np\n",
        "\n",
        "embedded_size = [2, 4, 8, 12, 16]\n",
        "maxlen = [25, 50, 100, 150, 200]\n",
        "\n",
        "#print (x)\n",
        "\n",
        "X, Y = np.meshgrid(embedded_size, maxlen)\n",
        "Z = np.array((len(embedded_size), len(maxlen)))\n",
        "Z = [[0 for _ in range(len(maxlen))] for y in range(len(embedded_size))]\n",
        "#Z[0][0] = 1\n",
        "for i, e in enumerate(embedded_size):\n",
        "  for j, m in enumerate(maxlen):\n",
        "    Z[i][j] = runembeddingNOGLOVE(False, e, m)\n",
        "\n",
        "#Z = f(X, Y)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
        "ax.set_xlabel('embedded_size')\n",
        "ax.set_ylabel('maxlen')\n",
        "ax.set_zlabel('z');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000, 25)\n",
            "[  92   25  104    4  226   65   16   38 1334   88   12   16  283    5\n",
            "   16 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "embedding_9 (Embedding)      (None, 25, 2)             20000     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 20,051\n",
            "Trainable params: 20,051\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 134us/step - loss: 0.6780 - acc: 0.5966 - val_loss: 0.6411 - val_acc: 0.6820\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.5583 - acc: 0.7534 - val_loss: 0.5288 - val_acc: 0.7348\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.4515 - acc: 0.8069 - val_loss: 0.4895 - val_acc: 0.7538\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 121us/step - loss: 0.3951 - acc: 0.8346 - val_loss: 0.4770 - val_acc: 0.7642\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.3577 - acc: 0.8538 - val_loss: 0.4750 - val_acc: 0.7646\n",
            "25000/25000 [==============================] - 1s 43us/step\n",
            "(25000,)\n",
            "(25000, 50)\n",
            "[2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, 50, 2)             20000     \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 20,101\n",
            "Trainable params: 20,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 134us/step - loss: 0.6831 - acc: 0.5768 - val_loss: 0.6433 - val_acc: 0.7172\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.5326 - acc: 0.7853 - val_loss: 0.4789 - val_acc: 0.7786\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.4014 - acc: 0.8345 - val_loss: 0.4302 - val_acc: 0.7952\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.3407 - acc: 0.8624 - val_loss: 0.4132 - val_acc: 0.8044\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 111us/step - loss: 0.3007 - acc: 0.8803 - val_loss: 0.4090 - val_acc: 0.8092\n",
            "25000/25000 [==============================] - 1s 43us/step\n",
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_11 (Embedding)     (None, 100, 2)            20000     \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 20,201\n",
            "Trainable params: 20,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 144us/step - loss: 0.6591 - acc: 0.6377 - val_loss: 0.5528 - val_acc: 0.7830\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.4278 - acc: 0.8384 - val_loss: 0.3846 - val_acc: 0.8326\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3114 - acc: 0.8810 - val_loss: 0.3450 - val_acc: 0.8468\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.2603 - acc: 0.9011 - val_loss: 0.3327 - val_acc: 0.8516\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 121us/step - loss: 0.2253 - acc: 0.9182 - val_loss: 0.3321 - val_acc: 0.8514\n",
            "25000/25000 [==============================] - 1s 44us/step\n",
            "(25000,)\n",
            "(25000, 150)\n",
            "[  12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_12 (Embedding)     (None, 150, 2)            20000     \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 20,301\n",
            "Trainable params: 20,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 145us/step - loss: 0.6279 - acc: 0.6683 - val_loss: 0.4698 - val_acc: 0.8172\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.3560 - acc: 0.8632 - val_loss: 0.3340 - val_acc: 0.8610\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.2623 - acc: 0.8994 - val_loss: 0.3060 - val_acc: 0.8662\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.2189 - acc: 0.9194 - val_loss: 0.2986 - val_acc: 0.8710\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.1885 - acc: 0.9330 - val_loss: 0.2991 - val_acc: 0.8720\n",
            "25000/25000 [==============================] - 1s 43us/step\n",
            "(25000,)\n",
            "(25000, 200)\n",
            "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
            "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
            "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
            "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
            "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
            "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
            " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
            "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
            "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
            "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
            " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
            "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
            "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
            "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
            " 5345   19  178   32]\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_13 (Embedding)     (None, 200, 2)            20000     \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 20,401\n",
            "Trainable params: 20,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 147us/step - loss: 0.6485 - acc: 0.6320 - val_loss: 0.4923 - val_acc: 0.8068\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.3622 - acc: 0.8654 - val_loss: 0.3238 - val_acc: 0.8728\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.2563 - acc: 0.9035 - val_loss: 0.2935 - val_acc: 0.8770\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.2109 - acc: 0.9252 - val_loss: 0.2832 - val_acc: 0.8830\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.1801 - acc: 0.9381 - val_loss: 0.2824 - val_acc: 0.8816\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 25)\n",
            "[  92   25  104    4  226   65   16   38 1334   88   12   16  283    5\n",
            "   16 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "embedding_14 (Embedding)     (None, 25, 4)             40000     \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 40,101\n",
            "Trainable params: 40,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 147us/step - loss: 0.6541 - acc: 0.6587 - val_loss: 0.5811 - val_acc: 0.7242\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.4917 - acc: 0.7890 - val_loss: 0.4941 - val_acc: 0.7564\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.3987 - acc: 0.8321 - val_loss: 0.4777 - val_acc: 0.7618\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3445 - acc: 0.8619 - val_loss: 0.4761 - val_acc: 0.7638\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3025 - acc: 0.8844 - val_loss: 0.4872 - val_acc: 0.7610\n",
            "25000/25000 [==============================] - 1s 44us/step\n",
            "(25000,)\n",
            "(25000, 50)\n",
            "[2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_15 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_15 (Embedding)     (None, 50, 4)             40000     \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 40,201\n",
            "Trainable params: 40,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 153us/step - loss: 0.6588 - acc: 0.6334 - val_loss: 0.5639 - val_acc: 0.7542\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 121us/step - loss: 0.4515 - acc: 0.8146 - val_loss: 0.4313 - val_acc: 0.8014\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3418 - acc: 0.8623 - val_loss: 0.4064 - val_acc: 0.8066\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 124us/step - loss: 0.2857 - acc: 0.8900 - val_loss: 0.4060 - val_acc: 0.8086\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.2445 - acc: 0.9092 - val_loss: 0.4148 - val_acc: 0.8054\n",
            "25000/25000 [==============================] - 1s 46us/step\n",
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "Model: \"model_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_16 (InputLayer)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_16 (Embedding)     (None, 100, 4)            40000     \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 40,401\n",
            "Trainable params: 40,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 147us/step - loss: 0.6381 - acc: 0.6522 - val_loss: 0.4911 - val_acc: 0.7996\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.3745 - acc: 0.8537 - val_loss: 0.3538 - val_acc: 0.8476\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.2726 - acc: 0.8946 - val_loss: 0.3298 - val_acc: 0.8536\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 106us/step - loss: 0.2208 - acc: 0.9203 - val_loss: 0.3313 - val_acc: 0.8522\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.1833 - acc: 0.9370 - val_loss: 0.3348 - val_acc: 0.8540\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 150)\n",
            "[  12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_17 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_17 (Embedding)     (None, 150, 4)            40000     \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 601       \n",
            "=================================================================\n",
            "Total params: 40,601\n",
            "Trainable params: 40,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 159us/step - loss: 0.6146 - acc: 0.6842 - val_loss: 0.4343 - val_acc: 0.8294\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.3317 - acc: 0.8732 - val_loss: 0.3247 - val_acc: 0.8594\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 3s 127us/step - loss: 0.2394 - acc: 0.9116 - val_loss: 0.3029 - val_acc: 0.8672\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.1892 - acc: 0.9338 - val_loss: 0.2977 - val_acc: 0.8714\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.1518 - acc: 0.9520 - val_loss: 0.3051 - val_acc: 0.8674\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 200)\n",
            "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
            "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
            "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
            "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
            "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
            "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
            " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
            "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
            "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
            "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
            " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
            "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
            "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
            "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
            " 5345   19  178   32]\n",
            "Model: \"model_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_18 (InputLayer)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_18 (Embedding)     (None, 200, 4)            40000     \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 40,801\n",
            "Trainable params: 40,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 150us/step - loss: 0.6042 - acc: 0.6709 - val_loss: 0.4051 - val_acc: 0.8438\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.3090 - acc: 0.8806 - val_loss: 0.3029 - val_acc: 0.8776\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.2228 - acc: 0.9177 - val_loss: 0.2862 - val_acc: 0.8806\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.1779 - acc: 0.9375 - val_loss: 0.2810 - val_acc: 0.8836\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.1459 - acc: 0.9525 - val_loss: 0.2876 - val_acc: 0.8828\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 25)\n",
            "[  92   25  104    4  226   65   16   38 1334   88   12   16  283    5\n",
            "   16 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_19 (InputLayer)        (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "embedding_19 (Embedding)     (None, 25, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 80,201\n",
            "Trainable params: 80,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 156us/step - loss: 0.6490 - acc: 0.6519 - val_loss: 0.5583 - val_acc: 0.7340\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.4604 - acc: 0.8012 - val_loss: 0.4815 - val_acc: 0.7618\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.3597 - acc: 0.8556 - val_loss: 0.4730 - val_acc: 0.7652\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.2917 - acc: 0.8917 - val_loss: 0.4867 - val_acc: 0.7620\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.2371 - acc: 0.9195 - val_loss: 0.5062 - val_acc: 0.7554\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 50)\n",
            "[2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_20 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_20 (Embedding)     (None, 50, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 80,401\n",
            "Trainable params: 80,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 154us/step - loss: 0.6235 - acc: 0.6727 - val_loss: 0.4859 - val_acc: 0.7802\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.3915 - acc: 0.8342 - val_loss: 0.4086 - val_acc: 0.8060\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 123us/step - loss: 0.3003 - acc: 0.8804 - val_loss: 0.4035 - val_acc: 0.8062\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.2420 - acc: 0.9100 - val_loss: 0.4138 - val_acc: 0.8092\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.1939 - acc: 0.9369 - val_loss: 0.4345 - val_acc: 0.8076\n",
            "25000/25000 [==============================] - 1s 44us/step\n",
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "Model: \"model_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_21 (InputLayer)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_21 (Embedding)     (None, 100, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 80,801\n",
            "Trainable params: 80,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 164us/step - loss: 0.5741 - acc: 0.7126 - val_loss: 0.3950 - val_acc: 0.8358\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 124us/step - loss: 0.3131 - acc: 0.8739 - val_loss: 0.3303 - val_acc: 0.8562\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.2338 - acc: 0.9127 - val_loss: 0.3262 - val_acc: 0.8514\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.1834 - acc: 0.9360 - val_loss: 0.3357 - val_acc: 0.8504\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.1423 - acc: 0.9563 - val_loss: 0.3540 - val_acc: 0.8490\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 150)\n",
            "[  12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_22 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_22 (Embedding)     (None, 150, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1)                 1201      \n",
            "=================================================================\n",
            "Total params: 81,201\n",
            "Trainable params: 81,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 166us/step - loss: 0.5991 - acc: 0.6815 - val_loss: 0.3958 - val_acc: 0.8436\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.2976 - acc: 0.8848 - val_loss: 0.3241 - val_acc: 0.8612\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.2041 - acc: 0.9280 - val_loss: 0.3001 - val_acc: 0.8698\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.1457 - acc: 0.9555 - val_loss: 0.3048 - val_acc: 0.8666\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.1015 - acc: 0.9743 - val_loss: 0.3212 - val_acc: 0.8636\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 200)\n",
            "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
            "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
            "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
            "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
            "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
            "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
            " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
            "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
            "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
            "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
            " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
            "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
            "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
            "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
            " 5345   19  178   32]\n",
            "Model: \"model_22\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_23 (InputLayer)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_23 (Embedding)     (None, 200, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 1601      \n",
            "=================================================================\n",
            "Total params: 81,601\n",
            "Trainable params: 81,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 174us/step - loss: 0.5704 - acc: 0.7004 - val_loss: 0.3614 - val_acc: 0.8570\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.2750 - acc: 0.8939 - val_loss: 0.2882 - val_acc: 0.8844\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.1916 - acc: 0.9324 - val_loss: 0.2787 - val_acc: 0.8862\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 114us/step - loss: 0.1410 - acc: 0.9560 - val_loss: 0.2876 - val_acc: 0.8848\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.1016 - acc: 0.9731 - val_loss: 0.3115 - val_acc: 0.8816\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 25)\n",
            "[  92   25  104    4  226   65   16   38 1334   88   12   16  283    5\n",
            "   16 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_24 (InputLayer)        (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "embedding_24 (Embedding)     (None, 25, 12)            120000    \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 301       \n",
            "=================================================================\n",
            "Total params: 120,301\n",
            "Trainable params: 120,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 192us/step - loss: 0.6406 - acc: 0.6524 - val_loss: 0.5419 - val_acc: 0.7372\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.4436 - acc: 0.8083 - val_loss: 0.4767 - val_acc: 0.7624\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 3s 125us/step - loss: 0.3357 - acc: 0.8687 - val_loss: 0.4783 - val_acc: 0.7674\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 112us/step - loss: 0.2551 - acc: 0.9113 - val_loss: 0.4950 - val_acc: 0.7652\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.1894 - acc: 0.9447 - val_loss: 0.5244 - val_acc: 0.7584\n",
            "25000/25000 [==============================] - 1s 45us/step\n",
            "(25000,)\n",
            "(25000, 50)\n",
            "[2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_25 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_25 (Embedding)     (None, 50, 12)            120000    \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 600)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1)                 601       \n",
            "=================================================================\n",
            "Total params: 120,601\n",
            "Trainable params: 120,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 3s 172us/step - loss: 0.6026 - acc: 0.6892 - val_loss: 0.4629 - val_acc: 0.7864\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3722 - acc: 0.8428 - val_loss: 0.4037 - val_acc: 0.8080\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 121us/step - loss: 0.2785 - acc: 0.8941 - val_loss: 0.4035 - val_acc: 0.8106\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.2102 - acc: 0.9292 - val_loss: 0.4221 - val_acc: 0.8092\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.1516 - acc: 0.9579 - val_loss: 0.4497 - val_acc: 0.8022\n",
            "25000/25000 [==============================] - 1s 47us/step\n",
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "Model: \"model_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_26 (InputLayer)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_26 (Embedding)     (None, 100, 12)           120000    \n",
            "_________________________________________________________________\n",
            "flatten_25 (Flatten)         (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 1201      \n",
            "=================================================================\n",
            "Total params: 121,201\n",
            "Trainable params: 121,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 183us/step - loss: 0.5730 - acc: 0.7118 - val_loss: 0.3938 - val_acc: 0.8336\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.3040 - acc: 0.8780 - val_loss: 0.3320 - val_acc: 0.8524\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.2120 - acc: 0.9250 - val_loss: 0.3302 - val_acc: 0.8554\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.1472 - acc: 0.9566 - val_loss: 0.3546 - val_acc: 0.8446\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.0964 - acc: 0.9797 - val_loss: 0.3660 - val_acc: 0.8470\n",
            "25000/25000 [==============================] - 1s 49us/step\n",
            "(25000,)\n",
            "(25000, 150)\n",
            "[  12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_27 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_27 (Embedding)     (None, 150, 12)           120000    \n",
            "_________________________________________________________________\n",
            "flatten_26 (Flatten)         (None, 1800)              0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1)                 1801      \n",
            "=================================================================\n",
            "Total params: 121,801\n",
            "Trainable params: 121,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 180us/step - loss: 0.5345 - acc: 0.7338 - val_loss: 0.3462 - val_acc: 0.8562\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 123us/step - loss: 0.2630 - acc: 0.8976 - val_loss: 0.2993 - val_acc: 0.8672\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.1829 - acc: 0.9354 - val_loss: 0.3009 - val_acc: 0.8696\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.1277 - acc: 0.9624 - val_loss: 0.3202 - val_acc: 0.8672\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.0853 - acc: 0.9804 - val_loss: 0.3546 - val_acc: 0.8606\n",
            "25000/25000 [==============================] - 1s 48us/step\n",
            "(25000,)\n",
            "(25000, 200)\n",
            "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
            "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
            "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
            "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
            "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
            "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
            " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
            "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
            "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
            "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
            " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
            "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
            "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
            "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
            " 5345   19  178   32]\n",
            "Model: \"model_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_28 (InputLayer)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_28 (Embedding)     (None, 200, 12)           120000    \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 2400)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 1)                 2401      \n",
            "=================================================================\n",
            "Total params: 122,401\n",
            "Trainable params: 122,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 179us/step - loss: 0.5332 - acc: 0.7349 - val_loss: 0.3414 - val_acc: 0.8630\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 113us/step - loss: 0.2577 - acc: 0.9012 - val_loss: 0.2867 - val_acc: 0.8796\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 120us/step - loss: 0.1689 - acc: 0.9431 - val_loss: 0.2878 - val_acc: 0.8794\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 110us/step - loss: 0.1081 - acc: 0.9714 - val_loss: 0.2918 - val_acc: 0.8780\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 108us/step - loss: 0.0654 - acc: 0.9881 - val_loss: 0.3078 - val_acc: 0.8810\n",
            "25000/25000 [==============================] - 1s 48us/step\n",
            "(25000,)\n",
            "(25000, 25)\n",
            "[  92   25  104    4  226   65   16   38 1334   88   12   16  283    5\n",
            "   16 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_29 (InputLayer)        (None, 25)                0         \n",
            "_________________________________________________________________\n",
            "embedding_29 (Embedding)     (None, 25, 16)            160000    \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 160,401\n",
            "Trainable params: 160,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 189us/step - loss: 0.6266 - acc: 0.6646 - val_loss: 0.5216 - val_acc: 0.7444\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.4259 - acc: 0.8138 - val_loss: 0.4764 - val_acc: 0.7682\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 121us/step - loss: 0.3229 - acc: 0.8760 - val_loss: 0.4782 - val_acc: 0.7688\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.2410 - acc: 0.9175 - val_loss: 0.5044 - val_acc: 0.7622\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.1722 - acc: 0.9528 - val_loss: 0.5316 - val_acc: 0.7600\n",
            "25000/25000 [==============================] - 1s 48us/step\n",
            "(25000,)\n",
            "(25000, 50)\n",
            "[2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
            "  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_29\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_30 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_30 (Embedding)     (None, 50, 16)            160000    \n",
            "_________________________________________________________________\n",
            "flatten_29 (Flatten)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 801       \n",
            "=================================================================\n",
            "Total params: 160,801\n",
            "Trainable params: 160,801\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 193us/step - loss: 0.5999 - acc: 0.6825 - val_loss: 0.4541 - val_acc: 0.7928\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 3s 126us/step - loss: 0.3597 - acc: 0.8484 - val_loss: 0.3999 - val_acc: 0.8096\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.2548 - acc: 0.9060 - val_loss: 0.4073 - val_acc: 0.8112\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 3s 126us/step - loss: 0.1753 - acc: 0.9479 - val_loss: 0.4344 - val_acc: 0.8044\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.1132 - acc: 0.9757 - val_loss: 0.4687 - val_acc: 0.8002\n",
            "25000/25000 [==============================] - 1s 47us/step\n",
            "(25000,)\n",
            "(25000, 100)\n",
            "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
            "  178   32]\n",
            "Model: \"model_30\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_31 (InputLayer)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_31 (Embedding)     (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten_30 (Flatten)         (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 1601      \n",
            "=================================================================\n",
            "Total params: 161,601\n",
            "Trainable params: 161,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 198us/step - loss: 0.5672 - acc: 0.7138 - val_loss: 0.3843 - val_acc: 0.8340\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 116us/step - loss: 0.2929 - acc: 0.8845 - val_loss: 0.3326 - val_acc: 0.8552\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.1958 - acc: 0.9315 - val_loss: 0.3365 - val_acc: 0.8470\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 111us/step - loss: 0.1242 - acc: 0.9675 - val_loss: 0.3557 - val_acc: 0.8476\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.0723 - acc: 0.9877 - val_loss: 0.3825 - val_acc: 0.8410\n",
            "25000/25000 [==============================] - 1s 48us/step\n",
            "(25000,)\n",
            "(25000, 150)\n",
            "[  12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32]\n",
            "Model: \"model_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_32 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_32 (Embedding)     (None, 150, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten_31 (Flatten)         (None, 2400)              0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 2401      \n",
            "=================================================================\n",
            "Total params: 162,401\n",
            "Trainable params: 162,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 194us/step - loss: 0.5303 - acc: 0.7379 - val_loss: 0.3374 - val_acc: 0.8636\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 3s 130us/step - loss: 0.2554 - acc: 0.9010 - val_loss: 0.3031 - val_acc: 0.8676\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 122us/step - loss: 0.1686 - acc: 0.9440 - val_loss: 0.3003 - val_acc: 0.8706\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 121us/step - loss: 0.1070 - acc: 0.9723 - val_loss: 0.3197 - val_acc: 0.8692\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.0629 - acc: 0.9886 - val_loss: 0.3465 - val_acc: 0.8634\n",
            "25000/25000 [==============================] - 1s 49us/step\n",
            "(25000,)\n",
            "(25000, 200)\n",
            "[   5   25  100   43  838  112   50  670    2    9   35  480  284    5\n",
            "  150    4  172  112  167    2  336  385   39    4  172 4536 1111   17\n",
            "  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
            "    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76\n",
            "   15   13 1247    4   22   17  515   17   12   16  626   18    2    5\n",
            "   62  386   12    8  316    8  106    5    4 2223 5244   16  480   66\n",
            " 3785   33    4  130   12   16   38  619    5   25  124   51   36  135\n",
            "   48   25 1415   33    6   22   12  215   28   77   52    5   14  407\n",
            "   16   82    2    8    4  107  117 5952   15  256    4    2    7 3766\n",
            "    5  723   36   71   43  530  476   26  400  317   46    7    4    2\n",
            " 1029   13  104   88    4  381   15  297   98   32 2071   56   26  141\n",
            "    6  194 7486   18    4  226   22   21  134  476   26  480    5  144\n",
            "   30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
            "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
            " 5345   19  178   32]\n",
            "Model: \"model_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_33 (InputLayer)        (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_33 (Embedding)     (None, 200, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten_32 (Flatten)         (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 3201      \n",
            "=================================================================\n",
            "Total params: 163,201\n",
            "Trainable params: 163,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/5\n",
            "20000/20000 [==============================] - 4s 202us/step - loss: 0.5138 - acc: 0.7414 - val_loss: 0.3237 - val_acc: 0.8666\n",
            "Epoch 2/5\n",
            "20000/20000 [==============================] - 2s 115us/step - loss: 0.2400 - acc: 0.9082 - val_loss: 0.2811 - val_acc: 0.8846\n",
            "Epoch 3/5\n",
            "20000/20000 [==============================] - 2s 118us/step - loss: 0.1577 - acc: 0.9470 - val_loss: 0.2855 - val_acc: 0.8814\n",
            "Epoch 4/5\n",
            "20000/20000 [==============================] - 2s 119us/step - loss: 0.0999 - acc: 0.9745 - val_loss: 0.3101 - val_acc: 0.8730\n",
            "Epoch 5/5\n",
            "20000/20000 [==============================] - 2s 117us/step - loss: 0.0603 - acc: 0.9897 - val_loss: 0.3236 - val_acc: 0.8766\n",
            "25000/25000 [==============================] - 1s 48us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f350fbe516c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Z = f(X, Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VOIqm4vEYMD",
        "colab_type": "code",
        "outputId": "71d23a4d-3608-4aba-9237-7877bfa533f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "X, Y = np.meshgrid(embedded_size, maxlen)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
        "ax.set_xlabel('embedded_size')\n",
        "ax.set_ylabel('maxlen')\n",
        "ax.set_zlabel('z');\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOx9eXxU9bn+c2ZfE7KThewJIWEPYamV\n1oKgeEG911Z7q1fb22IXLUK1egUtFqtA3YX6awW1ahELVNHaotS1CghhCTtkmWyTbWaS2feZ8/uD\nz/frmTUzk8mG5/l85kOY5Swz5zznPc/7vs/LsCwLHjx48OAxMhCM9gbw4MGDx9cJPOny4MGDxwiC\nJ10ePHjwGEHwpMuDBw8eIwiedHnw4MFjBCEa5HW+tIEHDx484gcT6QU+0uXBgwePEQRPujx48OAx\nguBJlwcPHjxGEDzp8uDBg8cIgiddHjx48BhB8KTLgwcPHiMInnR58ODBYwTBky4PHjx4jCB40uXB\ngwePEQRPujx48OAxguBJlwcPHjxGEDzp8uDBg8cIgiddHjx48BhBDOYyxoNHRLAsC7/fD5fLBa/X\nC5FIBIFAAKFQCIFAAIFAAIaJaLbEg8fXEswggyl5a0ceIWBZFj6fD16vN+Bv8hqXaAkJkwdPxjy+\nJoh4gPOkyyNmBJMtwzDwer1ob2+Hx+OBSqWCUqmEXC6HQCAAy7IBj7a2NmRnZ0OhUPBkzONyR8QD\nmZcXeAwKlmXh9XrR2dmJlJQUKJVKeDwetLW1QafTITc3FwqFAjabDX19fXA4HAAAuVwOpVIJpVIJ\nhUIBl8sF4FL0CwBerxcejydgXTwZ87jcwUe6PCKCkC2RDi5cuIAJEybAaDRiYGAARUVFyM3Nhd/v\nh9frpWQKAH6/H06nEzabjT76+/shFosDiJj8KxQKQY5FbnRMyJZhGAiFQqobE3LmyZjHGAUvL/CI\nHX6/P0CnZRgGNpsNJ06cAMuyqKioQE5ODiU8n88Hj8cTQLrhcP78eeTm5kIsFsNut1Myttvt8Pv9\nkMlklJBjIeP29nYUFRVFjIx5QuYxiuDlBR6Dg0SsPp8PwCWytVgsaGlpgdvthkqlQl5eHrKzsxNa\nPiFBhUIBhUKBzMxM+hrLsnA6nZSMBwYGYLfb4fP5IJVKw5KxTqdDcXFxwAWCuy6BQACRSMSTMY8x\nBZ50v+YgkaPH44Hf7wdwibAGBgbQ0tIChmFQWlqKtLQ0NDU1DWldDMMg0p0VwzCQy+WQy+XIyMgI\n2D63202j4q6uLthsNvh8PjgcDjQ1NQWQsVgspusg5WzB6yHSBFeq4MmYx0iBJ92vKUiNrdfrpWQL\nAAaDAS0tLZDJZJg8eTLUajV9jWGYgPfGi2ikG+0zUqkUUqkU6enpAdt/+PBhZGRkwG63o6enBzab\nDV6vN0A3Jg+xWByw3z6fD263O2B5FosFmZmZAVIFT8Y8kg2edL9m4JKtRqOBVCrFxIkT0dvbi9bW\nVqjVakydOhVKpTLks4mQ5nCBRKzp6ekBZAyARsZ2ux19fX2w2WzweDwQiURhyZiUvjU3NyMlJSWk\n1jicZsxXVPBIFDzpfk0QrsYWAPR6PVpbW5Geno5Zs2ZBJpNFXAapvU0UI0XaEokEEokEaWlpAc97\nPB4qU+j1erS1tcHtdkMoFEKhUMDtdsNkMkGpVEIikdDtJZEx3/jBIxngSfcyRziy9fv96OzspJHt\nnDlzIJFIBl3WaMgLyYRYLMaECRMwYcKEgOe9Xi/MZjOMRiP6+/vR0dEBl8tFyZgbGUulUrofRAt3\nu91hyZhhGIjFYp6MeQSAJ93LFKTGlhuh+Xw+tLW1oaenB/n5+SgtLQWAmAgXSA5pjhV5gguRSISU\nlBRIpVKUl5fT530+H42MjUYjtFotnE4nBAJBCBnLZLKA78fr9aK+vh61tbV0eXxkzAPgSfeyQ3BD\nA8Mw8Hg8aG1thV6vR2FhIRYsWAChUIiurq6Q7H40CASCsJFurKQxlsklWDoALmm5KSkpSElJCXje\n5/PR0jaTyYTu7m44HA4IBIKALjyWZSmpcsk4XBceKW/jGz8uf/Cke5nA7/fDYrHQW1qGYeBwOKDR\naGAymVBcXIyKioqABoZIJBoJ411eSBaEQiHUanVAZQdw6TcgZGyxWOByuVBfXw8gsCU62J8CQIgE\nRAibb/y4/MCT7jgHt6FBo9EgOzsbEokEGo0GDocDJSUlqK6uDnuSJkK64yGRlgjCRbrxQiAQQKVS\nQaVSAbhUfldXVwe/3w+Hw0GlCuJPwbJsCBkTM6BgMgaA7u5upKSkQKVS8Y0f4xg86Y5DRGpo8Hg8\naGxshFAoRGlpKdLT06OehPGS4FCrF8Y6houwBAIBJVUugv0p9Ho9HA4H/H4/5HJ5gG5MDIVUKhW9\n4+AbP8YneNIdR4jU0EC6x5xOZ0CCbDDw8sJXSPZ2xbI8kpBTKBTIysoK+CyXjPv7+2G326l0QZze\nCBmLRCL6uXCNH0BgrTHf+DG64El3HCCYbMmJotPpoNFooFAoMGXKFPT19UEul8e8XF5eCESyCSjR\n5XFborn+FKdPn0ZeXh5YloXNZoNWqw3xp+BGx+HImKsZMwwDk8mErKwsvvFjBMGT7hhGuBpblmXR\n09OD1tZWTJgwAdOnT4dCoQBwqdEhHhKNVy6IRJrkVpeUTcX7+bGAZG8X9+KYLLAsC5lMBoVCEdWf\noru7m/pTSCSSiGTs9/vR1NSE1NRUvvFjBMGT7hgEIduuri5kZWVRsurs7ERHRwcyMzNRW1sLqVQa\n8LnhlguCl+/z+eg2icVieDweCIXCgMSQSqWKuQ54NJGMRNpwLo8sM5x9ZjR/Cm4XXm9vb4A/hVwu\np40hpAuPfG6wxg+ejBMHT7pjCMENDc3NzcjIyEBHRwe0Wi1yc3Mxd+5cat4SDIFAQG0ZY0Gi8oLP\n56PbNHHiRMydOzdgfA85yQ0GA9ra2qjvAYnUZDIZVCpVxP24HDAcpOv3+wf1LOaCYZiILdFut5t2\n4el0OrS2tob4U5DomNsSDfATP4YKnnTHAMI1NHi9XjidTnz55ZfIz8/H/Pnz6W1hJAiFwpCTIRri\nJV2WZWEymXDw4EHk5+dj3rx5EIlE8Pv9NCISiURITU1FampqwGdJg4bb7aZadLAjGJmxNth+DgfG\nQ6QbL+lGg0QigUqlgkKhQGVlJX3e4/HQhJ3BYEB7ezv1pwgmY25LNHCJjI1GI7q7u2lnH7fxgyfj\nS+BJdxQRbkKDy+WCRqPBwMAABAIBFixYEPOJlkikG4uW6fF40N7eDq1WC7FYTDvauBjsJBKLxVAo\nFJDL5SgoKKDPc7VIrj0j0SIJGXOz9OMBw0W6yVxmOBIXi8VhL5perzfAYL6zsxMulyugHE6pVNJj\nmTvxI1rjB7e07etSUTF+juLLCOEmNNjtdmg0GlgsFpSUlKCqqgqHDh2Ka7nJ1nRJdNrX14dJkyZh\n+vTp6OjoiJtwue8LJvlwt7/RjMu5UyR8Ph98Pl/I9iSC8RDpRtJ0E0U83x3xp4jWEk1m5zmdTlgs\nlpA6Y7lcHnAMfF0nfvCkO0KI1NDAHYdTUlKCmpoaenAREo0n0h1KYozA7XajtbUVOp2OejUIBAJY\nrdYRKRmLlhhyuVyUjN1uN44fPx52vppSqUwqQcWLZBMkwXBHuvEiuCW6r68PdrsdkyZNol14ZrOZ\n+lMwDBPWLIh71xXc+OHxeGAymZCTk3NZNH7wpDvMIDWSer2eZowZhoHRaERLSwsA0HE4wRAKhfD5\nfDHfVg+VdIm00d/fj6KiIpSXlweclKPdHMEwDE3EZWRkoLe3F3PmzInYTEA6u8K12Q43hqNkLNlI\n1l1CuGUKhcKAlmgC4k9ht9thtVrR29sLh8MBILI/hcPhgMFgQFZW1qCNHySoCY7IxxJ40h0mBDc0\n6PV6KJVKKiNIJBJUVFREPTiSFblGez8hLKIjFxcXY/LkyWEJY6zW2UZqJmBZNsDzQK/Xw263Awg9\nwZNNksMhLyQbyUzMEQxG5MH+FNxtcTgcVKrQ6XT0tyLBh06no2RM1sFt/ACA999/Hw0NDXjssceS\nul/JBE+6SUakCQ12ux1arRYZGRmoqakJOw4nGORgixVCoTAu0iXTd48fP0515GhEMV4mR3DXF67N\nNpwBjdVqhdvtxpkzZ0KirUTI8+tMuomUAnITcsEt0d3d3TAYDDTiJXcxwZKSRCKByWQKSQKONfCk\nmyREIlutVov29naIRCIUFBSgrKws5mXGS7qxVi/Y7Xa0tLTAarVCJBJh/vz5MRHEaMsLyUI4AxqL\nxYKOjg4UFhZSa8aenh7qkxvJtDwSxgPpDqe8kCwwDEN9jYuKiujzXEnJbrdjYGAADz/8MC5cuAC1\nWg2DwYDq6mp897vfjRrg7Nu3D6tWrYLP58OPf/xjPPDAAwGvt7e34/bbb4fRaITP58PGjRuxbNky\neDwe/PjHP8axY8fg9XrxP//zP/i///u/mPaJJ90hItyEBpZl0dHRgc7OTmRnZ2POnDnQ6/VxGYYD\nyZcXbDYbmpub4XA4UFpaipqaGhw8eHBI1QfxYKyQbiQwDBP21jfYtLyrq4tOkAjXfcf1NhjLGA15\nIRF4vd6QvAZXUiLYtWsXNm7ciPz8fJSVleHs2bNRjzefz4df/OIX2L9/PwoKClBXV4cVK1agurqa\nvufRRx/F9773PfzsZz/D2bNnsWzZMrS2tmLXrl1wuVw4deoU7HY7qqur8f3vfx/FxcWD7g9Pugki\nXEODz+dDe3s7uru7kZeXR5sHgPij1kQ+E4l0SYWEy+VCaWkpMjIyEiKEy9naMRpJRjIt547z4dau\nCoVC2hbd398f0NU1lkC8GZK9zOEg3eCW90gwm8341re+hSVLlmDp0qVR33v48GGUl5dTV75bbrkF\ne/fuDSBdhmFgNpsBACaTCXl5efR5UlPucDggkUhiTt7xpBsn/H4/1f+USiUYhoHb7UZbWxt0Oh0m\nTZoUtnkgEdIdaqRrNpvR3NwMr9eLsrKykFHl8eJykRciIV5SjDTOx+v1oqenB3q9PqCrK9wI+FhJ\nbyjfe7RljodIN54KHrPZHDJ4NBK0Wi0mTZpE/19QUIAvv/wy4D3r16/HkiVL8Pzzz8Nms+Ff//oX\nAOCmm27C3r17kZubC7vdjqeffjrm84sn3RjBbWggJUl5eXnQaDQwGo1hS6y4GMlI12Qyobm5GSzL\nRixHSwSXs7yQzO0SiUSQy+VQq9UBGj7XfIbrd8BthSaP4GTUcNT9jgdNFwgvL0SC2WxOaiLtjTfe\nwB133IFf/epXOHjwIG677TacPn0ahw8fpnMGBwYGcOWVV2Lx4sUxeVnzpBsFkRoavF4vuru7odPp\nUFJSgilTpgwaJSUa6cbzGZPJRHXbsrKypGdxk0GaY5V0geQ2HoSTKyKNgPd4PLBarSFOYNxWaOJz\nkEwMR6Tr9XpHlXRNJlPMQUZ+fj46Ojro/zs7O5Gfnx/wnu3bt2Pfvn0AgAULFsDpdEKv12PHjh24\n5pprIBaLkZ2djSuuuAL19fU86SaKcBMaiLbT0tICh8MBhUKBWbNmxXwiJBrpxnJb2d/fj+bmZohE\nIkilUsyePTuu9cSKoZ70Y03T5GI4JkfEur9isRhpaWkhrdBcMu7p6YHVasXhw4cDWqHJIxGi8/v9\n4ybSjXWZJpMpZnmhrq4OjY2N0Gg0yM/Px86dO7Fjx46A9xQWFuLDDz/EHXfcgXPnzsHpdCIrKwuF\nhYX46KOPcNttt8Fms+HQoUO45557YlovT7ockLIvn88XUCxPuscEAgFKS0shFArR2toaF4kkSrqR\nKh5YlqVkK5VKUVVVBbVajQMHDsS1DrKsoRJiLMsY6/LCWGqOILaM6enpSE9Pp7PTpk6dCrfbTclY\nq9XCZrOFrVtVKBRRycrn8yU90h0uGSTWSDeeOmGRSIQtW7Zg6dKl8Pl8+NGPfoSamho8/PDDmDNn\nDlasWIEnn3wSP/nJT/D000+DYRi88sorYBgGv/jFL/DDH/4QNTU1YFkWP/zhDzF9+vTY1hvTuy5z\nRKqxJRaEMpkMkydPptlru90eYtQxGJKVSGNZFnq9Hi0tLZDL5aiurg4pcUpkHYlGJ3a7HU1NTTAa\njRAKhbTTi9g0cpsLxjLpJhvDReJcX4rg6RHxtkIPh7wwHIhVXkjk2Fq2bBmWLVsW8Nxvf/tb+nd1\ndTW++OKLkM+pVCrs2rUr7vUBX3PSJQcquSUiJ0l3dzfa2tqQkpKCadOm0XE4BCKRaNiTYsGfYVkW\nfX190Gg0UKlUYbcrESRaBma329Hc3Ay73Y7S0lLqyUo6vYKbC4gBjcvlgsvlGnMlVGMt0g3GYAQZ\nrRXa6XTSyJjbCu1yuaDVapGamhrgdTAWEct3SY7jsXRchcPXknS5NbZtbW0Qi8XIy8tDZ2cn2tvb\nkZGRgVmzZkEmk4X9vFAojDvSTYTcBAIBLT/SaDRISUnBjBkz4ho+ORjiLQPz+/04deoUbDYbysrK\n6AlOzNNJNJWdnU0/Q5oLdDodTCYTzp07R0uoSEQ8mgbmw4GRJt1I4JIxaa91OByQSqU4duwYVCoV\nbYUmxjPB3Xetra2oqqqKaf0Mw+DIkSNgGAZz5syJaRsPHTqEN998E5s3bx7SNBGn05mUQGS4cXkc\n4TEiXEODQCBAX18fWltbMXHiRNTV1Q1aOxlv/Wyi2zowMIDu7m4wDBP1IhDus7Ge8LHuC2kddjgc\nmDx5MjIzM+k6BruYkOYC4pNbVVUFAGETRcQzl0vGI+EMNhyRbjK3OR7S1Wq1IVl4gjNnzuDee+/F\n448/DgDIyckJWC7Xl8JisWDXrl14+eWXccMNN+DGG2+M2gptt9uxc+dO7N+/H3PnzsW2bduibueF\nCxfw5JNP4rPPPqNBT0lJSUz7GA5Go3FMu4sRfC1IN9yEBjINobOzEyqVKqZxOATDefvi9/upvKFS\nqZCeno4pU6bE/Pl4W1AHI12Hw4Hm5mZYrVaUlpbSkd2JIFjTjZS1J565VquVapMsy4boxWNZH062\nXhrOBc3lcoV0an344YfYsGEDHn74YXznO98JeE2n0+H++++HXC5HcXExGhsbQ5ZJpCCFQoF3330X\n27dvx7x583D33XdTT+XgVmiFQoHu7m489dRT6OjowM0334zVq1dH3Jeenh4899xzeOutt6BWq3Hf\nfffh1ltvDdt1Fo/7WzyVC6OJy5p0w01ocLlcaG1thcFgQGFhIaqrqzEwMDDqt7V+v5+a42RlZWHO\nnDnweDxobGyMazmkzCzWEz6SvOBwONDS0gKz2YyysjJqrt7U1BR2OcmKFIM9cwmCI7Cenh7Y7XbU\n19eHzFhLRC8ey5ouqRfn/qZNTU1Yt24dfvnLX+Ib3/gGgEu2ho899himTZuG+fPnByzD6XTi/vvv\nh91ux7PPPkvJKdw2+nw+bN68Gbt378ayZcvw8MMP09v+4KSt0+nEH//4R7z88suYMGEC7rrrLsyc\nORPnzp2DQqGgv4lSqYTL5cL27dvx5z//GT6fD3fccQfuvPPOqEQZb43uWHcYAy5D0o3U0OBwOKDR\naGA2m1FcXIzKykoIBAIYjca49dmhbh/3QPf5fNBqtejo6EB2dnaAvMH1CY0VpKEiHuNzbsTodDrR\n3NwMs9mM0tJSVFdXJ408hlK9wDWXIXrxkSNHMGvWrIDpw9yW29HUi5NFup2dnXjqqadw/fXXo7Cw\nEABQX1+P9evXQ61WUy+A9957D5s2bcKsWbOwcePGAN2fZVls3LgRZ8+exebNm6M63TmdTqxduxaf\nfPIJbr/9dtx1110RL+AajQYPPvggzpw5g+uuuw6//OUv0d/fj5qaGjpTzWq1oqenB++88w727NkD\ni8WChQsXYuXKlSgrKxtUg42nRtdoNPKR7kgiUkOD1WpFS0sLnE4nSkpKQkhEJBIlRLqJOElxy7PC\njTEPTiIkoh0n6tfgdDrR0tICk8mUMNkO9n0MR8lYJP+DePXi0Y50TSYTdu3ahRkzZqCurg4+nw97\n9+7FK6+8AolEApvNBoFAgA8++ABPPPEEioqK8NhjjyErKwt79+7FE088gbq6Ojz++OMht+l/+ctf\nsG/fPtx5551YuHBhxG0wGo1Ys2YNTp48ifvuuw+33HJL2Pf5/X7s2LEDzz77LORyOZ544gksWbIE\nZrMZJpMJwKXzitSNP/3002hra8O8efOwZs0aFBcXw2q1hox+50bFpBU6ngCClxdGCNyGhhMnTmD6\n9OlgGAYmkwktLS3w+/3UfyDcSZAo6ZIKhniyraTZobe3lyY7uE5k4d4/3CY5fr8/4KIUS0tzMKGQ\n+tFYCHWkdNhY9WKbzQYAdN5WuPriRBCJdD0eDw4dOgSRSIQFCxbAbrfj7bffxt69e+FyuZCSkoL8\n/Hw8+eSTOHPmDObNm4df/vKXcLlceOutt/C3v/0Ns2fPxm9+8xuoVCrs3r0bzz77LBYsWIANGzaE\nEO6BAwewdetWLFq0CHfccUfE7e3u7sZdd92Frq4ubNy4EYsXL474vnXr1uHIkSNYuHAhfvOb31CN\nn9uNdvToUWzevBkNDQ2oqKjAn/70J1x55ZX0OwmWAYgvBRnhQ1qhyXHV1dVFyTjS+RKP2c1oYtyS\nbriGBqfTCZ1OR03DY/EfGArpxtP94vV64XQ6cfTo0YhOZMEYzkiXjOjR6/UoLCzEzJkzYzYyTzQq\nHO36yWh6sVarhclkCltfnIheTOqq29raUFdXh4GBAezfvx/vv/8+jEYjZsyYgd7eXuzatQsWiwVX\nXHEFvv/97+PYsWP42c9+BrFYjHvvvReLFi2C3+/Hxo0b8dFHH2Hx4sW49957IRaLsXPnTmzduhVX\nXnklHnnkkZBjUaPR4KGHHkJFRQXWrVsXsN3ci9+FCxcosW/dujVsGznLsnjnnXewadMm+P1+/OY3\nv8F//ud/hkhlXV1deOaZZ/Dhhx8iOzsbv/vd73DDDTcMeqxH8qXo7u7GwMAATTCTqdDEl4L8LmKx\nGCaTCbm5uYP+NqONcUe64ciWYRj09fXRcd1TpkyJuUsrkWgSiJ2sPR4P2tra0NvbC6FQiKlTp8Zc\n1pKo52000nW5XGhpacHAwABKSkrAMAwmTJgQd7VDojWjY7HiQCAQUNmBa0LN9cuNphfL5XJotVqU\nlJTAaDTiwIED+PDDD6HVaiGVSjFr1iwcPHgQXq8XM2fORG5uLr744gvU19dj1qxZuO2226BQKPD0\n00/j1KlTqKurw6pVq5CZmQmHw4FHH30Uhw4dwvLly7Fq1SowDIM9e/Zg69atuOqqq/Dwww+HRH9m\nsxn33XcfJBIJNm/eHFLbTaoCDh8+jHvvvRcqlQrbt28Pq/caDAZs2LABH330EWbPno1HH30UBQUF\nAe/R6XT4/e9/j7///e+Qy+W45557cPvttyelplytVgesj5Qekt9Gq9Vi9erV6OjoQG5uLhoaGlBT\nU4Nbb7016nGd6NQIADh58iTuvPNOmM1mCAQCHDlyJOaSznFHuj6fDx6Ph36ZXV1daGtrw4QJE5CW\nloby8vK42mITjb4GI+twY8wHc7JPBiKZ5HAn/XLnoVmt1rii6aEQ51glXSC8HDCYXtzc3IwjR47g\n5MmTMJvNyM3NhVarhd/vR2pqKtRqNfr7+1FfX4+rr74aOTk5+Mc//oEjR46gsrISq1evxtSpU/H3\nv/8d27dvh1AoxJo1a3D11VeDYRgMDAxg3bp1uHjxIm677TZcc801YBgGZ8+exfPPP48rrrgiLOF6\nvV6sW7cOPT092Lp1KyZOnBiyv36/H/X19Xj55ZdRVFSE559/Hjk5OSHv+/jjj7F+/XpYrVasWbMG\nt912W0DUarPZ8PLLL+Oll16Cy+XC8uXLcf/99w/Zu5kgnIEOtxWarGffvn24++67ccMNN0Amk6Gp\nqSnquT2UqRFerxe33norXnvtNcyYMQMGgyEumXHckS5JenR2dqKjowNZWVmora2FVCrFuXPnaGfU\ncCNSpBtckrZgwQIaFSYaVceDYDtILtkWh5n0G6+EMZaJcyRgMBjwwQcf4NNPP4XFYqHVMizLwmaz\nobi4GFqtFgaDAWlpaVi0aBEyMjLw2WefoaOjAwUFBXjwwQcxb9489PT04IEHHsDJkycxZ84crFq1\niuqjWq0WDzzwAAwGA9avX4/8/HxaJ7t+/XpkZmZi7dq1YfXNLVu24PDhw1i7di1mzJgR8jrLsnj9\n9dfx4osvora2Fk8++WTIVAyLxYLNmzdj7969qKqqwrZt21BRUUFf93q92LNnD7Zs2QKdToelS5fi\nlltuQXFxcdIIl6wn1i4zs9mMyZMnx1TXPpSpER988AGmT59Ov1uuVBULxh3pWiwW1NfXIzc3NyTj\nn6g+mwiCCTR4jHlFRUXILfhIka7f74fb7YZGo4HBYAhLttz3x0OiQ+nGG8uEHU2n9vv9aGhowJ49\ne6DRaGilDHCp7bmmpgY2mw2nT59GX18fZs2ahRkzZiAnJwdvv/029u3bh8zMTPzgBz9AVVUVHA4H\n/vCHP+D999+HQCDAT37yE1x33XU0CXbu3DmsXbsWAPDEE0+guroaLS0tAIBNmzahr68PW7ZsCSFK\nAHj33Xexc+dO3HzzzVi+fHnYfXnqqafwxhtvoK6uDs8++2xI8u3w4cN46KGH0Nvbi5UrV+LOO++k\n5xnLsvj444/xxBNPoKWlBbNnz8bzzz+PmTNnQqPRJL0sL14D81i9dIcyNeLixYtgGAZLly6FTqfD\nLbfcgl//+tcx7tE4JF2lUhkxCZUo6SaiU5J1kfpfk8k06BjzkSBdchfQ1NSEoqKisOTPRbzeC5er\nvBAOTqcTH3/8Md5++21YLBb6PQmFQtTW1iInJwdnzpzBF198AblcjiVLluDaa6+F1+vFCy+8gDNn\nziA1NRUrV67E0qVLIRaL0dPTg2eeeQYnTpzAjBkzcNttt0EqleL8+fNwu924cOEC/vKXvyAtLQ0P\nP/wwjS5ZlsX+/fvxySef4Kc//SmmTp0asr0NDQ3YtGkT6urqcPfdd4e87nK58PDDD+Nf//oXbrrp\nJixfvjyAcJ1OJ5577jm8/uZJ42YAACAASURBVPrrKCoqwp///OeASLmnpwf33nsv6uvrUVxcjC1b\ntmDRokX0eB/tUT3JLhmLNDXC6/Xi888/x5EjR6BQKLBo0SLU1tZi0aJFMS133JGuUCiM+MOSYYDx\nghBoPEP6SJ1te3v7sE6PIKQ42AWBaMharRZZWVmYOXNmTBeRROp6L0fSDbb03LVrFw4cOBBwEZfJ\nZJg3bx5SUlLw0Ucf4dChQ8jLy8P//u//4tvf/jYsFgt27NiBTz75BFKpFN/97ndx0003QS6Xg2VZ\nvPfee9i2bRsYhsGqVauoRkvw7rvv4tVXX0VpaSlWrVoFgUCAhoYGeqxt27YNM2bMwPLly0OOCSJV\nTJw4EY8++mgIUVksFvzqV7/C0aNHsWrVKlx//fXo6emhrw8MDOBHP/oRmpub8f3vfx/33HNPQBLM\n4/HgnnvuQWNjI9avX4//+q//CtExR9vA3OPxxDzAcihTIwoKCrBw4UJq9rRs2TIcO3bs8iXdaBCJ\nRHGPOSefi5V0bTYbzf6np6fT9thYMJS620gEyk3YFRUVoaSkBCKRKOaoPd6RQJEiY5LcHO126kTB\nsizOnz+PZ599Flqtll4cBAIB5HI5VqxYAY1Gg08++QRerxezZs3CddddhxkzZsBkMuG1116jcsGN\nN96IadOmobKyEnK5HL29vXjmmWdw/PhxzJw5E6tXrw5IWrEsi5deegk7duzAvHnz8NBDDwUQns1m\nw+bNm6FSqbBy5Up0dnbS+mKZTAaHw4FnnnkGLpcLf/jDHyCRSHDq1ClcvHgRFy5cQGNjIxobG+Hx\nePDoo4/i2muvxcDAAD1G/H4/1q5di/b2drzwwgu44oorQr6fLVu24MSJE3jqqadC/GcJRnM+WrwX\n86FMjVi6dCk2b94Mu90OiUSCTz/9NKrXRDDG3RkSjeCGGulGA8lWkzHmWVlZsFqtcU+PiPeiQIg6\n+MDzeDxobW1FX18fioqKaMKus7NzWOWC4PcTX11uUon03JMHme81FiNdh8OBN998Ex9//HHAbyMW\ni6FQKFBYWIj+/n68+uqrARJCXl4ebDYbduzYgXfeeQdutxtLlizBzTffjIyMDJw9exYA8M9//hMv\nvvgiWJbF3XffjWXLlgUcMx6PB08++ST279+P6667DqtWraLENTAwgKamJrz11lvo6+vDI488AqlU\nSkn0woULuHjxIqxWKwCgrKwMP//5z6HT6ej3rFAoUFlZiRUrVuCaa66h0w24F/I///nP+Pzzz/Hg\ngw+GJVwypeSmm26KSLjA6A+lBGKvRhrK1Ii0tDSsWbMGdXV1YBgGy5Ytw3XXXRfzNo470gUiE8VQ\nu8vCwWKxoLm5GR6PJ2CMucFgiDtqFYlE1EA6nm3jrodLtsHVEcClyCyeC0+i8gK3bbi8vByTJ0+m\npOtwOELcqIRCIZRKJZxOJ0wm06h65/r9fhw7dgy7d+9Ga2srfV4oFEIikUAsFtNa0GPHjgVICAqF\nAi6XC3/729+we/duWK1WXHnllfjBD35As9vApdv9V199FSdOnMDMmTNxzz330NItn8+Hixcv4vjx\n4/jss89w8eJF3HjjjaiqqsKLL76IpqYmXLhwAf39/fQ7VavVePDBB+Hz+QKkEO6xYbPZUF1djYqK\nChQWFiIvLw8KhQJ2ux1utxterxcXL16EUqmkdyYNDQ14/vnncfXVV+Pmm28O+30xDIMNGzYMerwP\nB+nGmmtxOp0xSwsEiU6NAIBbb70Vt956a1zrIxiXpBsJiZJuuM+RMeZ+vx9lZWUhWdFEjMyHIi9w\nmyzCkW3w++Ndfqzw+/3QaDSw2WwoLS2lWrbH46FOWKSLi3sL7fF4YLFY0N/fH+CFIJPJaERMLAWH\nq3Oto6MD7733Hg4cOAC32w3g0v6LxeIAkyHSATV79mwsW7YMM2bMgEAgoMYt+/btQ39/P+bMmYNb\nb70VpaWl8Pl8OHv2LA4dOoRDhw6hvb0dMpkMv/jFL3Dttdeira0Nu3fvxtGjR3HixAk4HA6wLEv9\nBXbu3Bkw+YB8nyzLQigUUgIGLkXh5eXlqKysxOTJk1FZWYny8vKw1QxccP0oBgYGoNfr8bvf/Q5p\naWm4/fbb0dvbG9W/eDBCHQ7SBWKLXseLwxhwmZFuMuQFo9GI5uZmMAyD0tLSiNnQREb2xKufApcO\nuLa2NhiNRto+HO3KP1yJMRJhk2YP4nERK0ibp0QiweTJkwEEjpKxWq3o6+uD3W6nxM3t+oonyckF\ny7I4duwYXn/9dXR3dwMA1Z4J2Xq9XlitVrjdbshkMlxzzTVUQjCbzdi3bx8++eQTnD9/HgzDYPr0\n6bjvvvtQVlaGY8eO4e2338aXX34Jk8lEuw7LysqQm5uL+vp6/PGPf6QXGW65GXBJkyfPA19NCyGY\nMGECqqqqkJaWhrlz52Lq1KkoLCxM6C4h2I/ihRdegNlsxrZt25Cfnx/iRxFt3l04jObMtfFidgOM\nU9JNtrwgEolgMpmg1WohEolQUVExaKtuIpFuPERNRgnpdDrk5+cPSrYEiTQ7RHs/2Y6enh4UFhYi\nNzcXGRkZCXsvcH83hgkdJQOEtt+2tbXB4/EE9NurVKpBLxYnTpzA9u3bodPpAHwV1YrFYgiFQrjd\nbgwMDMDn8yEjIwNXXXUVrr/+eohEIhw5cgQvvfQSjh07Bq/Xi6KiItx+++2YNm0aGhsbsXPnTpw4\ncQIejwcqlQo1NTVIT0/HwMAAjh07BpPJRKdKE6Il2xtOHmAYBkVFRTRyJQ+SIT9x4gSqq6sTvvgE\n4+2338ahQ4dw3333Ua+FwfyLY/GjSLZTW6wwm818pDsaiLeciSQI2traaAQz2C0awVAHTUYCl+QK\nCgqQn5+PzMzMuKoRkiEvcK0nCwoKKOlfuHBhSM0RsSBS+y0ZPW61WtHR0QG73Y4jR45ALpcHJO5a\nWlrwwgsvULIVCoWUbIFL+h/R1ouLi/G9730POTk50Gg02L59O7744gvY7Xakp6fjP/7jP1BRUYH2\n9nZ88sknePHFFwEAWVlZmD59OtVnP/roI/j9fvrgdqoF3+HI5fIQci0tLY3qUxDPBIXBcPr0abz0\n0ktYsGBBRF0ynH8xEN2Pwul0QqvVJs2/OJ7I2Wg08qQ7nBjqwceygWPMi4qK4HA4YiZcILGoOpq8\n4PV60d7ejq6uLkyaNAnz58+HUChEU1NTXOQeyXsh2jZx3+/3+2mLdW5uLt0OgtGsQJBIJEhPT6fJ\nTJvNhtraWpq4O3/+PHbs2IH+/n4AgYkxQhYul4tGlatXr6YdVn/4wx8wMDAAuVyOefPmYdKkSejt\n7cXHH3+Mv/71rwCAvLw8lJWVobu7G62trWhubqbSAHkAgRFaTk4OqqqqUFFRQYk2Ly8v7tvwZM1c\nI0Y4aWlpuP/+++M+l6L5UdTX14Nl2aTNu4t3agQvL4xBsCwLnU6HlpaWgDHmAwMDsFgscS0rkSaB\ncPICIdvu7m4qI3BJLpHINR6SJvvh9/upeVBOTk5En99IpDtato3Ej+DJJ59Ee3s7fU4mk0EsFsPt\ndlOXKLFYjMmTJ2PhwoXQ6XTYvHkz2traIBAIUFlZialTp8LhcODzzz+HzWaDSCRCWloaZDIZ+vv7\ncf78+QCTfO73IBQKqUl+ZWUlbRNNlg9BMvRSlmXxyCOPoLe3Fxs2bEgqSYlEIohEohA3MJfLRZN3\nBoOB3mHEohfHQ7rjZWoEME5JN9oJTkiHS1wsy6K3t5eOMZ8+fXqAicZIeTZw5QWv1xtw+x4cUYb7\nTCxIxBvBZrPh0KFDyMjIGHQa8khMQo4VNpsNDz30EJ0jR8hWKBTC6XTCbDaDZVlaSVFYWIju7m68\n8MILYFkWOTk5KC8vh8ViwcmTJ+H3+yEWi+l4J6/XC71eTyUDArFYjEmTJlGPhcrKShQXFwd0aB09\nejTpJDDUC9ubb76J/fv3Y/Xq1SgvL09qpYHP5wu5KHD9i4k2DcSuF5MpK7HAbDajvLw8afsznBiX\npBsNhECFQiG91WltbUVqamrEMeYjRbokaaXRaOjkiMHMzIerBIxE/Y2NjfD5fJg3b15MdY5jocHB\nZrPhmWeewalTpwBc2meJRAKBQACn00klhNTUVJro0Wg0OH/+PLX/1Ov1aGtroye20+mkQ0y5uixB\nUVERli5diiVLlqC4uHhQAkymBpsMnD9/Hr///e/xzW9+E7fffjsuXLiQ1EqDeOfyxaIXG41Gqr8P\nNu+OlxdGEeSWkpxU6enpmD17dlRCGQnSJYkpUo4T68j3eLvYBiNdlmVhMBjQ1NQElUqF6upqaDSa\nmAvLo5FusueMBYNMNjh8+DDdFpI5t9vt9A6HlESZTCb09fVBKpViwoQJ0Ov16OrqQmdnJx3xRIZ/\nBlcYSKVSzJ07F9/4xjdQXV0NgUAAm80GvV4Pm80WtuNurMJms1Ed93e/+13Yu8GhIhnLC9aLSQkh\nKWez2WwB0yOIXvz555+jt7cXSqUypvUMxbycvF5dXY3169fj3nvvjXs/xyXpRrPgczqdOH78OHJy\ncjBnzpyYSmyGMpxyMK2NWwWQl5cHpVKJkpKSmNeRTHmhv78fTU1NkMlkVGJxuVwjZu2YKLxeL7Zt\n24ZPP/2UEjs5wW02G1iWhUwmg1KphN1uR29vL4BLLbButxt2ux0Gg4HqscEkS/Y/PT0dixcvxsKF\nC1FbWxv22CG3xqTjTqvVwuVyQSgUBhDxcF+AYgXLsvjtb3+Ljo4ObN++nV6Qkl1TO5wtwNHm3Vks\nFrS0tOD06dO4++67IZFIsGDBAmzZsiXidiZqXk6wZs0aXHvttQnv17gk3WBwM+5CoRAVFRVxzUpK\n9OSIZpRDyLazsxN5eXk0MUUK9GNFvCQXrnrBZDKhsbERIpEI1dXVAZM1RtLaMV6QqbP/+Mc/6DaS\nxJ/D4QBwKSL1+Xy0LlYgEMDlctHnuGVc3EiWEO/EiRNRU1ODvLw8FBYWIiMjAx6PB0eOHIFQKIRI\nJKLOdty/SRlaZmYmlbIICeh0OpjNZhw4cABKpRJqtZqS8VAHXsaL/v5+NDQ04Oc//zlqa2vp82Mx\n0g1GtMGvXL34kUcewenTp/Hyyy8jJycn6jk2FPNy4FJ9c0lJScxRdTiMS9LlFpYHjzHn2rUNN8JF\noT6fL6TkKlhGiCcKijfS5ZKoxWJBY2MjWJZFZWVl2IaP4W4bTgQsy+Ltt9/G7t27KTmSTi3SvkuS\nXaQ0iZAqtyEhuAOM/KtQKOhJpdVqodVqh3V/BAIBBAIBhEIh/Ztk+wl5BxN68P9tNhvS0tIiXgjI\nMsnzaWlpWLlyJTIyMrBr164QkhgPka7P54t5xprJZEJaWhpEIlGAOXkwhmJebrVasWnTJuzfvx9P\nPPFEAnt0CeOSdH0+H01GcaNIYGSnR3DXxY22J06cGFGzjeQaFgmJdJiRcfQejwfl5eVR3fST3cE2\nVHzwwQd49dVXaTs3y7LweDyUTIOTXcFES8gWAH2ddKARPb2wsBDf/va38c1vfhMpKSm0bE8qlSIl\nJYXqvWRd3L+52xDtfW1tbcjNzQ37Po/HA5fLBafTCbfbTZN45PvlXpC9Xi81yyd/ezwe+i8AWofM\nfWRnZ2PlypUAELb+fDyQbjwlYy6XKylDMIHI5uXr16/H6tWr45rBGA7jknSJpheO2MRiMb31jHeZ\n8R6IQqEQHo8H7e3tlGwj1bdyPxMP6cYT6RKbRafTierq6phmNw3V2pH7/FDwxRdf4IUXXqDSQPCD\nq8UGt9eSv0lrL7GXdLvd8Hg8EAqFqKurw7e//W1861vfCsiYE7AsC5VKFdCOHAt8Ph8cDgecTicc\nDgd9kAYM7nMkE08ujEKhkH6fHo8HNpsNdrs94P1OpzPACCcY06dPp80b8SKZMsd48dIdinn5l19+\nid27d+PXv/41jEYjLVG86667Yl4/ME5JVygUorCwMOxrQ/FfCFdrGAl+vx92ux2nTp1Cfn5+yLy2\nSIhXLojl/cE2i2azOeZhefGeeOGaQvx+P7RaLZxOJ9UvY6mG6OnpwY4dO3DgwAEazZIHl1DDJb+4\nREt+b5lMBqfTCb/fD5FIhKuuugrf/OY3qTmPw+GATqejLcRcoiTG5QKBIIQog0mT+1y4ypJwJBDc\nSMGVGshvQCJ1ss8kMgaA8vJyrFixgu6zXC7HhAkTkJ2dPepWmcDoT40AYjuWh2Je/u9//5u+Z/36\n9VCpVHETLjBOSReIHHGJxeIh2TsORpyEYNrb2yESiVBeXh5ypYyGZFYjuN1utLS0oL+/P8BmkTQL\nDAe48gKpg9ZoNMjIyIBUKsXAwADa29vh8XggFosDkkg+nw/nzp3De++9h9OnT9OolisRhNNjuf+S\ncidygSQexSzLUk8DsViMrq4uqgvHCqK1i0QiMAwTQIjkdbJt3Aic+zr33+C/g5dDls9dR3V1NUpK\nSiCXyyGTyaBQKCCRSCCTyfDDH/6Qvo9ExlarNaCMimuVOZKJO9Lxl+xlxnIhIcdaLBiKeXmyMG5J\nNxJEIlFC9o6DuYZx22Szs7Mxd+5caLXahHrXhxrpejweaDQa6HQ6lJSURJz0OxwgpKvX69HY2IjU\n1FTU1tbS7517p+B2u9Hd3Y233noLBw8epD61wcmucERLbsFJ84LX66Vm3izLYmBgAACgUqmQkZEB\nvV6P9vZ2tLe3IzU1FVOnTsWVV14JtVpNnczIQyqVwmg0oqurC+3t7WhtbaWm4eT7zczMDPtZuVxO\nLUTtdjstHRsYGIBOpxtU2iKkrlQqIZPJIJfLoVAo6L8//vGPQ6Y3uN1uOomCgFhlchsCYrHKJM0F\nycZoygsmk2lQV0AuhmJeTrB+/fqY1xeMcUu6w2HvGO5zXLLNysoKaJMdCSNzbqQbbLMYq91jMmG3\n29Hd3Q2n04kZM2bQdmruPvX29uLdd9/Fv//9b+j1+rAabPD/AQREs6T0i2VZZGRkQKFQoLW1Ff39\n/cjIyEBKSgoMBgMMBgNmzpyJa6+9FlOnTsW0adNQUFBAL0J2ux0XL17E2bNncfLkSZw7dw4XLlyg\nHgAikQhlZWWYPXs2ampqMGfOHEyZMgWpqano7u5Gc3MzWlpa6L8NDQ0BM9QAIDc3F6WlpfjWt76F\nkpISpKSk4OzZs1Cr1TAYDOjt7UV3dzc6Ozuh0+mwZ88e1NXVxfydx5prGMwq02q1Qq/Xo7W1FTab\nDSdOnAjp9Er0eBoO0o3V5Gc8daMB45h0IyFZc9L8fj91kwomW+5nnE5nXOtJhHRJtUZXV1eAzWIk\nJJIUHAxWqxUXL16E2+1GWloanbUFXDIbOXHiBP71r3+hoaEBdrs9LLEGe8pyDbtVKhVkMhmMRiMc\nDgdUKhUKCwuh1+uh1WohkUigUCig1+vR29uLuro63HDDDVi5ciVSU1NpW/PZs2fx3nvv4ezZszh3\n7hw0Gg1dp1qtRnV1Nb773e+iuroaU6ZMQXZ2Njo7O3Hw4EHo9Xq89tpraG5uhkajCfhtlUolSktL\nUVtbixUrVkCpVNJuwZ6eHrS1teHjjz/Ga6+9FnD8CQQCWgO8ePFiFBYWhk3kRcNQf8vgTi+/34+j\nR49iypQptNOL2y2pUCgCIuNYOu7inWWWTIynqRHAOCbdSAdBonWkhHS5ZJuZmRnVAGa4I11ShsZt\nHY4lmhhsgnA8cDgcaGpqgsPhQEVFBTUPstlseP/99/HOO+/QttpwZt1ETgBAu7ZIE4FYLEZpaSkd\nRUSiWIVCge7ubhgMBiiVSloelZmZiUWLFuHKK6+EUCjE0aNH8fzzz6OxsRHnzp2DwWCg211QUIAp\nU6Zg+fLlqKyshFqthtVqhUajQUtLC9544w20tLRAr9fTzwiFQkyaNAllZWWYNWsWnarscrloAu79\n998PcaRLT0+nZLp48WJUVFTA6XRiyZIlyM/PH7LWmewLKJFspFIppFJpWPPy4I47kUgU1f9gOCLd\nWDFeHMYYhvkpgJ+OW9KNhES1TaFQCL1eT5NCg7ltkc8MB+kG2yzG2zqcjAYGt9uN5uZmGI1GlJeX\nIzMzEy6XCzt27MCHH36I3t7esBIBcMn3lpy8LMsiLS0NKSkp0Ol00Ov1kEqlKCoqgtlsxvnz5yES\niZCSkgKTyYTu7m6IRCJK1kqlEjfffDOWL19OSX/btm3YuHEjAFA7wWnTpmHy5MmoqanB9OnTkZub\nS0nB4/GgsrKS/lYZGRkoLS3F4sWLUVpaitLSUohEIkybNo1GoVu3bsWGDRsAXKqKmDRpEgoLCzF3\n7lwUFhaiqKgIhYWFKCwspHWwV111FW0jrq+vR3Fx8ZB+A4JktxRHI8hoM+6iJe7I8MtkJe7iKQMj\njRFjHSzL/j8A/++yI914wbIsjWxlMhlqa2tjNn9JZE5aNNINrgYgxN/X1xfXOhIhXXJie71etLa2\nore3FyUlJaiqqkJ3dzc2bdqEDz/8kG47ye57PB7agKBWq6l/rd/vR2pqKlJSUtDb24u+vj4oFArk\n5eWhr68PjY2NVH+0WCzwer2YOnUqMjIy8Omnn2LBggW46aabsGTJkoAoj2EYXH311cjMzITf78eK\nFStoxQohBavVioaGBtrRpFKp8Pjjj6O4uBiTJ08Oe4I2NjYGRKTXX389JdisrKyYIs0//elPyM3N\nHZbIdLSXN1jirqenB+3t7XA6nWETd/GOGYpHrjCbzXEl0kYb45Z0o11NY9E0uWSbnp6O8vJy2O32\nuMY4Jzqyh7Sycrelr68PLS0tmDBhQlzEHw6JdJl5vV7aEksGYB45cgSbNm3CmTNn6PKIhaLL5aJG\nL2lpabSiwev1IiUlBUqlEn19fdDpdFCpVEhPT4dOp6O1xCqVCkeOHEF+fj6+853vYO7cufQkW7Zs\nGaZNm0ZHlgejuLgYxcXFOHLkCCVKkUiE1NTUAG2PeDRYrVbMnTsXVqsVjY2NIeY04QxqCgoKAgy5\nY0FFRQWAS4SRzMh0OLrHkrE8buKutbUVU6dOpTmI4MQdmXHHJeJoibt4anRNJlPEuv2xBoZh7hi3\npBsN0YxouNFkWloaJbj+/n7ajx/veuIBl6i5NotqtRozZ86M2MoYr19DrKRLuqG+/PJL2lLd29uL\nX/3qVzhz5gzdP5FIBKlUCrfbDYvFAoZhkJKSQseDezweKBQKpKWlQa/Xw2AwIDs7GyUlJTh58iSk\nUinq6uowc+ZMFBQUwGq1Yv78+SgtLaUztkg761Cm/3LBMAwUCgUUCkVA8opM/yW3ylarlZpqp6Wl\nDdmyMdlywHBpuskG2cZwI31YlqUz7mw2W8DU4XCJu3i6No1GY0Bid6yCYZhaAPeOW9KNdlCHI12S\nAGppaUFaWhpmz54dYGieCIEORV4IZ7MYCaQ8LtYTOZaRPSS6bm5uht/vR21tLQQCATZu3Ih///vf\ntMyLuDkxDAOLxUJv2RUKBfr7++FyuahsoNFoYDKZUFBQgNmzZ6OwsBAMw9Ck1oQJE2iXVUZGBtRq\nNfx+P+RyOdUZhUIhSktLkzbmJhxEIlHIrTIxOAcQNoHEjdAGI6zhIN2xTOKxgGGYuBJ3wKXvMZZB\nl2azeVwk0gDcBSB93JJuNHDLxrijelJTU0PIliDRqDXezzgcDvT09FB/hFjMMwhRJ2sisMFgQGNj\nI9RqNWbPno1Tp07h3Xffxeuvvw632033iZwkVqsVLpeLWhkaDAZ0d3dDJpOhuro6YFTN7NmzkZOT\nA4VCQWWLb3zjG5RsyfvCkS3LssjMzByW4v3BQG6VU1JSQhJIJCrWarXUv1ehUASQMXcMebJJN9Z6\n1VgxmpUGwYiUuOvt7YXBYKAyIEnccWerkah4vNTpsiz7Q2Aca7rRQLqjiIwQbVQP9zPxEmg8Jxax\nWfR4PFRKiBWEdGMtPYpEulxf3WnTpkGpVOLChQvYuHFjgEWiQCCAWq2G0+mEwWCAQCBAdnY2srKy\nwDAM+vv7UVVVhdLSUlpfK5fLce211waQLYkKvV4vNaAhkXMw2ZLnsrOzo0b9w4VI2fJwBtrEd8Nq\ntWJgYAAdHR1wu90Qi8UBt8fJiijHQiJtpEFm2wUPuuR23LW3t2PlypVwOp147LHHsGDBAsybNw/z\n58+PuNxEp0bs378fDzzwANxuNyQSCX7/+9/jO9/5TkL7Nm5JNxLhETPps2fPIjMzc1CyJRguS0ir\n1YqmpiZ4PB5UVFRAIpHgwoULcS1jqJ63NpsNjY2N8Hq91FfXbrfjnnvuwfnz5wMMZRQKBdVqSf1j\nbm4uZDIZJBIJpFIprrrqKtrGSkhWoVCAZdmIZOvz+aBQKOD1eqm3LJdsWZZFfn7+uIhYBAIBjbS4\nIJrlwMAAHA4Hjh49CgABkVkievVYt2FMtvwBhPddCO64KykpwbFjx7B48WL8/Oc/R1NTE06dOhWR\ndH2+xKdGZGZm4t1330VeXh5Onz6NpUuXJuzDPG5JNxikI6m5uZnqiMQdPhYkMlI9GojNot1uR3l5\nOdWxiMlLPEi0ddjpdKKpqQk2m41uA8uyeOWVV/Dmm2/S22ByUqenp0Mul1OD7aqqKjrKXCKRBPgF\nEAMbEpV6PJ6YyJacSFKplPopSKVS+P1+qNXqUetqSoYkIJFIkJ6eDplMBofDgalTp8Lv99NMvsFg\nQFtbG9xuN53vRR4KhSIisY71SHe4fBdiCZaAS+fUFVdcgYULF0Z931CmRsyaNYu+p6amhjrMJVJl\nNG5Jl6uf6XQ6tLS0QKVSYebMmTSbPlLgnrBOpxPNzc2wWCwoKytDZmZmwMmcSJlZInW3nZ2daG5u\nRllZGWpqaqgssHLlSlitVgCgJTxkwCMx+xaLxfQhlUppdEGyzB6PB1KplFY+qFQq2tIbjmxJEoVY\nLpLEoEwmo5n0wsLChKLcsTKLjAsuqRGphmskzs3kEzImPrvBUbFYLB7z1RBjwUs3lv0ZytQILvbs\n2TPosNtoGLeky41scJg7xAAAIABJREFUVSpVQAWAWCymhibDDUKIXq8XGo0G/f39KCsrQ3V1ddgT\nJdHa3lg+QwxxOjo6kJmZidmzZ1O3rueeew4fffQRJT4StZJRMWTUCykNI3aCJGEkk8kCLPQI2ZIW\n3cHIViwW09tQ0rEWHPmOJnkmk9gGW1akTD63vlWn00Gj0dDWdBIJk6h4KNuabBvG4SDdeJeZrN8u\n0tQIQupnzpzB/fffjw8++CDhdYxb0gUuJafClVsNRZ+N9+QTCAS4ePEiBgYGaMfTYCdcvBisBMzv\n99MhmPn5+SgtLaU+rR0dHbj//vtpbS2JViUSCSVZ8iCarUKhoNN1pVIpLWwnCUqxWEy70ZRKJVwu\nF32eyBXcpgXSKkwImBAvcIlsCwoKRqViYbiQKIFHqm9tamqi5uo6nY7aNQY3eMQqzQxHpJtsWSjW\nSDeeJoqhTI0gxkg33ngjXn31VZSVlcWxN4EYt6TLMAzKysoiGpkn6qkb6wFEokqz2YzMzEzMnz9/\n2DLCkZodSDmNRqNBTk4OHRVESpseeeQRNDQ0gGVZmllXKBQB02y5ZEtkBELKJFNLuuhUKhX9XlUq\nFZ3xRaQGIk/4fD5IJJIAs26SOOOuO55bw+HGSEa68YC0W0+YMCEkKibyRG9vL5qbm2M2MU92ZBrv\nhIdYlxnLeRhPC/BQpkYYjUZcd9112LhxY4jfcbwYt6QLDJ+nbrQf2+fzob29ndosklKq4SSOYHmB\nK62kpaWFmPO0tLTgT3/6E2w2G53jRPRBQnoSiYQ+CNmqVCo6941Es263m3oqkMoE0gJM1kmkCXLB\nIknJ4NIw4Kv5dsAlsrXb7bQAfrCE0njBSDRHCIXCsG3P3JKq3t5eOByOkLZnktRMFkbbwDzWXMBQ\npkZs2bIFTU1N+O1vf0sNzz/44IO4bTqBcU66kTDUkT3hQGwWOzo6kJeXR20Wz549O+zTh7mJtIGB\nAVy8eBEKhSKkbdjn8+HBBx9Ea2srAFCylclkAZGtWCymVQhkLAwhTblcTiNbQrZEv3U6nfB4PJRY\nyfLICUIickK2AAKiYOASMZBGArFYjEmTJlGiCE4ocUf9DHdVw1iNdMnyhmJiHtz2rNPpYDQaQ5J2\npPMwXoymphuvl26iUyPWrVuHdevWxbyeaBjXpBvpAEmkUwwIT7rBNovB034Tjarj9VKw2Ww4evQo\nBAIBampqQmpEDx48iBdffBEOh4N2+cjlcqrFcqsRiHxApAYS2YpEIkq2RKclkS0p/idasUwmo7eV\nwWRLysBYlg34rkhkTOSG/Px8CIVCKJXKsAkli8UScOvM7QIjY3vGYvXCWKs2CG579vl8KCoqgkAg\ngNVqhcViodNAEml7Hq4Ot1i+w/FmYA6Mc9KNhEQPeC6Bcl3IopmZD6UaIZbozW63o7OzE263G9On\nTw+5lXK5XHjkkUdogoCbACP1tSKRiD5PyBa4FAERzc/n81Gy9Xq91GycRLEksiV6OZlmG4lsyXhx\n4Cuy5eq6crk8Yl10pISSw+GAxWKhPfp2ux0NDQ00IiYzzRIhqGQS5VivqyVleqQMkHuLHK7tmVRP\ncC943LZnouGPBsZLCzAX45p0kx3hkOw8McaJxWZxKEbm0UjX5XKhubmZjlNnGCbEy/Sf//wn/va3\nv9FIgzhqEXIkmi3Rc7lkq1Kp4Ha7qfG01+sNIVsS2ZJkGrfBIZhsuTW3XLLlJtK433N6enrcJUFk\n/0iPfn19PaZMmRJgIxic2Ver1SM+nnysG95Ei0wjtT1HmiahVqvhcDgwYcKEUWkv5kl3jCGeg58k\nIbRaLTIyMuJqH05m3S2Z9KvX6+lYdb1eTyfVApdK5Z566im0tbXRms/g6FYikUCtVtOIhvghcMmW\nECzLstR8htRwErIl5WpczTbYM4GcbOREJvot9zlCemS5ySIRUk8cKbPf09NDfSVItEYi4+EyqRnr\nhjfxkmMkUxrS4NHa2koHcAIIGxXHu32xfn9GozGk7Gus47Il3Xhu4YnNot/vx8SJE1FZWRnzeogO\nmsi2ccGtiigqKgooQSNk5/f7sXPnTnz22WcAQKNXhUJBZQSpVAq1Wk2rFoh0QMiWJMVIO7JSqaTb\nH45sidZLkmzBTQ8k8uUmy7jVCiTyZRiG/hZErhguRMrsE5Mao9FITWrInYDL5YLD4UhKk8ZY03SD\nkSwTc9L2rNPpMHHiRKSmpoaYAbW3t9PvmZsYjValEu/UiJqamiHvy0hiXJNutAObaI/Rfjyj0Yim\npiaIRCJUV1fT5E08GOqcNL/fD61Wi/b29oCqiOD39/f3Y8OGDejp6aHEyO0qk0gkVAMlEZzf76dt\nuyQpRrrGyN+ELIPJViKR0MiXkC2pAyWRCLe1F0BAh1kw2ZJSMaPROKxeuZFAKiKCozWXy0WliY6O\nDjQ2NlJ5gksS8UghY510geTWRnPlikhmQOR75lapAIFmQGq1mlYexUO6fCJtjCBaVYHZbEZTUxMA\nUNctAFTLjHc9icgLXq8XPT09aGlpQVZWFubOnRu2NZNlWezatQsNDQ0ALl1MuA0MJIIgr0kkEnqx\nYRgmgGwJCZPIltz2BZdzccnW4/HQv30+Hy3WJ9IBIVei34YjW/J+7sj1sQLSjtvZ2Ymqqiq6/8GT\nJbjJJELGkbT+8UC6yUQs1QuR2p5JVGwwGOhYH3Lu9vb2Dtr2zGu6I4xYIl0ugm0Wg3+skTAyJ9aT\n586dQ0ZGRtREXXNzM1555RXawkukA+KNQMqmSHUCGXtDmka40axCoYDH4wloweVGxIR4I5Et1w2M\nfJb8SzRHQgzhyFYoFEImk9Eyr7EG7jYNJk9wPXSJPEGImFhcJlvTHWtlcVwkWjImFArDmgH19fXR\nxg5u23M4M6DxMgmYi3FNutHAJVCuzWJFRUXE29uhSgWDwWg0UiPzSZMmoTjCiG6/34+//vWvOHz4\nMICvkkUKhYISL1m3XC6Hy+WipEy6xjweTwDZkgiURLMAaAKNkDH5mzvVAQCtUuDKB4SkuVotWQch\nWS7pkkekLsKxgME8M6LJExaLhRKE1+ul2nAi8kQwkp1ISzaSWadLjh+1Wh1wfgSbAZ06dQqrVq0C\ny7LYunUr5s2bhzlz5kQ8p4DEDcwB4PHHH8f27dshFArx3HPPYenSpQnv42VNug6HA2fOnIlosxju\nM/FKBbFEx1arFRcvXgTLsqiqqkJ/f3/Eg/T48ePYs2cP3G43jQ5JZQKxUGRZNqDygJAiN7INJkIS\nzZL950azJILlki1pgCB/cyPb4GoFrqYbiWxJ9DtWkeiFINxtc1tbG/UKDpYnuDXFo1XbmmwkW/4I\np+kG125XVlbi6NGjWLhwIa644gqcPHkSPT09uOuuu8IucygG5mfPnsXOnTtx5swZdHV1YfHixbh4\n8WLCF5pxTbqRTmKXywW9Xg+bzYYpU6ZEtFkMxlCn+wbD4XCgqakJDocDFRUVAYMPg9fjdrvx1ltv\n4fjx4zQaJZUJxIGLRFsul4sSJImqSIRKbBWJgxhXOggXzZJqBFJREK0ygWtQw9VzuZ/nRrTB7yHb\nkKxIN9lRczJNapRKZcAIeVLrarFYQuSJ4Kz+cF6chusuY6TqiIPXyTAMrr/+etxwww1R3zsUA/O9\ne/filltugVQqRUlJCcrLy3H48GEsWLAgof0b16QLBJ543BpXUuDNvRUcDMnyunW73WhubobRaER5\neXlIhB1cZnbw4EHs27ePEiSpuyW6FZlszLIsrTflmtJwPWy5NbTc233ih8slWwAB1QgkiuXqwoR0\niVcCAPp8JLLlRrYkMh8O0k0mkrlN4TRYbq0r931kpD25bbbb7QEGNUS3TxaSVS42nCBm+LEiFsIf\nioG5VqsNGAFUUFCQ8Kge4DIgXeArm8Wenh5a4zowMACdThfXchK5WnM/4/V60drait7eXpSUlKCq\nqirsMglRW61W7Nq1C21tbQHRLUmWkQkN5IpOolRyC88lYIFAQImZS3iEbLkyBJEOuOVewT4J3MiW\n28ZLtoWQO5dog8mWyBrc1mrirTsWM/Ij3RxB7iqkUikyMzPp816vl5YvdnV1wW6348iRI0NuOgC+\nagEey4i1ZCzZnXqRDMyTjXFPuu3t7Whvb0dBQQEWLFgQkEEfqfIklmXR2tpKr6bc7QgHYnz+xhtv\nAAA1fUlJSYFcLqe3naSLjGi1hESJjECqM7h+B1ytNZzFIolcuWTLJVhuhQK3s4yQKJdUY0mcEQN0\nr9cLrVYLg8GArKwsWpYGfHWbSJY1GhjuSDceiESigOoJi8WC2traAHmivb2dXpS5RBzOP5eLZEe6\nw3HXEivpWiyWgMqHaBiKgXksn40H4550U1JSwjYUJGrvCMR+0rAsS01BfD5fiANZOPT19WHPnj0w\nGo00OiVSAre+ltumS/aHW23AbcElkScxJCc6LZELuGTLNRcnZE1u+ck6wnkmcEkVQAB5c1/jVjOQ\nqFqv16O5uZnWIxOtmUS95F/yGWB0iHistgEDsckTfX19If65xHuC/I7JjnSHy0s3lmWSadWxYCgG\n5itWrMB///d/Y82aNejq6kJjYyPmzp2b0L4BlwHppqenh9VhiXlNvCDOWdF+dFJL2NzcjIyMDCiV\nSjoiJ9pnTpw4gf3799PbepVKhdTUVFqpQGwWCdlyqwlI1EmMZghJEdIjRf0CgSBgLhk3guXW23L9\nbrm1t4QwuNUP4aJc8hx5nnvykbpit9uNpqYmSCQSzJw5M8DLgtviTMAlYPIARoaIx1KkG7ysSIgm\nTwQ7hZHyQVK/Tu6mhorh8tIdSwbmNTU1+N73vofq6mqIRCJs3bp1SPs87kk3EoY6PSLSl2owGNDU\n1ASVSoXZs2dDJpPRur5IB0pnZyf+/ve/00kO5JaQnJzBJV/cxBghU+CrelmyndzPchNo5DNcaSGY\nrLl/R6q55VYhBP8dLC+Qixwx5+nr66PSiFqthtFohFqtjpqdH4yIuREx8fslF1ayzKGQcbKIMpla\nYyIEHuyfS7bJbrejr68PbrcbZ86cCZAnSAXFYPJEMEZ7asRIGJgDwNq1a7F27dqY1xUNly3pEj0z\nXhDSDe4SM5lMaGxshEgkwtSpUwNu8yKN+fH5fDh58iQOHDhADaJTUlJoREqkgHBkS6oCSCaX1H0G\nky0hP67cACBAUgjnlxDcust1D4uHbIk0QUjeZrPBaDSiqqoKWVlZcLvdMJvN1JDcbrfT74F0IymV\nyohkGUzELMuiq6uL6vgkmudGxcEdcrEQcbIj3WRF4slKOBJPBNIOXl5eHiBPkN+HK08QIo5mZD5c\nkW4s+zweW4CBy4B0k62dBUfINpsNjY2N8Hq9AT4NXIQrG/N6vXj11VdhtVrBMAxSU1MpQZEkGDGY\nIf4IpDKBLI+QLSFEkkjjNiwEywXhRuVw38st/eJ63wJfuYzFkiAjFzVCcj09PWhtbUVeXh7mzp1L\nTxqpVIqsrKyA8TEejwcWiwVms/n/t/fl8U1V6fvP7b6vlLa0dKE7O7TFosKgiNsIjjrK6Di4C45o\nlUVRRgVGqdZRQGFGXADl6w9lGJdRGAYHBBGhZUeg+5Yu6Ub2tNnP7496jidpkiZpAqX0+Xz6Edvk\n5twk97nved/nfV7U19ezHUBISAgjY2tdXDKZDJWVlQgPD0deXp6ZVwUlXZqW4NfGy9XspScGYk7X\nkw5jrqQneE0x3VF5Qg3hyPt3OZrdAIOAdN0NSroajQbV1dVQq9VIT0836ziy9RyKjo4OlJSUoLu7\nm80fA3p8Rmn+FoAZ2fLRK9XR8ppZniApgdJCGQCztl1KtnwUzD+f7w7jbRZ5srVGuHQdPKHR1ubQ\n0FDk5uY6lCf09fVFVFSUWTu20WhkEVdTUxNUKhXrvKMpHJpfszau3VZU62jBbqDmdD01NcIe7KUn\nqDlNQ0MD+94KgoD29naX0hP9gUwmM7uZXy647EnX3gdMtazO3IkFQUBDQwM0Gg3S0tIwZsyYPr9E\nfKSrVqvxxRdfwNu7Z5IDjVhpMwQlVr4ll/6OphtoGoEvitEIlVcZ8HpbGrXw6QLeKwGAWWRoaTTO\nkzQlX8uIlyfbrq4uVFVVgRCC0aNHWyVCZ+Dt7d3rQtfr9aiqqoJYLGZTbM+cOcPyxKGhoQgLC7NL\n9H3liY1GI9rb22E0Gs3yw/0p2LmbdN1JYq6SON2JhISEsE47muqRyWRQq9Vm6Qk+InZkzhqFM++d\nQqFAenq60+dyqXHZk649UNmYIx84bbBoaWlBTEwMJk2a5PCHT6v3dJsdHBwMjUYDf39/5oVg2bJL\nUwV0LA4lbkq2NJo1mUyMePlolY9mKQnzBEtzuvR94FUJfIMD/Td9HC9B439HI3na9Ue77Tzhjcun\nKxISEpCdnc2IghK+QqGARCJBQ0MDdDodAgICzPLE9ibb0mN1dXWhsrISgYGBLF1hKyKmz3OkYOdu\n9YK70wvuNqcJCwsz6/ai6QmlUmkzPUG9c62tz9HzvRwdxoBBTrq0om5vxhk/Wj0hIQFpaWlOXzTU\nnay5uRn79+9nxuJ8Xpa27BJCeuVv+ZwtjYBprpZGJvSHL37RL6dl9xglTp5g+Um+PNlaFsbo46iW\ntq2tzWxAoV6vx/Dhw90S3VqDXC5HZWUlQkNDe+VtAXO3r/j4eAC/jlqi6Ynm5mZoNBrma0AjYqqc\n0Ov1qKmpgVKpRFZWllme3jIiBn4tzjlasBvo6QV3zouzRuL20hNKpdIsPUFd8ygR8wqavjBUSLtE\nsPflticbI6THAaqurs5stHprayvUarVTa/D29oZWq4VEIjEjW9rsQImV97Tlmwvo/1NS5p28bDl4\n8c0PwK95Xvpv+t7Y0tZaa+Gl7yVdr8lkQlRUFEwmE2tuiIiIQFdXF2pra5lPACW1vpQI9kBz6Dqd\nDjk5Ob0mD9iDIPS0SAcGBppNtuWVEx0dHayJxWAwYPjw4cjKyrL7OtbyxHzBjk9RADD7L/1bfwnT\nE4U0dxa+HPVJ4NMTFIT0eEvT5o62tjao1Wro9XpUVlaaNXdYew+GIt0BCGtG5oQQdHR0oKamBpGR\nkb1Gq9Oo1Rn4+Pigrq4OtbW1rGOMz8fyLbu85wGNdGlUSavBlFCB3laKfLQLmCsUeIKm6+K7yqwV\nxfj0AgUlD6VSiaqqKqvNDRQGg4EpERoaGqBSqRxSIvCv1dDQgPb2dofsN52Bn58fhg0bhmHDhjHl\nAy3gdXV1QSQSMXUJv1460NMa7BXs9Ho9amtr2c2WtjrTiNiVPLEnIt2BQuK0JhEQEMAKYnK5nKX4\nVCoVmpqaWHqCdm4GBQXBZDINqRcuFZyJdKVSKSorKxEUFISJEyciMDDQ6nOcdRqjW1tKepb+CLxW\n1jJVQAmYbl/pdAU+D0vTIzxhWuZpAfOGBj4FwacQ7JEthVarZZaUtmRy/PtlObLbmhIBAItcwsLC\nEBISgs7OTtTV1fWSmbkTGo2GSf5sKR+o+RA1mKH+t7RgR8nYWg4S6LmRt7W1ob6+HsnJyczoqK8O\nO+DXPLGtc3d3S/FAIl1roKk4a2PgaXqitrYWS5YsQVNTEx566CFMmjQJN954I6655hqbx+3LwPzZ\nZ5/F999/DwCsiUQmkwEAnnvuOezcuRMmkwmzZs3CunXr+vWZXPakC9i2C6Skq1QqUVlZCS8vL4wZ\nM8bultKVTrampibI5XIm2+L9bS3J1lYXGY1Y+XwWJVv6O15jy+cSLZsYLItj1tIL1sBHnaNGjUJM\nTIxLXy5rSgSTyQSVSsUi4gsXLsDLy4tdWLRjzRaxOQuTyYSGhga0tbUhLS3NrrTI27v3eB6TycSc\nvujOiG6l+TyxTqdDRUUFQkJCeuWg+1JO8HliSsSW8r6BTpKeOJ61nC6fnoiPj8cPP/yAa6+9Fu+8\n8w5Onz6N7u5uu8fsy8B8zZo17N/vvvsuTp48CQD46aefcOjQIZw5cwYAcO211+LAgQOYMWOGy+c4\nKEjXFkymnkm7ra2tVmeiWYMzpEsNb1QqFXx8fHr529JKrCXZ8vlTXubF63DpWgCY5WD5Apo1JYJl\nCkEqlUKj0SApKcmseGd5HrxawBNRJ023UAP3/Px8BAcHM2KjXhb0veMjTGc8AgghzGAnLi7O5XPx\n8vKyOr+ru7ubKSfKy8uh1+sRGhoKHx8fSCQShIWFOaSccLRgR7fWBoPBZmrDGQx0Ene0BZjuAJKT\nk5FiZ0QP4JiBOY9t27Zh5cqVAHquPY1Gw1RIer3eKY9uaxgUpGsZ6Wq1WtTU1ODChQsIDQ3FhAkT\nnJJ/ORPpVlVVMd0tb1DDR5XWush482/eM8FSmcATK328NXtFa//v7e2NmJgYKJVKiMViyGQytnWm\nOUwAEIvFiIiIsKoWcAeMRiNEIhHa2tp6RdCU2KhLPy8Ju3DhAurq6qDX6xEYGMhIOCwszKoiRa1W\no7KyEr6+vpg0aZJd1YoroAU7mUwGiUSCtLQ0xMXFsVZahUIBsViM7u5u+Pr69iow9kXEPBnqdDpU\nVlZCp9MhMzOTvY/8f13REw/0SJf6jfQFer07cl07YmBO0dDQgLq6Olx//fUAeiwer7vuOsTHx4MQ\ngoULFyInJ8eRU7GJQUG6FPzkiFGjRiE+Ph5isdipLbKjpGswGFjHGs3B0gvAMrcL/OrmRXOxvDLB\nUqFAf2ctLcAXx3g5GAAzsqXnLAgC2zpTYjOZTOjs7DQr+kilUlRUVJgRRX+lRYT0uLHV1dU5HHXa\nkoRRL1mZTIbGxkZotVoEBAQwQpNKpUwC5ikZkVKptJpKsCwGATDzNOAn2vKpCTrzjgfdPTU2NrK0\nCP/9tddh50jB7nKIdK3VWiyhUqkc9tJ1Bp999hl+//vfs3Oqrq5GWVkZmpqaAACzZs3CwYMHMW3a\nNJdfY1CQrtFoRH19PVpaWpCUlISCggJ4eXlBpVI5be/oKEF7eXlBJBKZWSrSLzz1QqD/5rWRfLTL\nF7osCdjSUtEy8qWaWt6bls/z2gLf3JCZmcmaG2gOU6FQoK2tDVVVVU4VkyxB8+iBgYH9jjoFQWDz\n4ujWjmpzGxoaUFFRwY5fU1NjFhG7oy2V6npVKhWysrIcutj9/PwQHR1t1j5O6wtKpRKNjY29Coze\n3t5obm5GREQE8vPzbeY2Acec2KwV7NxtYn6xcrqWkMvldgu8PJwxIf/ss8+wYcMG9v9ffvklCgoK\nWB3olltuweHDh4dIt76+HgB6mZl7anpEd3c3ysrK2BeEEqE1LwS+M4wnW8solSdlXrtrLV/r5+fH\nUhJ8dGMPtAmkubkZycnJyMjIMHuOtRwmrRgrFIpeOVee2Hgiph66XV1dfSof+gOFQoHKykqEhYXh\nmmuuYWuguk968+jq6nJqq8+DarkbGhqQnJyMrKys/lWtrSg9TCYT5HI5amtroVar4efnB4lEAo1G\nYyZh60+rM/2eqFQqaDQamEwm5gFCz6c/ROxOdYUnbB0dMTAHgPLyckilUrOBk0lJSfjggw/wwgsv\ngBCCAwcO4JlnnnH8hKxgUJBuenq6VZmXNZ2uO0ALVPRC5+VbfE6XXgTWyJb+nf8dT7KUSIOCguDn\n5we1Wg2JRIKRI0eyi8ieEoGCLyzRyQ2ORiZ8xZjPudKImKYoaFXfaDSiq6sLqampyMnJcevFSKHT\n6VBVVQWNRmO1icKaa5ZerzdrkqBbff7GYSnAVygUqKioQFhYmMdy3Xz6JTk5GfHx8UxqRuVRVFZH\n89r8zYPOz7MGnoiNRiPq6upw4cIFjBs3jgUHtgyA3FGwcxWOkq4zUyMcMTAHeqLcP/zhD2bv6e9/\n/3vs27cP48aNgyAIuPnmmzF79mzXTu4XCH24Kw28sa1WQLuMLEEIweHDh3H11Vc7dbzDhw/jqquu\nsvqlk0gkOHv2LHQ6nVmRi5Il3+7Lky3vZQDALHK1FtHSDit6LLlcjnPnziEpKYmRRF+kRrf4AQEB\nSEtLs9rc0F9Q4qipqWF2fzStw6sQ+jKm6QsmkwmNjY1oaWnBqFGjMHz48H6ROr/VVygUrKkjKCgI\nGo0GBoMBOTk5HhPfq9VqVFRUICAgAOnp6X2+N3xem65Zq9WyNlpb6RSJRILKykrEx8dj5MiRNvO8\nlhI2HvYKdkePHkV+fn4/3glznDx5EmPGjOnz/di5cyfOnDmD1atXu+213QybX85BEenagqsXJU1L\n8B88JZfKykqz8TnWlAl8Z5llNMxHwNYKYwEBAQgMDDQz7aZb+oyMDCgUCtTU1PTaNvPeArS5QaPR\nICMjw2NbfJVKxYgjNzfXLG9LVQhKpdLMmMYRFYIlLly4gKqqKgwfPtypSN0eLLf6hBA0NTWhoaEB\nEREREAQB5eXlAHo3dfSnwEjrDxcuXEBmZqbD0Zq1vDbQk06xZhIfHBzMbiTjx4+326prrRbgSMEO\ncP9gSk+M6hloGNSk6yosSZe2kPLdQ5YTGIBfHbmA3o0NgHX9Le8qFhQUxP7GdzIBPReGZWGGlyq1\nt7ejq6sLBoMBRqMR8fHxyM7Odqgv3lnodDpWWMrMzLQaDfIqBN4KkFchiEQiM4cwSsY0Iu/u7kZl\nZSUEQcCECRMcqmq7Aj6VUFBQYPaZ8U0dYrEYSqXS5QJjZ2cnqqurER8fj7y8PLds3y1N4gkhaGxs\nRENDAyIjI0EIwZkzZ5xqzQYcs8QUiUSsEYh/njMSNks4qq4YIt1LDHsRLc2ROfMloKRLdZ+EEOTk\n5EChUKCxsbGX4oB39qJtvNa6yCw1tDQdQQcGAr3J1l7ellbIo6KiWMEnMTERISEhUKlUrKDl6+tr\nRmr25pTZA7/FT01NZe2ujsKeCkGhUEAul6OxsZEVe4xGI0aOHIkRI0a4XXML9OR66Xtky2SH5n75\n3QJfYKTdavaaOjQaDSoqKiAIgk0PC3dArVajvLwcwcHBmDp1qtnNw55JvKMyQXoNqVQqlJeXY/jw\n4Rg/fjy7xhxGMYVRAAAgAElEQVTpsHNXnlihUPTZFDFQMShI1x6spQr6giAIqK6uhl6vR0ZGBqKi\nolBTU4P29vZeagRKtpSE+a2aNbKljxEEATKZjE0RdoZseUilUlRVVfUaYWNNM2pZ0adk4oi0ipKL\ns8W4vkCLj9QhrK2tDbW1tRgxYgSCg4OhVCrZOGx/f3+z1IS9zi97IOTXOWspKSlO3zz4AiN/TN66\nsL6+3qyLyZM3D5PJxAplWVlZVncetlqzeZlgdXW12c3D0iTeaDSiuroaKpWq15xAax12fK7YWsHO\n2cYOHkOR7iWGvQvGGdI1GAyoq6tDe3s7EhISkJmZye7iljlby8gWgFn+FjA3qOFzt0FBQeju7mb9\n4nz7p6NkSyc3ALBp5EJhTTNqbWAk9Z/liZjv8vJklEaLfkFBQcjLy2OfFx8R83KwlpYW5pnLR/F9\n3TxoKiE8PNymFtYVWKZTZDIZe53w8HCo1WqcP3+eFb/ccfMAfi2UxcXFOZ2ysNXqbM0k3svLCxqN\nhnkp2/se2FI/OKIndnRnqlAohkh3oMIR2RhvZD5y5EikpKSwC4EQgvLyctbaaUm2lsYzll1kvDIh\nKCiIyXz0er2Z9Z+jd3xqHyiXy5GRkeGynyhve0hBiZjmL+VyOQghGDZsGGJiYtjF4k4pGN94YE/X\nS99zy84vvpBEW3Ct3TxoB6G9VII7QHXKGo2mVzQI9L558Gvmdbl9pYCodE6n07k1323ZEUgNffR6\nPRITE9Hd3Y3y8nIzk3hH01b28sTUDS48PJxdF3Q91iLiy9VLFxgkpOtIpGsN1JKvtrYWMTExzMhc\nJBKx5+h0Ovj6+kKn09kkW0tlAq+19fb2ZooEuk6a81UoFDhx4gTCwsIQHh7OqvnWzseyuYFG4e6E\nn58f85rVaDTIyspCVFQUI4jW1tZeBOFq1xdVCzQ1NSElJcXlxgNr04b5KL61tRUKhQIGgwGRkZEY\nMWIEu5m68/3jUxb2JG22bh6WOw86a4x/nymp0fy9O6Rz9s6HmiClpaWZmcNbWzMt5FJTe1v6Z2vv\nB9Uq09dxJCJua2u7bCPdQaHTBXoiHmug+lFLZyCJRIKqqiqEhIT00rC2tLRAq9UiKSkJ58+fB/Cr\n4oCfpMuTLf0b39jAa20B60UyPrpUKBTQaDS9qvkKhQK1tbUYPnw4kpOT3dp2yePChQuorq5GdHQ0\nUlNTbb6O5Zp5InZkm0/z0JGRkUhNTXXr+BgedPRPeHg4Ro4cybbNdM2U1JztVLME9WQIDQ1FWlqa\n286HH1VPi2DUcyIhIQERERFW/Rv6CxrN+vv7IyMjw6nGELpm+kNN4ikR8ybxWq0WZWVl8PX1RWZm\npt3XodeMRqPB22+/jU8++QTnz5/3mBzSDbD5RRo0pEuLFpZoaGiAt7c3EhMTAfRUXqkMKSMjw+o2\nkxoYx8XFobW1lWkHrZEtf4HRvK01ra2jRTK6/aSV8Y6ODhBCEBYWhoiICBYV96fRwBI0b+vt7Y2M\njAyXtqp9ETG9OKqrq2EwGJCVleURORtdCzVhtzeSh+9UUygUvcYP0ejSFqkZDAaW6snOzvaIAQvw\na6Gss7OTtW/TNdPRUpZGOq7cmKncrKWlBVlZWW7bvvMm8bQRRavVwmAwIDY2FnFxcQ7J7k6dOoXC\nwkLMmTMHy5Yt80iXoBtx5ZJuS0sLdDod4uLimCtYZmam3S+URCJBW1ubWZcYYJ1sef9aOlOM19ry\n2yRHi2QajQY1NTXQaDTIzMxESEgIk1XRH51OxzwQXPGdBdyXH7YFevOQy+Vob29Hd3c3goODER0d\nzdbdnyKSJXiHrtTUVMTGxjp9bMvoUq1W92oZDgoKYi3QI0eOREJCgke2+ACY+1tcXBySkpKs3gAo\nqdE1UzlYcHCwWZ7YXgSuVCpRXl7Odh+e2k1R35KAgACMGDGCKT5oCohXTlDNtlarRXFxMfbv34+N\nGzdi/PjxHlmbmzH4SZdOarAEzX+ZTCaWM+rrApHJZGhtbTUbYQ6YWy/yaQRfX18olUoQQphvJ+0k\nc4ZsabdSR0dHn/PCeENt+qPX69mFZk+0z5MT3/PvbhDSM4+utraWkQaNLilBWIuIXSFiuVyOiooK\nj6Qs+DlwUqkUEokEXl5eGDZsGNt9uHubzxfKsrOznd590KYO/gZiranDy8sLdXV1kEqlHo3WaQ6/\nubnZzN3O8jH0O61UKlFSUoKioiLmKbxgwQLMmDGj3ybiFwlXHumaTCaIRCI0NDQgMDDQKTmNVCrF\nhQsXrJKt5Vhz2kUmFouh1WqRkpLitN6Wd7NKSEhAYmKiSxcwL/ehP0aj0YyI6ZY4KirKo/lU6i3g\n7++P9PR0u/pUGhHzeW3qKdAXEfNqgaysLI+MhQd6vk/0hkhVFpTQeO8GywGXzn6O/HfB3YUyvqmD\ntmd3dXWxZhV3eGRYQ1dXF8rKyhASEoL09HSHomitVovXX38dBw8exMqVK6HRaHDixAlcc801uOGG\nG9y6Pg9h8JMubX+lVdfa2lrExsZi2LBhEIlEDm9J6Ch1qk3kjWpCQkLg7++Prq4u+Pv7m0WRbW1t\nbJoAL3PpCxKJBNXV1QgPD8eoUaPcnqeirmCdnZ1oampimmU+srQ3/dZZUFKnfr2uVphtETFPaJ2d\nnWhubvZoFR/41fvB3hYfMO/6okQMOJ5vpR1lQUFBSE9P91jOknbidXd3Izs7G4QQs3VTjwxHHc1s\ngRACkUgEsViM7Oxsh78Lx48fxzPPPIO7774bS5Ys8Vhg4GFcGaTb3t6OqqoqhIWFIS0tDf7+/iyH\nNHnyZIeOo1KpIJVKmazLz8+Pka21Lx3N2XZ1daGiosKMHOiPteeq1WpUVVWxgp6nikq04UMqlSI9\nPR1RUVFmnUj81pOPLJ0txvARWlJSEpNmuROUiNvb29He3s6q4lRuR3OA7npdrVaLyspKmEwmZGVl\nudQYYplvVSqVAGAWEQcHB6OxsREdHR0enXwBgDnCpaSkIC4uzuZ3mtYQLB3NHG3qUKlUKCsrcypH\nrNFoUFRUhMOHD2Pjxo0YM2ZMv8/3EmLwk25NTQ2r7vJbTIPBgBMnTmDKlCl9HoNuvUwmE7RaLUaM\nGAE/Pz+bX0xbRTKtVgu5XM5Ijcp8wsPDERQUBIlEApVK5ZHiFb8+qht1pNjDG7vQiw1ALyK2FuVR\naVZYWJhHonUKmufUarUslaDRaMy2+ZY3PVeImK/i9zVJ2BXw73VnZyckEglzPaM3kP66mVlCq9Wi\nvLwc3t7eyMzMdDqFYNnUwefj+Yg4ICAAIpEIHR0dyM7OdljSdfToUSxatAhz587FokWLLtfolsfg\nJ12aXrCEM566arUaGo0GISEhOH78uM3nOFsko7lWmhOkRB4UFGQWpbmLrKgONiIiAqmpqS4fl4/S\n6IVGW0dppCMWi1mhw1NdXnwjhSOpBMsozdHdB/BrQY7mvD1Vxdfr9ewGkp2dDX9/f6uFL2cMaayB\n3nwbGxuRnp5u1oHoDvC+HhKJBDKZDH5+foiJiXFIdtfd3Y3Vq1fj6NGj2LhxY7+HPg4gDH7StWVk\nDvTMrneEdPkuJWvPccWUhq/g880NfNFLLpdDqVTCaDSybacrudbu7m422ywzM9MjKQuj0Qi5XA6R\nSMSmZ1jmiF1tMrAGaqvZHxLktc+U0CyJODAwEI2NjSzP6amCHN/p1ZesjU8D0dQELYw6YitJc8S0\nAchT0aPJZEJtbS2kUilycnLg7+9vVXZHc9t6vR7x8fE4ffo0lixZgvvuuw+FhYWDIbrlMfhNzN1x\nkVseg5Kwqw5gCoUCVVVVCAgI6DWc0bLHHTC/yFpaWnrl/8LDw61u8Q0GAzPGTk9PNzO2cTdkMhkz\nFB83bhy8vb1hMBgYodXW1rImA56InbWT5FMJfRn69AW+9Za2s1IilsvlaGlpYVv8kJAQiMXiPiNi\nV9DV1YXy8nKmpulrB+Lq3LqQkBC0tLSgvb0d2dnZHpt+AfTsDMrLyxEbG4vc3Fz23YyKijKThRkM\nBrZr2rp1K7Zv3w6JRIIbb7wRQUFBUKvVHl3nQMKgId2+4GyvvY+PD/R6Pby9vZ0mW41Gg+rqami1\nWmRmZjqsfeQvMjqtlN/iNzY29tria7VatLa2YuTIkcjPz/fYXKuuri5m5G7pNubj49PrIuP1uLQv\n3xE7ST6VYG0EubsgCAIMBgOampoQHByMadOmwcfHxywibmpqMhv17ioRm0wmNDQ0oL29vd+FMntz\n65RKJVpaWtDR0cGsHCUSCQwGg0vNM/ZgNBpRU1MDhUJh1djHEj4+PoiIiEBZWRn27NmDp556Co89\n9hjOnz+PEydOuG1dlwMGTXqBEAKdTmf1b6WlpZg8ebLD2xdCCE6cOIGkpCSEh4c7NG0XcK65oT8w\nGAwQi8Wor683M1SnpBAeHu6W0eMAzIYaUm9hV9FXqzDQM9m5L++H/sJoNDJZW1ZWlt1ij2Vqgi+M\nWlbyrUEqlaKyspKlljx1U+RJMCcnB0FBQWaaXNo84465dbRLjmrKHbUiXbVqFc6cOYP3338fmZmZ\nrpzm5YTBn9O1R7onTpxATk6OQ109tEjW2dkJsVjMZk7Rgpc1qQwvl0pMTERCQoLHLi5qgWcwGJCZ\nmckiDD6ypD4CPKGFh4c7FaFRB7a6ujqPnhPVRTc0NECj0cDX15fNUfPEFp9uyRMTEx0mDEv0RcQ0\niheJRNBoNB4bm0RBdcT0c3Kki9FSk8vfQGw1sljaYzp6PR0+fBhLly7FQw89hCeffNJjN9MBhiub\ndM+cOYPU1FS723x7eVsaoVEZGO8E5uXlhba2Nlbo8ZRcio+iHa1CW2sw4InBlnEOdc0KDg5GWlqa\n2zuUKKhdJZ3wS6VZfRGaPWKwhe7ublRUVMDHxwcZGRlun+DAa1vFYjEkEglrqOnPuu1Bp9OhsrIS\nBoMB2dnZLumI+XVTMrZ8v0NDQ5kpkjMabLVajZUrV+L8+fN4//33kZ6e7sppXq4Y/KQL2LZ3LCsr\nQ1xcnFVNrKuKBCrLMhgM8PX1NWu3DQ8Pd0neY+u1aLW7Py3C9Fj2jHOokQt15/JUHz7gnCrB1rot\nI2JrNweaT21ra7PZ8+8u0EJZQEAAMjIy4OPjY7XJoL83EH4X4oluPPp+K5VKSKVStLW1wWAwmDnd\n2duBEELw448/4vnnn8ejjz6KJ554wq3RbWNjI+bNm8dMqR5//HEUFhZCIpFg7ty5qK+vR0pKCrZv\n386GcxYWFmLXrl0ICgrCli1bHG6W6geuDNK15TRGHel5I2ZXFQk6nQ61tbVQKBRmzQ20mMETA9/l\nZUt5YA+06SA0NBSjRo3ySMRJ111fX4/Ozk74+vqaeQjQH3ddNFqtFlVVVdDr9f2yd7Rl+MM7r9Hc\nbWxsrEfzqc4UyhyJLO0RcX+8bp1FR0cHqqurkZKSgtjYWDPTckv9c1BQEJRKJeLj47Fy5UpUVlbi\n/fffR1pamtvXJRaLIRaLMXnyZCiVSuTm5uKrr77Cli1bEBUVhWXLluH111+HVCrFG2+8gV27duHd\nd9/Frl27UFJSgsLCQpSUlLh9XRa4skm3vr4efn5+GDFihMt2i/wkXHstlJbPUalULC1BlQd9aVqp\n+sHTTQfAr4UevnjlSJuws2Yu1lIJ7i4yUu2zRCJBY2MjdDoda+Pm33N36kHpLLT+FMrsRfK8Hre9\nvR0tLS0ej9jpiB5CCLKyshwyK2psbMTixYtRW1uLYcOGYc6cObjlllswY8YMj62T4vbbb8fChQux\ncOFC7N+/H/Hx8RCLxZgxYwYqKiowf/58zJgxA/feey8AICsriz3Ogxj8Ol0ATFNrCSr/suwkc+QC\n4ZsbYmNjnZqEa218N7UJlMvlTNNKC3UhISGss4fmbT1l4qLRaJivwLhx48wiTmvSNZPJxCIcOsKb\neh/wNxBr7ykl9mHDhiE/P9+jhRSJRIKmpiakp6dj+PDhZjsQXtfanyYU4NeOMo1G0+v9cxb8lBFr\no+nb29tx9uxZCIKA8PBwyGQyGI1Gt+eIAbDxVaNGjXLIQtHf3x8BAQHYunUrwsLCcPToUQQEBODE\niRM2033uRH19PU6ePImrrroKbW1tjEjj4uLQ1tYGAGhubmaWqwCQmJiI5uZmT5OuTQwq0rUFOo+M\nOoc509xAJ9RaNjf0Zy2RkZFm+WWtVguRSMSm7gqCgKamJigUCqaacFdqwWg0sq2wM22hXl5ebLIt\nfyx6A6mvr4darTZriggICGDOZv0lpr6gUPRM+Y2IiDC7MQqC0EvXykfyYrGY3Xx4NzBbRMznUx3d\n8bgCQRBYa7BarUZeXp6Zmb1MJoNIJHI4t90XeG8GRxo3gJ734sCBA3jhhRfwxBNP4B//+Ae76d56\n661Or8FZqFQq3HXXXVi7dm0v2Z+j1/ilwKAmXZpGoFuz48ePQxAEs+q9tU4pfnvv6YISJfbg4GBM\nnTqVjXmnF5dUKkVDQ4NZvtKVQh0fscfHx2PKlCn9znFSAT6fw9Tr9ZDL5WhsbIRMJoOPjw+CgoLQ\n3NxsU3LXH9Bpwmq12uEpv7YiedqEYq0bkKYlKioqEBAQ4DAxuQqatrAcrW4tIu7u7mZFLzoy3VEi\n5gu16enpDpv7KJVK/OUvf4FIJMK///1vJCcnu+fEHYRer8ddd92FP/7xj7jzzjsBALGxsRCLxSy9\nQGs4CQkJaGxsZM9tampin/ulwKDK6fKeuraKZEaj0Uz+pVarmZ41JCQEcrkcMpnMo80NQE9kQc23\nHela60+hjs6Fc8RQvL/gUwkpKSnw9vaGTqczc12jBRhe++xKBZ+ShaemX9BuQNoqrFarERgYaDar\nrq9pt86C18K6qu/lidie2sNkMqGsrMypohwhBPv378eLL76IhQsX4pFHHvFYgdLeGh544AFERUVh\n7dq17PdLly5FdHQ0K6RJJBIUFxdj586dWL9+PSukPf300ygtLfX0Mq+MQpper4fBYHC6SKbValFX\nV4fW1lYzBzB6Ybmz+GI0GiESidDW1tbvglJfhbqgoCC0trZCqVQiMzPTo73t1HvWaDQiKyvLrnCe\nby6ga7ec+RYWFtankYunzb6BXyPOmJgYpKSkMMNvy4kR7jD8oWoBT9xELNUeHR0d0Gg0CA0NxbBh\nwxxKTSgUCvzlL39Bc3MzNm7ciKSkJLetDwAefvhhfPvttxg+fDjOnj0LAJg7dy4qKioA9HwWERER\nWL9+PaZNm8ZSMABw3XXXYevWrbjnnnsgEomQnJyM7du3IyoqCoQQLFy4ELt370ZQUBA2b96MvLw8\nt67dCq4M0l26dClCQkKQl5eH3NxchIaG9vnFpZMbIiMjkZKSwoZPdnd3m0VntPjCWzE6c4cnhKC9\nvR11dXV9TiDoD6j5TFNTEy5cuAAfHx8EBgba7ajrD6iqQywW98t7licF+r4bDAYEBweztQcFBaGx\nsRESiQRZWVkevYnQ6Qo04rTnLcDPUKO7J2cMf7RaLSoqKiAIArKysjzWjAL0SM7Onz/PnMf41myl\nUml286MprPDwcOzbtw/Lly9HYWEhHnroIY98d3/44QeEhIRg3rx5jHR5LF68GOHh4Xj55ZdRX1+P\n2267zerjBgiuDNKtqKjAkSNHUFJSghMnTkCn02Hs2LHIzc1Ffn4+xowZw6IiqVSK+vp6h8eO8zk/\nuVzOqvd8NGzrwlIqlaisrERAQIDHt/dU2xseHs465Ox11PWnUCeRSFBVVYWYmBhmWelO8CkVOg7J\n19cXUVFRZmbf7nxddxXK+KnCtC2b98egNz+xWAyRSORUPtUV8Mbs9kbnWEbEzz33HMrKyqDX6/HA\nAw9g5syZmDVrlsfSbrbIlBCCpKQk7Nu3DxkZGUOkO1Ch0Whw6tQpHDlyBEePHsW5c+fg6+sLX19f\n+Pv7480330R2drbLd23e0lAul7PZabzpjEgkQldXFxtm6CnQHDE/VcEWLHWhcrncbJJwX4U66v/g\nSCqhv9BoNKioqICXlxcyMzPh6+vb6+YHwCy37WqelTYe+Pn5ISMjw+0RJ2/4LZVKWaExNjaW5Ynd\nuQuhUKvVKCsrY3P4HLlJEULw3Xff4eWXX8YzzzyDa665BidPnkRZWRlWrlzp1vXxsEWmP/zwAxYt\nWoRjx46xx40ZM4ZdV6+++iqmTZvmsXW5gCuTdC3xr3/9CytWrMCtt96KgIAAHDt2jJnU5OfnIzc3\nF3l5eYiMjHT5i6/VaiGTydDY2AiFQsHsDPntvTvF+XTqcWtra79yxI4U6oKCgtDU1ITW1laPTCHg\nwZ9XRkaGXY9gKl3j86ze3t691m7PLJy+lqcbD+hrtbW1sY48S+c1R6dcOPNazvjqymQyvPDCC5BI\nJHjvvfcuaqXfFuk+8cQTSE9Px+LFiwH0XGcqlQrR0dE4fvw4fve73+HcuXMeDWycxBDpAj0i6aio\nKLPIjI7WLikpQUlJCY4dOwalUomcnBxGwhMmTHDITMSykYLmbfkJETyZ8VtkVyKzzs5OVFdXm02k\ncCf4Ql1HRwebFBETE8PW7s4pERRUAUGLV668N/wuhOZZLW+AAQEBUCgUKC8v79drOQr6WrT7z9Zr\nWZv75qxfg1KpRFlZWZ+vxYMQgv/+979YsWIFlixZgvvvv/+iKxOska7BYEBCQgKOHz+OxMREq8+b\nMWMG/va3v12MApmjGCJdZ6DX6/Hzzz8zIj5z5gx8fHwwefJkTJ48GXl5ecjIyDAjOSrL8vPzQ3p6\nul2S5ru76BaZL7z05YdLDcVpPtoVdylHQTvXCCFse08bIvhcpTsKdXRaBNVHu7uZgi8ayWQyKBQK\nAD36zujoaGZ/6W5Qr1u5XO6wlpiHLStJa1pck8nE/I9zcnIc1phLpVIsW7YMCoUC//jHP1gjycWG\nNdLdvXs3ioqKcODAAfa7jo4OREVFwdvbG7W1tZg2bRp+/vlnj+5SnMQQ6fYHVCZ07NgxlJSUoLS0\nlEWYo0ePRnV1NXJzc/Hkk0+6XFGnhReezHgtKzVTr6+vh0Qi8egkYcB8y91XKqG/hTpCCJqbm9HY\n2OgR1yzL12pvb0dtbS2Sk5MRGRlpFlXaIjNXIZFIUFlZ6ZTht6PnYenXoNFooNPpEB4ejpEjRyIi\nIqJPOR0hBP/5z3+wcuVKPP/887jvvvvcHt1ak4KtWLECH3zwASserl69Glu3bsX+/fvR3t4OQRAw\nbNgwfPzxx9i2bRsKCgqwYMECdsx//etfePnll5lB08qVKzF79my3rrufGCJdd4MQgtWrV+PDDz9E\nbm4uOjs72XQFKlmbPHmy07PBeNCLim7vu7u7ERwcjLi4OEZmnvAxoKbYrrpzOVOoUyqVKC8vZ0Ue\nTw4ndKRQZsu9jK7d0by8Xq9HZWUl9Hq9y163joK6qUmlUowaNcrM0J7K7nifCUrEEokEzz//PLq7\nu7FhwwaPeRFYk4KtWLECISEhWLJkidljz58/j3vvvRelpaVoaWnBDTfcwHZ1lxmuDMObiwlBEDBx\n4kScO3eObYONRiPKyspQUlKCL7/8Ei+99BKMRiPGjx+PvLw85OXlIScnx2FiCQgIgF6vh0gkQlRU\nFEaNGsWiytbWVlRVVYEQ4pbKPWCeSpgwYYLLqgRbBi68/KuyshIajQYAMGLECMTGxnrUetHRQhlt\njAkKCkJcXFyvtbe3t6O6upqNR7c0zfG0160lZDIZysvLMWLECOTn57PXogRqzfDnzTffhEKhQHV1\nNR577DEsWbLEo63u06dPR319vUOP/frrr/GHP/wB/v7+SE1NRXp6OkpLSzF16lSPre9iY4h0+4Hf\n/va3Zv/v7e2NsWPHYuzYsXjkkUdY1HT8+HGUlpbirbfeYlEd1Q7n5eVZHYWj0+lQU1ODrq4uM/8H\nalVIc258q2pDQwPLD/Npib5yrLzRd19KAVdBjWfoTUEqlSItLQ2hoaHMGtCyo64v1YEjkMvlqKio\nQHR0tMt+E/ZMc2iLMLW/NBgM8Pf3Z766niJco9GI6upqqFQqjB8/3mb+23LtFy5cQHBwMHx9fXHX\nXXehqqoKN998M/bs2eOxsfO2sH79enzyySfIy8vDW2+9hcjISDQ3N6OgoIA9hjqCDSYMpRcuMggh\n6OzsZEW60tJSNDc3IyUlBXl5eZg4cSKOHDmC5ORk3HTTTYiNjXX6wqXbS5pj7e7utplj7W8qwRnw\nkxXS09Otbu9568v+FOqoh4Fare6zo6y/4CcYU3mVu1uEedA8cV8z0SzX+O9//xurV6/G8uXLMXfu\n3IvqwmVZIGtra2PeJi+99BLEYjE2bdqEhQsXoqCgAPfffz8A4JFHHsEtt9yC3//+9xdtrW7CUHph\noEAQBMTExOC2227DbbfdBqAnaqqursaWLVuwYMECJCYmwmQy4ccff2RpibFjxzpcWff19UV0dDSL\nWGn1Wy6Xs048nU7HRg2lpaUhOjrao5Nq6WSKviYrWLO+5At1LS0tdgt1loWyrKwsj5KLSqViuxdr\nXsu8dI33T+aJ2NHJzQaDAZWVldBqtZg4caLDeeLOzk4sXrwYgiBg7969ZhNULhV4r97HHnuMXQsD\nzRHMExgi3QEALy8vZGRkQK/X4/Dhw0hNTYVOp8OpU6dQUlKC999/H2fPnkVAQAAmTZrEiHjUqFEO\nEaUgCAgICEBAQABiYmLQ0NDAmikEQcCFCxdYzs0yP9xfwqJa4vj4eOTn57tE7H5+fhg2bBhTUFha\nX9bX10Ov1yMgIABdXV0ICAjAxIkTPdopx0uzsrOzbYryfXx8EBUVZZZHpp1pcrkcra2tDjVEdHZ2\noqqqyikzHEIIvvrqK7z++ut46aWXcPfddw8Yj1lqwQgAX375JcaOHQsAmDNnDu677z4sWrQILS0t\nqKqqwpQpUy7lUt2OofTCZQJCCGQyGY4ePYqSkhIcPXoUtbW1SEhIwOTJk1lHnT07SppKsGW4w5uS\n04YCftY1VA8AAB33SURBVGvvzBh3WpQDgMzMTI9W72mhrLm5GcOHD2f2nYQQVuxyZUadLVDnMb4B\npr+gOxFeuhYQEMDsRr29vTF69GiHdzvt7e1YvHgxfH198e6777rd18GaDGzp0qX45ptv4Ofnh7S0\nNGzevBkRERGYM2cOvvnmGwA9u7Crr74aI0eOxKlTpyAIAlJSUrBx40ZGwq+99ho2bdoEHx8frF27\nFrfccotb136RMCQZG4ygDl9HjhxBaWkpjh49CplMhqysLFaomzBhApqbm1FZWYmRI0c6TYDWNLhU\nx0rJmNeC8q5jnm4VBswLZdS/l18Lr8Htb6GOzxPn5OR4dBIG1S7X1dUhNDQUBoOBGdnzTneWOlxC\nCL744gsUFxdjxYoVuPPOOz0S3VqTge3ZswfXX389fHx88PzzzwMA3njjjcvBnMYTGCLdKwUGgwHn\nzp3DkSNHcOTIEezZswcAMHPmTFx99dXIy8tDVlaWy7pHurWnJCyXy5ntpa+vLzo7OxEbG9vnWPX+\nghKgSqVCTk6Ow4UyVwt11Os2KSkJI0aM8Og2XafToby8vJfVIx28Sd93pVLJ3vv9+/djxIgR2L59\nO0JDQ7Fu3TqP3/DskemXX36JHTt24NNPPx0iXcs/DJHu4MWf/vQnjBs3Do888ghraz569Cib/pub\nm4vc3FxMmTKlX7O+6HwttVqNkJAQdHd3OzQWyVVQvam7CNBeR11wcDDa2toYAXrSltOV0Tk0ml+1\nahUOHjzIOtLy8vLw97//3aM3B3tkOnv2bMydOxf333+/2xzB6GCCywRXBuk2NjZi3rx57CJ5/PHH\nUVhYeKmXdclgMpms5hvpxV1aWspsL2m7L01LTJo0CSEhIXa/5IQQtLS0QCQSITU11UzeZm8sEm84\n4ww0Gg3Ky8vh4+ODzMxMj5l902i+vr4ebW1tjGgdtb50BfTcfH19mceFI2htbcWiRYsQGhqKtWvX\nIjo6ms34o8UpT8EW6b722ms4duwYvvjiCwiC4BZHMKPR2GvnNMBJ+MogXbFYDLFYjMmTJ0OpVCI3\nNxdfffUVRo8efamXNuBhNBpRWVnJ8sMnT56ETqfDuHHjGBGPHj2akUFnZyfq6+sRGhqKtLQ0hwiI\nn5Uml8uh1WodGovE54k91bzBg+qJAwMDkZGRAR8fn37NqLMH/saVmZnp8LmZTCZs374da9aswauv\nvoo5c+ZcdAKyRrpbtmzBxo0bsXfvXps5b2ccwXhilclk2LBhA5KSkvCnP/3JPSfhOVwZpGuJ22+/\nHQsXLsSsWbMu9VIuS2g0Gpw8edLMBD4wMJCZjKxZswaZmZkuV+8dGYtECEFlZSWioqI8nifm24Xt\nTVfgH9+fQl13dzfKysrYrDdHI+fW1lYUFhYiKioKa9asuWTOWpaku3v3bixatAgHDhwwS424wxGs\ntLQUzz33HCZOnIjvvvsOd999N1asWOHuU3InrjzSra+vx/Tp03H27NmBZGx8WePHH3/EggULcP31\n1yMiIgLHjh2DSCRCUlISM/nJzc3tlwk89fCVSqVobm5maonIyMg+xyL1B674z1qDLR9fy8nHzc3N\naG5uRlZWlsNucSaTCZ999hneeecdrF69Gr/97W8v2fb63nvvxf79+1nhdOXKlSgqKoJWq2XRekFB\nAd577z2nHcEs02I7d+7Ee++9h4kTJ+Kvf/0rKioqMHfuXGzYsAHXXHONx8/VRVxZpKtSqfCb3/wG\ny5cvx5133tnv4xmNRuaR8O2337phhZcnxGIxfHx8zKIY2iTAm8CrVCqMHj2amcCPHz/eqfytZaGM\n5of5sUh+fn4IDw9nZOZqgYs6dMlkMpe8bh0BX6iTSqVQKBTw8/NDXFwcG9PTV35aLBajsLAQMTEx\nePvttz1m62lNfyuRSDB37lzU19cjJSUF27dvR2RkJAghKCwsxK5duxAUFIQtW7Zg8uTJ/Xp9nnBr\namqQlpYGqVSKF198EXq9HkVFRYiJicHf/vY37Nu3D59//rlHzXr6gSuHdPV6PW677TbcdNNNWLRo\nkVuO+fbbb+PYsWNQKBRXNOk6Cp1OZ2YC//PPP8PX1xeTJk1i+eH09PRe0SQtJnl7e/c5FddyhDvN\nDzszFol6GIwYMQIjR470aNRICGGmQlQFwd9IDAZDr/y2t7c3TCYT/t//+39Yv349ioqKcOutt3p0\nndb0t8899xyioqKwbNkyvP7665BKpXjjjTewa9cuvPvuu9i1axdKSkpQWFiIkpISl16XJ1uJRII/\n//nP0Gg00Ov1WLFiBUwmE7Zt24apU6di7ty5AID8/Hw8/PDDeOKJJ9xz8u7FlUG6hBA88MADiIqK\nwtq1a91yzKamJjzwwANYvnw53n777SHSdQGEECgUCjMT+JqaGsTGxjLf4aNHjyIxMRF33XWXS4Uy\nSw2rvbFIer0eVVVV0Gq1yM7O9mi7MNCz8yorK0NkZKTN1m3LQl1JSQnWrFnDvChee+01XH311R4d\nz05hmavNysrC/v37ER8fD7FYjBkzZqCiogLz58/HjBkzcO+99/Z6nKPQarV477334OXlhaeeegoA\nsGDBAkyaNAnz589Hbm4ucnJysGXLFmzYsAFisRi33347pk6divb29gHhI2EDV4bhzaFDh7B161aM\nGzcOEydOBNDjSH/rrbe6fMxnnnkGxcXFUCqV7lrmFQdBEBAeHo6ZM2di5syZAH7tuNq2bRsWLVqE\nuLg4GAwGHDx4kOWHJ02a5HD+VhAEBAcHIzg4mF30fKFLJBJBpVLBaDRCr9cjLi7uorQnU6Ofvkbn\n8BaMcXFxKC0tRVBQEB566CH4+vrik08+YcWki422tjb2nsbFxaGtrQ1Az8zBkSNHssdRG0ZnzdCD\ngoJw7NgxHD9+HLm5uYiKikJkZCSmT5+OqVOnspvPXXfdhVWrVqGsrAxTpkzx6Mh6T2JQke61116L\nPiJ3p0BzW7m5udi/f79bjimTyfDoo4/i7NmzEAQBmzZtGlQGzY5CEAQkJiZCoVBgz549GDNmDIxG\nI86fP4+SkhLs2LEDy5cvByHEzAQ+Ozvb4Sq/l5cXy/vGxMSgvLwcXl5eiI2NRVdXF6qrq62ORXJH\nNMkPvMzLy3O4MNfU1ISnn34aI0eOxIEDB1we/+QpCILg1vSGv78/5syZg5aWFmzfvh25ubmoqanB\n5s2bsXfvXib3XL9+Pf70pz/hxRdfREpKitte/1JgUJGuu3Ho0CH8+9//xq5du5ir1f3334//+7//\nc/mYhYWFuPnmm7Fjxw7odDp0dXW5ccWXH/7617+yf3t7e2PcuHEYN24cHn30UZYyoCbwb775Jioq\nKhAZGcmUEvn5+XY9ZanXbXNzs02NL/1sZTIZRCIRdDodgoODzfLDjkrVTCYTG50zevRohwtzJpMJ\nn3zyCTZu3Ig333wTs2bNGjDC/9jYWOYKJhaL2ZbeFRtGmrs1Go0ghOCxxx5DUVERbrzxRnz88cf4\n+uuv8cYbbyA/Px/t7e0wGo1488030dbWhjvuuOOyJ1xgkOV0PYn9+/fjb3/7W79yunK5HBMnTkRt\nbe2AuaAuN9Ax97wJfEtLC1JTU1k0PHnyZISFheHcuXNQq9WIjIxEWlqaw8RJyV4ulzOPA0fGItHR\nOfHx8UhKSnL4M25sbMRTTz2FUaNGobi4+JJLHC1zukuXLkV0dDQrpEkkEhQXF2Pnzp1Yv349K6Q9\n/fTTKC0tdeg1aIfZ7Nmz8fnnn0MQBOzYsQPff/89iouLcfDgQfzwww84e/Yspk6dilWrVnnylD2B\nK6OQ5km4g3RPnTqFxx9/HKNHj8bp06eRm5uLdevWXfQxKYMN1ASed1traWmBr68vHnvsMUybNg1j\nx47tV9qAH4tEp0LQsUghISGQSqXo7u7G6NGjHXYfM5lM2LJlCz744AO89dZbmDlzpkdvxlTfSlFb\nW4tVq1ZBJpOxybwNDQ0QBAFKpZLpb3/3u9/hnnvugUgkQnJyMrZv346oqCgQQrBw4ULs3r0bQUFB\n2Lx5s9UuMxrdEkKg0+mwfv161NfX48EHH8STTz6JL7/8EvHx8aiursa2bdugVqvx17/+Fb6+vrhw\n4YLHOxA9hCHSHQg4duwYCgoKcOjQIVx11VUoLCxEWFiY2RbbWaxZswYffvghBEHAuHHjsHnzZo8W\nhwY6DAYDZsyYgVtuuQW/+c1vcOLECRw9epSZwE+ePJlFxP1pggB65InNzc1oaGhghO7o6HmRSISF\nCxciMzMTxcXFHtEH24PRaERCQgJKSkqwefNmq5N53fU6lnabP//8M9577z0QQrBp0yasWbMG8+fP\nh4+PD3788Ud8/PHHmD9/vkNtwgMYQ6Q7ENDa2oqCggI2peHgwYN4/fXXsXPnTpeO19zcjGuvvRbn\nz59HYGAg7rnnHtx666148MEH3bfoyxByubxXAYqawJeWljK3tbq6OiQkJDASzs3NRXR0tMOjc6qq\nqtDd3Y2cnBwEBgaajUXi9bfUKKetrQ3Z2dn4/PPPsXnzZrz11lu4/vrrL0mqac+ePVi5ciUOHTpk\ncxy6uyAWi/Hss89i6tSpSE1NxZw5cwD0pGOuvvpqJCUlISIiAuHh4Vi3bh26urouWWuzG3FlSMYG\nOuLi4jBy5EhUVFQgKyvLrDrrKgwGA7q7u+Hr64uuri42rfZKhrWKvyAIiIyMxE033YSbbroJwK9e\nC0eOHMHBgwfx9ttvQy6XIzs728wE3lLHy4/Oyc7OZqTJj0WyHD0vl8vx3nvv4fDhw9BoNJg9ezZE\nIhH0ev1F0d5a4rPPPmP6WsD6ZF5XwTc6UD+GRYsWISwsDC+88AIyMjKQk5ODpqYmpKSkYNeuXRCL\nxdixYwd7/wYzhiLdi4xTp07h0UcfhU6nw6hRo7B58+Z+fcHXrVuH5cuXIzAwEDfeeCM+/fRTN672\nyoNer2cm8EePHsWpU6fg5eWFSZMmITs7G9999x3mzZuHm266yWFyMBqN+Oijj7BlyxasXbsW+fn5\nOH36NI4dO4aFCxd6dAKzNeh0OowYMQLnzp1DbGyszcm8/YFMJoOfnx/Onj3L9LtPPfUUQkND0dXV\nhUOHDqGrqwt33nknNm3aNBiDBfueqHZ+hjCAIZFIyHXXXUfa29uJTqcjt99+O9m6deulXtaggslk\nIgqFgrz66qskLi6O3HjjjWTMmDHkuuuuI0uWLCGfffYZqampISqViqjV6l4/Z8+eJddddx15+umn\niUqlutSnQwgh5KuvviKzZs2y+re6ujoyZswYp49pNBrZv7ds2ULi4+PJk08+Sf7+97+T7u5uUlBQ\nQPbt20eMRiNJT08nRUVFpK2tjWzZssXl8xjgsMmrQ+mFfoJcQiPl//3vf0hNTWWdOXfeeSd++ukn\n3H///U4dxxmTkysNtFPMy8sLZ86cQUxMDAghEIvFzAT+/fffR3t7OzOBz8vLw4QJE7Bt2zZs3boV\n69atw7Rp0waMTHDbtm1mqQVbk3kdAS2UeXl5obOzE2KxGBcuXMB///tftLW1YenSpTCZTJg5cyau\nu+46lJeXIy8vDz/88AOWLFmCBx54wO3nN+Bhj5Evwd3hsoXJZCKEmN/xPY0jR46Q0aNHE7VaTUwm\nE5k3bx555513nD7OgQMHyPHjx80inKVLl5KioiJCCCFFRUXkueeec9u6ByMMBgM5d+4c2bRpE5k/\nfz5JTk4md999N1Gr1Zd6aWZQqVQkKiqKyGQy9rv777+fjB07lowbN47Mnj2btLS0OH3czz77jGRk\nZJDJkyeTO++8k2i1WkIIIevWrSOpqalkzpw55IknniCJiYnk66+/dtv5DGDY5NWhnG4/cObMGSiV\nSuTl5Vm1FuT1iZ6Kcl555RV8/vnn8PHxwaRJk/Dhhx+6ZHPoqMnJEByDJz9zS6SkpCA0NBTe3t7w\n8fHBsWPHLtpORafTYeHChVCr1Vi/fj2+++47fPnll3jqqadw9dVXQ61W46GHHgIAPPnkkwgLC8Ok\nSZPcvo4BiCHJmLvxzTff4ODBg6irq0NDQwNWrVqF8ePHY9++fZg5c6ZN0w+TyYRjx44hJCRkQI0R\nsiTdiIgIyGQyAD0EEhkZyf5/CAMLKSkpOHbsmNn0X1t2jJ7AggULsHfvXlRVVQEAnn/+efj5+eHh\nhx9Gamoqjhw5gjNnzuDxxx/3yOsPUNgk3YtbNh0kaGxsxNq1a6FSqfDPf/4ThYWFaGlpwYsvvohn\nn30WRUVFSE1NxY4dO3D+/Hl89tln6OzsBNCTK92xYwf++c9/AgDrQR/IcNbk5OGHH8bw4cPNcoNL\nly5FdnY2xo8fjzvuuGOIwD2Mr7/+muVLH3jgAXz11Vcee63i4mJER0fj/fffBwA8+uijqK6uxu7d\nu9Hd3Y2CgoIrjXDtYoh0XcCJEycQHx/PJlS88MILOH36NNMkvvPOOygsLMSaNWuwb98+bNy4EYsX\nL4ZWq4VEIoG3tzcTf3t7e0MQBDPiNZlMMBqNl+r0APxqcgLAzOTEETz44IPYvXu32e9mzZqFs2fP\n4syZM8jMzERRUZFb13slQxAE3HjjjcjNzWXEZ8uO0ROgXZX/+Mc/0NjYiIyMDNxyyy1ITk72uFfx\n5Ygh0nUBkZGR6OjowCeffIIDBw6gqqoKq1atglqtxmOPPQagZ0s+atQoPPjgg/j+++9RVlaGyspK\n6PV6HDp0CLt378aYMWOwYMECyGQyCIIAk8kEoMeS0NKchZJwd3f3RfH2nTNnDj7++GMAwMcff4zb\nb7/d4edOnz69V0fRjTfeyCwZCwoK0NTU5L7FXuH48ccfceLECfznP//Bhg0b8MMPP5j93d12jNYw\na9YsXH/99Zg3bx4AYN68ef3ysR7MGCJdF5Ceno6QkBB8+umnqKqqgp+fH5qbmxESEoJhw4ZBp9NB\nKpViypQpzPykqqoK6enpqKmpgUQiwaZNm3Du3DmcPHkS586dA9Aj15k9ezamTp2Kt99+GxqNhr0m\nJeH6+no8++yz2Lt3L4CeqJiStau49957MXXqVFRUVCAxMREfffQRli1bhu+++w4ZGRn43//+h2XL\nlvXrNXhs2rQJt9xyi9uOd6WD2ikOHz4cd9xxB0pLS/u1U3EVy5cvZ2mEgZ4yu5QY0um6gBEjRqCw\nsBBr167F66+/jttvv521mQI9OV+1Wo2cnBx4eXnh5MmTCAsLQ2BgIGpra3HbbbchNjYWer0eeXl5\nEIlE0Gq1WLduHXbu3Inu7m689NJLqK2txejRo/H999/jgw8+QGZmJjIyMuDt7Y3k5GQAMOtmevDB\nB5n/gjPYtm2b1d9TYncnXnvtNfj4+OCPf/yj2499JUKtVrOxRGq1Gnv27MHLL7/MdirLli1zeqfi\nKqKiopj+d6BokgcihkjXRUyfPh3Tp08H0DPnqby8HNdeey0AoKWlBQqFgpHw7t272fign3/+mY1t\naWpqQmxsLJqbm3H+/Hk0NTXhpptuQlxcHBoaGrBv3z54eXnh0UcfRVFREcrLy7Fjxw7m1woA27dv\nR2JiIq6++mrs3LkTTz/9tNk6+ZTFpcaWLVvw7bffYu/evU5flNYaOCjeeustLFmyBB0dHWYV/CsB\n1Nwb6PHhuO+++3DzzTcjPz8f99xzDz766CNmxziEgYEh0nURJpMJhBB4e3vD398fEyZMwIQJE0AI\nYf6tNIfZ1NSEmTNn4sKFC6itrUV0dDT0ej3+9a9/oby8HEuWLMEXX3yB4uJi3HnnndizZw9aW1sR\nFRWFvXv3Ys6cObjnnnugUqkwa9YsTJ8+HYQQPP/88zh9+jQEQcCECRNgNBrZCGyFQoGwsDCrZGsw\nGODj44OmpiYMGzbsohiM7N69G8XFxThw4IDDfrM8HnzwQSxcuJDlDCkaGxuxZ88edhMaDGhsbMS8\nefPQ1tYGQRDw+OOPo7CwECtWrGC+t8Cv8/9Onz7d6xjR0dEe2akMwQ2w1zlx0Xo3BgFsdaLRTjVC\nCFGr1eTAgQPkxRdfJOPHjyd33HEH+emnnwghhDz99NPklVdeMXuuTCYjzzzzDPn0008JIYRUV1eT\nRYsWkZ07d5JvvvmGXHvtteyx9913HykoKCCEEHLu3DnyzDPPkPHjx5PZs2eTkpISq2t7/vnnyauv\nvkoMBoPL520Nf/jDH0hcXBzx8fEhCQkJ5MMPPyRpaWkkMTGRTJgwgUyYMIHMnz/f6eNa8wW46667\nyKlTp0hycjLp6Ohw1ylcUrS0tJDjx48TQghRKBQkIyODnDt3jrzyyivkzTffvMSrG4KDGPJe8DRs\nbd+pKsHLywtBQUEsLfHaa69Bp9MxW79FixZhwYIFyMvLw/Dhw7FgwQLMmTMH58+fxzXXXAOgxwS9\nsbERY8eORXFxMZusC/Q0M0yZMgU6nQ5vvvkmcnJycPr0aXz66afYtWsXpkyZgvr6emzYsAEVFRW4\n4YYbQAhBYmKizTE29EvibGrCWo74kUceceoYjuDrr79GQkICJkyY4PZjX0rEx8czuVdoaChycnLQ\n3Nx8iVc1BHfh0if6rgDwpEU1uIQQMx/V5ORk/Oc//8FXX32Fxx9/HNnZ2QB6WieLi4vx1FNP4aOP\nPoJarUZCQgKkUikSExPZ80tLSzFr1ix89913aGhowNatWzFz5kysXr0a58+fx7lz51BcXAyVSoWl\nS5fi8OHDqK+vZ5VvYqXaLAiC2doNBgN++uknpra4lOjq6sLq1av7NTvLWhMHALz77rvIzs7GmDFj\nLsnIcx719fU4efIkrrrqKgA9vrfjx4/Hww8/DKlUeknXNgTXMBTpXmTYihrJL736iYmJZmQ6Z84c\npKam4vTp0ygoKEBtbS28vb3x5JNPoqioCFFRUZBIJDh37hymTJmCQ4cOIS8vD8XFxWyAY0REBA4f\nPozQ0FDMmzcPY8aMwd69e3Hs2DGkpaVZXUdHRwd2794NQRAwY8YMtqaQkBC0trayken8eV3MinVN\nTQ3q6upYlNvU1ITJkyejtLQUcXFxDh3DWp74+++/x9dff43Tp0/D398f7e3tHlm/I1CpVLjrrruw\ndu1ahIWF4YknnsBLL73EfG8XL17cb9/bIVx89OW9MIRLAEEQBNLHByMIgh+APwOYAEAEYDEhJEQQ\nhHQAWwD8nhDSyj3+PgATAbxKCFEIgrAQQCyAtwghMu5x3oQQoyAIWwBIAYQDyASwCEAVgHEAfgLg\nRQjR2VibFwAQQvonIO593BQA3xJCenkPCoJQDyCPENLZn2MKgrAdwPuEkP/1c7n9giAIvgC+BfBf\nQsjbVv6eAhvvxRAGNobSCwMQloQr9MCLktkvj9ERQtYSQh4ihLwCgIbHdQD+D8D3giAcFQRhgyAI\nwwDUAphBCFH88rgbAHTyhPvLcWn4mgOgihDyMIDphJBSAHkAFgMYBuBVQRDOC4Lwd0EQnhYEIZE7\nhskDhLsNwGEAWYIgNAmC4P4kcQ8yAUwTBKFEEIQDgiDke+h1bELo2TJ8BKCMJ1xBEHgXpTsAnLV8\n7hAGPobSC5cBfiHhXpGvIAjeAKg8QiYIgtcvpPkegPcEQcgEMJwQ0ikIgglAlSAIB9BDzKMBfGLn\nZf8M4BFBEF4CsBFAO4AY9ETVMkLIc4Ig/APAzQA2AJAD+FgQhAUA0gCUAfgnIcQtPcuEkHv7+HuK\nO14HPddEFIACAPkAtguCMKqvnYebcQ2APwH4WRCEU7/87kUA9wqCMBE934V6APMv4pqG4CYMke5l\nDC4qpf9vAnq2979Em5UAKn/5m0QQhHnoIZIRAL4H0PzL483SGYIg+BFCjgM4LgjCevSQ880AEgDI\nCSG0P1kBYDqA5wghHwuCsApAA4Af0RNJBwqC8PeLTFj9RROAL35Zc+kvN6thADou1gIIIT/CujXg\nrou1hiF4DkPphUEIjnwFi98bCSFHCCFfEEI+JoSU/PInL0EQvAVBoDfhBwRB2CUIwj0AVAAaBUEI\nRE+kW/nLsX8H4F8AjhBC/iYIwlgAfwRwP3pyxaUAllxmhAsAXwG4DgB+2Sn4AXAqTzyEIdjDUKQ7\niGGN8H4hYq9fimVBhJAuy4gZPcSjRE+02gbgLwAyAKQC+FwQhD+jZwv8DCGEbn+nADgC4B0AtwKY\nhJ68sj8hROuB0+s3fskTzwAwTBCEJgCvANgEYJMgCGcB6AA8cBneOIYwgDGkXrhC8UtU+xZ60g1V\nAI6ihzSP2yDrEPQU0srQQ0xTAPyAnvzwuwCyAcwD8CAhRH8xzmEIQ7gc8f8BFpgNz4UEqr0AAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbCXD4YRnMff",
        "colab_type": "text"
      },
      "source": [
        "# Lab7.3 LSTM for IMDB positive/negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEcIOaP_nHvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras import preprocessing\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 100\n",
        "embedded_size = 20\n",
        "epochs = 3\n",
        "\n",
        "def runLSTM(printme=False, embedded_size=embedded_size, maxlen=maxlen, epochs=epochs):\n",
        "\n",
        "  # Load imdb dataset and print a few samples to check.\n",
        "  #\n",
        "  # IMDB: sentence (x) -> positive/negative (y)\n",
        "  #\n",
        "  # “The food was really good” \t\t\t\t -> pos\n",
        "  # “The chicken crossed the road because it was uncooked” -> neg\n",
        "\n",
        "  (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "  # x_train has a size (training_size, ). Because the sentences have variable size,\n",
        "  # we cannot represent this in matrix format.\n",
        "\n",
        "  if printme: print(x_train.shape)\n",
        "\n",
        "  # The first step is to make the column size constant.\n",
        "  #\n",
        "  # We do that by \"padding\" the sentences. If the sentences are bigger, we clip them.\n",
        "  # If they are smaller, we insert a \"NO_WORD\" token to the sentence.\n",
        "\n",
        "  x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "  x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "  if printme: print(x_train.shape)\n",
        "\n",
        "  # Let's see the first sentence\n",
        "\n",
        "  if printme: print(x_train[0])\n",
        "\n",
        "  # Input shape should be now (training_size, maxlen)\n",
        "\n",
        "  # Let's use an embedding to try to help to estimate \n",
        "\n",
        "  xi = Input(x_train.shape[1:])\n",
        "\n",
        "  # Embedding input is (training_size, maxlen)\n",
        "  # Embedding output is (training_size, maxlen, embedded_size)\n",
        "\n",
        "  x = Embedding(max_features, embedded_size, input_length=maxlen)(xi)\n",
        "\n",
        "  # Using LSTM to classify sentence as positive or negative\n",
        "  #\n",
        "  # “The chicken crossed the road because it was uncooked”\n",
        "  #\n",
        "  # h0 -> The \t\t-> h1\n",
        "  # h1 -> chicken \t-> h2\n",
        "  # h2 -> crossed \t-> h3\n",
        "  # h3 -> the \t\t-> h4\n",
        "  # h4 -> road\t\t-> h5\n",
        "  # h5 -> because\t\t-> h6\n",
        "  # h6 -> it\t\t-> h7\n",
        "  # h7 -> was\t\t-> h8\n",
        "  # h8 -> uncooked\t-> h9\n",
        "  # h9 -> pos\n",
        "\n",
        "  # return_sequences: Boolean. Whether to return the last output in the output\n",
        "  #     sequence, or the full sequence.\n",
        "\n",
        "  # return_state: Boolean. Whether to return the last state in addition to the\n",
        "  #     output. The returned elements of the states list are the hidden state\n",
        "  #     and the cell state, respectively.\n",
        "\n",
        "  #\n",
        "  # What's the difference between return_sequences and return_state?\n",
        "  #\n",
        "\n",
        "  x = LSTM(32)(x)\n",
        "\n",
        "  #\n",
        "  # Try to get accuracy on validation set over 90%.\n",
        "  #\n",
        "\n",
        "  x = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "  model = Model(inputs=xi, outputs=x)\n",
        "\n",
        "  model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "  model.summary()\n",
        "\n",
        "  history = model.fit(\n",
        "        x_train, y_train, epochs=epochs, batch_size=32, validation_split=0.2)\n",
        "\n",
        "  p = model.predict(x_test)\n",
        "  if printme: print (\"Loss: %f, Accuracy: %f\" % tuple(p))\n",
        "\n",
        "\n",
        "  #Return accuracy\n",
        "  res = model.evaluate(x_test,y_test)\n",
        "  return res[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzbpI179polO",
        "colab_type": "code",
        "outputId": "5b11f0e4-749d-4f6b-ee6f-3e6b70a84918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  res = model.evaluate(x_test,y_test)\n",
        "  print (\"Loss: %f, Accuracy: %f\" % tuple(res))\n",
        "  # Try to change the maxlen or the embedded_size and plot a 3D grpah with\n",
        "  # embedded_size x maxlen x accuracy in a python jupyter notebook.\n",
        "  #"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.432305, Accuracy: 0.831800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX7FjvgfuIbi",
        "colab_type": "code",
        "outputId": "ec9acea1-38aa-4897-e389-d8221cc0556a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%matplotlib notebook\n",
        "import numpy as np\n",
        "\n",
        "embedded_size = [10, 20, 30]\n",
        "maxlen = [50, 100, 150]\n",
        "\n",
        "#print (x)\n",
        "\n",
        "X, Y = np.meshgrid(embedded_size, maxlen)\n",
        "Z = np.zeros((len(embedded_size), len(maxlen)))\n",
        "print (\"X:\", X)\n",
        "for i, e in enumerate(embedded_size):\n",
        "  for j, m in enumerate(maxlen):\n",
        "    Z[i][j] = runLSTM(False, e, m, 1)\n",
        "\n",
        "#Z = f(X, Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: [[10 20 30]\n",
            " [10 20 30]\n",
            " [10 20 30]]\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_5 (Embedding)      (None, 50, 10)            100000    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 32)                5504      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 105,537\n",
            "Trainable params: 105,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 59s 3ms/step - loss: 0.5011 - acc: 0.7426 - val_loss: 0.4134 - val_acc: 0.8074\n",
            "25000/25000 [==============================] - 25s 1ms/step\n",
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 100, 10)           100000    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 32)                5504      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 105,537\n",
            "Trainable params: 105,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 118s 6ms/step - loss: 0.4518 - acc: 0.7768 - val_loss: 0.3589 - val_acc: 0.8388\n",
            "25000/25000 [==============================] - 49s 2ms/step\n",
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, 150, 10)           100000    \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 32)                5504      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 105,537\n",
            "Trainable params: 105,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 175s 9ms/step - loss: 0.4572 - acc: 0.7743 - val_loss: 0.3394 - val_acc: 0.8494\n",
            "25000/25000 [==============================] - 74s 3ms/step\n",
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_8 (Embedding)      (None, 50, 20)            200000    \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 32)                6784      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 206,817\n",
            "Trainable params: 206,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 61s 3ms/step - loss: 0.4829 - acc: 0.7570 - val_loss: 0.4230 - val_acc: 0.8058\n",
            "25000/25000 [==============================] - 25s 1ms/step\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_9 (Embedding)      (None, 100, 20)           200000    \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 32)                6784      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 206,817\n",
            "Trainable params: 206,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 119s 6ms/step - loss: 0.4450 - acc: 0.7855 - val_loss: 0.3922 - val_acc: 0.8294\n",
            "25000/25000 [==============================] - 50s 2ms/step\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, 150, 20)           200000    \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 32)                6784      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 206,817\n",
            "Trainable params: 206,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 179s 9ms/step - loss: 0.4490 - acc: 0.7883 - val_loss: 0.3262 - val_acc: 0.8646\n",
            "25000/25000 [==============================] - 72s 3ms/step\n",
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_11 (Embedding)     (None, 50, 30)            300000    \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 32)                8064      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 308,097\n",
            "Trainable params: 308,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 61s 3ms/step - loss: 0.4848 - acc: 0.7592 - val_loss: 0.4135 - val_acc: 0.8064\n",
            "25000/25000 [==============================] - 25s 1ms/step\n",
            "Model: \"model_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_12 (Embedding)     (None, 100, 30)           300000    \n",
            "_________________________________________________________________\n",
            "lstm_12 (LSTM)               (None, 32)                8064      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 308,097\n",
            "Trainable params: 308,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 120s 6ms/step - loss: 0.4321 - acc: 0.7922 - val_loss: 0.3528 - val_acc: 0.8482\n",
            "25000/25000 [==============================] - 49s 2ms/step\n",
            "Model: \"model_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_13 (InputLayer)        (None, 150)               0         \n",
            "_________________________________________________________________\n",
            "embedding_13 (Embedding)     (None, 150, 30)           300000    \n",
            "_________________________________________________________________\n",
            "lstm_13 (LSTM)               (None, 32)                8064      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 308,097\n",
            "Trainable params: 308,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/1\n",
            "20000/20000 [==============================] - 176s 9ms/step - loss: 0.4358 - acc: 0.7880 - val_loss: 0.3563 - val_acc: 0.8540\n",
            "25000/25000 [==============================] - 75s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-248be5dea4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Z = f(X, Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfWxaGID59n5",
        "colab_type": "code",
        "outputId": "1461fec7-6f5e-40ec-9a4c-4d2a5f168392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "print (\"Z\", Z)\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
        "ax.set_xlabel('embedded_size')\n",
        "ax.set_ylabel('maxlen')\n",
        "ax.set_zlabel('z');\n",
        "\n",
        "ax.view_init(60, 35)\n",
        "fig"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Z [[0.81304 0.84408 0.84356]\n",
            " [0.81332 0.82816 0.861  ]\n",
            " [0.81344 0.85016 0.85392]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIBhHxJquQCAQjCNC6QoEAsE48j/BWpjucWvtngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIBhHxJquQCAQjCNC6QoEAsE48j/BWpjucWvtngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Xknpiq7MF2",
        "colab_type": "code",
        "outputId": "4c95181f-954b-4df6-a046-e34e7dac964c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Example script to generate text from Nietzsche's writings.\n",
        "\n",
        "At least 20 epochs are required before the generated text\n",
        "starts sounding coherent.\n",
        "\n",
        "It is recommended to run this script on GPU, as recurrent\n",
        "networks are quite computationally intensive.\n",
        "\n",
        "If you try this script on new data, make sure your corpus\n",
        "has at least ~100k characters. ~1M is better.\n",
        "'''\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "path = get_file(\"nietzsche.txt\",\n",
        "        origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
        "\n",
        "text = open(path).read().lower()\n",
        "print(\"corpus length:\", len(text))\n",
        "\n",
        "chars = set(text)\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 100\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"nb sequences:\", len(sentences))\n",
        "\n",
        "print(\"Vectorization...\")\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# build the model: 2 stacked LSTM\n",
        "print(\"Build model...\")\n",
        "xi = Input((maxlen, len(chars)))\n",
        "x = GRU(256, return_sequences=True)(xi)\n",
        "x = Dropout(0.2)(x)\n",
        "x = GRU(256, return_sequences=False)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(len(chars))(x)\n",
        "x = Activation(\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=xi, outputs=x)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "adam = Adam(0.003)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=adam)\n",
        "\n",
        "\n",
        "def sample(a, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    a = (np.log(a + 1e-8) / temperature).astype(np.float64)\n",
        "    a = np.exp(a) / np.sum(np.exp(a))\n",
        "    try:\n",
        "      sample_result = np.argmax(np.random.multinomial(1, a, 1))\n",
        "    except ValueError:\n",
        "      error = 1.0 - np.sum(a)\n",
        "      a[0] += error\n",
        "      sample_result = np.argmax(np.random.multinomial(1, a, 1))\n",
        "    return sample_result\n",
        "\n",
        "# train the model, output generated text after each iteration\n",
        "for iteration in range(1, 60):\n",
        "    print()\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Iteration\", iteration)\n",
        "\n",
        "    model.fit(X, y, batch_size=4096, epochs=4)\n",
        "    model.save_weights(\"weights.hdf5\")\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print()\n",
        "        print(\"----- diversity:\", diversity)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print(\"----- Generating with seed: '\" + sentence + \"'\")\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(200):\n",
        "            x = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            # predict next char\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            # full sentence being generated\n",
        "            generated += next_char\n",
        "\n",
        "            # shift sentence\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # let's consider only one sentence\n",
        "            if next_char == \".\":\n",
        "              break\n",
        "        print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "606208/600901 [==============================] - 0s 1us/step\n",
            "corpus length: 600893\n",
            "total chars: 57\n",
            "nb sequences: 200265\n",
            "Vectorization...\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100, 57)           0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 100, 256)          241152    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 256)               393984    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 57)                14649     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 57)                0         \n",
            "=================================================================\n",
            "Total params: 649,785\n",
            "Trainable params: 649,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "200265/200265 [==============================] - 44s 221us/step - loss: 2.8804\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 2.2758\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 2.0669\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 174us/step - loss: 1.9202\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just on'\n",
            "that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just one and the sour and the soured and a probeding and the prosention of the senting the foren the sour itself the senting the sould to the sour and the will the senting in the his not in the more and the \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just on'\n",
            "that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just oner and at the canser a man experion of the ligetion of the refinest and the extored of the forentions and differengs of the deenther has the was a the sopered more expidion of uther of the counder wou\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just on'\n",
            "that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just ones of batt \n",
            "sequery in renogle-ssation--is\n",
            "con'afingt ald e\n",
            "us the strenglys, whhic ding's, us eletarion, as\n",
            "itreagats.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just on'\n",
            "that should be advised against. with this\n",
            "qualification, the recommendation referred to is a just onoussions evenigion undipathang.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 2\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.8049\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.7150\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.6355\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.5704\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the dart'\n",
            "ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the darter of the strong and the deligious spirit of the subject of the senses of the powers and the problem of the subjection of the subject of the subjection of the subject of the sumple such as the superio\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the dart'\n",
            "ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the darter as in the sumple of the soul of and all that we cruelces and denery of the problem of the great relection of its proble\n",
            "and to subject as the more conscience of the man has peacour\n",
            "happiness, he th\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the dart'\n",
            "ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the darthhins strobgeary sees, perfect\n",
            "itsiquity, consciences and is so alwarding\n",
            "is be true he deer to rematire ,or involunse, knowss\" or of morals\n",
            "with there were conthis open that we stoil, pud shaops,\n",
            "\n",
            "23\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the dart'\n",
            "ly run\n",
            "dry, the first thought is of subterranean demons and their pranks. it\n",
            "must have been the dartht fast.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 3\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.5185\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.4745\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.4358\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.4003\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experi'\n",
            "se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experience to the contrary of his\n",
            "finations and the conscience of the contradical the contradical the reason that is the\n",
            "result of the conscience in the conscience of the more that the contradical of the\n",
            "co\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experi'\n",
            "se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experience.\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experi'\n",
            "se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experients to the fuent) of lang, and the\n",
            "relage that is ut the cover of morality.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experi'\n",
            "se of its supposed vital utility. it is\n",
            "not known that the same degree of satisfaction can be experimons to the thouuase religion of truth.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 4\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.3744\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.3470\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.3194\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.2951\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with r'\n",
            "ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with regard to the sense of the strive of the strive of a long of the\n",
            "strange of the same distrust of the same distrust of the state of\n",
            "perhaps the same distrust of the sense of the state of the sense of\n",
            "th\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with r'\n",
            "ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with regard to the formerly a great created the more of\n",
            "percept a series of self-surpomet these thing at the extreourness of a little forms\n",
            "of imperious cause and will to be precessed the sympathy of the\n",
            "po\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with r'\n",
            "ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with regard to an order to be of all former than a bad\n",
            "decipates for these madually, this canneed did not necessity\n",
            "refined conficuidutions (deneed must\n",
            "finally difficult soul:\"--he has as free spirits and \n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with r'\n",
            "ferings make\n",
            "him revolt against the noble taste which seems to deny suffering. the\n",
            "skepticism with religio_ that cum; destluctly neightrons; and\n",
            "merrones will; in asward regrived, does not secrotical is! neverthe the ear\n",
            "only instinctively or clear toroughroud kinds),\n",
            "and not-mescophaying it it is s\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 5\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.2735\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.2525\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.2341\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.2159\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "mean'\n",
            "iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "means of the soul of the good and interpretations are the desires of\n",
            "man in the same time and superstitions and self-destimes of his taste it is\n",
            "always been so that the most subject of the same as an\n",
            "and \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "mean'\n",
            "iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "means of religious european has it will be the most community and\n",
            "happiness of everything that which immediately all these to and do\n",
            "the to and overcoming notion to any does not with it is the higher ores\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "mean'\n",
            "iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "meaning orespecre how many sesses for all the refound to also toth me for\n",
            "that the fact that the world--so late the a moralie the\n",
            "courage and soul a delight in this conditionality--this keing in his\n",
            "proba\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "mean'\n",
            "iment: its dregs come up.\n",
            "\n",
            "80. a thing that is explained ceases to concern us--what did the god\n",
            "means of rits:--this vesliate of venytions lay with it! withdravation:\n",
            "\"his invertion of god\" abonds when its actmenles,\n",
            "betraess his \"descrys\"! and 6hat evil, for this\n",
            "continate jey socire a willing for \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 6\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1978\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1858\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.1705\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1543\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: ' and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and'\n",
            " and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and strength,\n",
            "which are the same still manifold in the possessive conscience and soul of\n",
            "morality and the soul of his thing in the possession of an eximple of an exception in the\n",
            "possible of the end, and\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: ' and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and'\n",
            " and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and speaks\n",
            "that even the most sensible of and the soul.\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: ' and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and'\n",
            " and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and puriom\n",
            "or especially and diffical close personalize.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: ' and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and'\n",
            " and history of the so-called moral feelings\n",
            "and which, in its progress, is called upon to posit and wopandy\n",
            "and aufne evable words of a pelsonare exbediest)-one our finus good that is\n",
            "prepore.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 7\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1378\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1264\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1150\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.1002\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 're a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of cul'\n",
            "re a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of culture, the\n",
            "state of all the spirit and the spirit of the spirit of the strength of all\n",
            "the spirit and the state of all the spirit of the spirit of the sublime\n",
            "and the state of all the spirit and the st\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 're a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of cul'\n",
            "re a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of culture they\n",
            "are solether, in the subject of all the strength of aspition of its\n",
            "own existence and developments and devalids here the latter of\n",
            "distrust--when they have always become so that all the spir\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 're a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of cul'\n",
            "re a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of culture, briev? of \"going estancial\n",
            "modern and pasce of which me liberit himself and the\n",
            "sloep _ppict of logic, and the moral of the listrant\n",
            "of socisty constinections, they have been possifisate even un\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 're a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of cul'\n",
            "re a counter-argument, and not rather a\n",
            "condition of every higher culture, of every elevation of cultice is so pretess him\n",
            "\n",
            "f7ly above-\"pots!\" vapiotic especial:--what wish tree is\n",
            "know why themselves has the philosophy has for edoint dhaning subrage and\n",
            "only to revale).\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 8\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0857\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0776\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0653\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 174us/step - loss: 1.0577\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally depe'\n",
            "us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally depertents to\n",
            "be evil to the possession of the world, and a stark of man, that\n",
            "as the pressist and present rame, the than the heart of the spirit, the\n",
            "strong people, as is a philosopher in the conscience \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally depe'\n",
            "us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally depersent\n",
            "and prejudiced, and an art in the first than a master and the\n",
            "world in provided men who have a religious things of the actions in\n",
            "fact that he does not believe it will be assumptions who has the\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally depe'\n",
            "us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally deperate\n",
            "hes the anti-thithert presented by the saint, events--that that rearing\n",
            "indiring, theselves are bargatly also in such a type would be\n",
            "sach with which is it fair mess of disand the honoured not ce\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally depe'\n",
            "us accident,--is the factor upon\n",
            "which the excitation of our passions to white heat principally dependent\n",
            "brok)! with this self-convedivence and what all that the \"mby nature_\"\n",
            "responsibility--the rure, super-ancrenriciplatizing, than, than hor\n",
            "you, every other, are spirituality intingcenative in en\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 9\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0445\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0360\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0238\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 1.0172\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and '\n",
            "at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and of the world, and substances\n",
            "in the contradiction of the conscience and a subjection of\n",
            "the conception of the subjection of the truth in the present dangerous\n",
            "and the last curion to an and consequence\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and '\n",
            "at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and believed and extent the\n",
            "greatest explanations, of soulsands and even the conscience which has\n",
            "conscience, say, with at last to be the freghts of the desireed by\n",
            "his own love, and all love of knowledge\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and '\n",
            "at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and the past of their dangers all\n",
            "hours with he first to the unintenhaual as a painful geance, to the ect\n",
            "of the philosophers, but only then people are present-mendinessed and entmis\n",
            "of wantwary latkernat\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and '\n",
            "at he demands from himself a verdict, a yea or nay, not\n",
            "concerning science, but concerning life and invention and self-generations\n",
            "of revolution): and is, he was now religions, inmerated as\n",
            "many when thereif impulses.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 10\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 1.0049\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.9942\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9863\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9776\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: ' that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educ'\n",
            " that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educative, the\n",
            "soul of the sense, and the same as the same precisely thereby the\n",
            "other hand, that the same time and excessive in the same time and its\n",
            "finally, the possibility of the present desires the r\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: ' that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educ'\n",
            " that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educates to the\n",
            "world of life in the present statest and actions from themselves and\n",
            "the same comprehensive all the idea of notions and humanity, and the\n",
            "moral in the poor man of his point of self and the\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: ' that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educ'\n",
            " that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness education, stronguide\n",
            "them, says above all that which at once and all them to require essent, and\n",
            "the origin of this origin and love, i means in every people and more\n",
            "comparasing that is to say, with thei\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: ' that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educ'\n",
            " that consequently the modern, noisy,\n",
            "time-engrossing, conceited, foolishly proud laboriousness educative; and\n",
            "morality in anlk not to their friend wrons to me to sex  strongest apposed to his\n",
            "ormand justity, fall they will not latte moresferred, some\n",
            "\"desiraring objection, over my suffieing.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 11\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9693\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9581\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9485\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9398\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, frien'\n",
            "easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, friends, that is to say, the\n",
            "to make more complete into the interpretation of his that men who are\n",
            "the present dangerous spirits and the spirit of a cload, and man as so the\n",
            "social possibility of an action\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, frien'\n",
            "easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, friends? the neighbourso, of the\n",
            "thing is a philosophers and delight in the mind of the most depending, consequences\n",
            "as well good and and morality.\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, frien'\n",
            "easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, friends? the great\n",
            "man for as most didection is only the worl'- the love rouses, because even\n",
            "the presses with great sense among the like papry.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, frien'\n",
            "easts, sure of our bourne,\n",
            "                      our aims self-same:\n",
            "     the guest of guests, friendérerly beauth illuticate shall different? and\n",
            "extent, the likes of nature is only dry folly, (san, with and have comely\n",
            "the like general thing--no only will it in his habey and disclete in\n",
            "one refina\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 12\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9345\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9239\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9190\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.9102\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live her'\n",
            "ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live herd and the\n",
            "strong contrary, which is also a refined the self-destinct of such\n",
            "a discoverers are completed to the process itself, and also the religion of\n",
            "self-desticians, and as the sense of the senses\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live her'\n",
            "ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live herd and the\n",
            "influence of the superior to the neighbour--the beht, in the\n",
            "last extent the free spirit as a personal man of a seed about him in\n",
            "every sanction in the tragedy of the proced in the same erou\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live her'\n",
            "ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live her ever their\n",
            "sensealse, seek and for reverence my become a which is does its believe that\n",
            "a man delight on itself in his dependences be the from of his dall\n",
            "entmisurmendes.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live her'\n",
            "ed o'er\n",
            "                      with love and fear!\n",
            "     go! yet not in wrath. ye could ne'er live herd, aed\n",
            "a means of givly \"whom al on think\" or others, yet is inverts respects\n",
            "when he would not even day; \"i\"centur, in woman, wence a malivious\n",
            "expediation at the personal truth awon, are fundared to\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 13\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.9031\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8980\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.8881\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.8792\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "voliti'\n",
            "om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "volitions and depresing themselves which in the free spirit of\n",
            "the extiresporance of the end, and and one of the heart and states of\n",
            "sentiment, and the problem of great devilous, when the most upon himself\n",
            "\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "voliti'\n",
            "om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "volitions, the problem of german spirit and the most religious men,\n",
            "therefore it is a further and incling development of one's own self-deception,\n",
            "the christian dangerous state of the lightering to the powe\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "voliti'\n",
            "om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "volitions unconditions for the greatest doctranizoner, and are a\n",
            "feeling of some prospective and sidection on this the believer has\n",
            "preperatival cestance, desidesuanity it is a creature of my still\n",
            "outer ac\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "voliti'\n",
            "om of will\"--that is the\n",
            "expression for the complex state of delight of the person exercising\n",
            "volition will as anning as the european on prejedfii froundstives\n",
            "for any eifice; for weol one had now it has -nelethones to repegt\n",
            "\n",
            "of which go enough of all peris--to first them! for instance, to which\n",
            "pr\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 14\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.8740\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8614\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8589\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8517\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: '[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question w'\n",
            "[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question when they are all the same\n",
            "with and suffering of all the standard of man and woman is even the\n",
            "most indiscipling and delight in the art of being displeasing the other man of\n",
            "all good gives his own self\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: '[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question w'\n",
            "[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question which he has always to\n",
            "anticialisy in the case of so that the standpoint of his sentiment\n",
            "and truth.\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: '[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question w'\n",
            "[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question when they could every one\n",
            "dost from the first from order of real things--the might father of\n",
            "their law and became understand our piwcy and virtues\n",
            "with reverence a goman cause.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: '[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question w'\n",
            "[menschen und dinge]. this can be thought out and it is\n",
            "worth while doing so, even if the question when vost upon himself.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 15\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.8453\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8403\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.8303\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8279\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: '-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med'\n",
            "-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med ores the race of stated type of moral easy,\n",
            "perhaps, that the latter and the religious stain under the right man does not be\n",
            "preached thereby the thinking or of and notion and free spirits--as the\n",
            "is\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: '-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med'\n",
            "-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med originates and more philosophers and morality evanion\n",
            "the one will serve and speak of some primordially extens to be sure,\n",
            "when they are any simely and calls in order the might and in the last\n",
            "is the\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: '-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med'\n",
            "-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med tim ut co loxing to him who one must reality the\n",
            "objection things, or the great sible for attemmed to trouble espicious and\n",
            "the most rational experient of generation! that which we have a from\n",
            "a as a\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: '-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med'\n",
            "-one is pushed\n",
            "to this probably unpleasant conclusion, now that the influence of\n",
            "respectable but med to which they were a philosophers and distinction,\n",
            "and of such digated, philosophy afulnt of sufflioistiblek; and how right\n",
            "abor its fearly, among the fceetwe, such as demod] ramining? people who at\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 16\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8203\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8175\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8082\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.8027\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself)'\n",
            "nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself).\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself)'\n",
            "nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself).\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself)'\n",
            "nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself)) theie could year of man that species, for\n",
            "in the self-conscience he the domain of our dream which they were \"the soul\n",
            "and genuial, more imploked, but notion for hand, in all this himself\n",
            "and was rec\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself)'\n",
            "nt of view, to\n",
            "get along without the little \"one\" (to which the worthy old \"ego\" has\n",
            "refined itself): it i mean powerful to themselves something\n",
            "herditual probress and curional the odden and relacess and disideration,\n",
            "so learned.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 17\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7977\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7890\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7851\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7821\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how ma'\n",
            "hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how man concealed general\n",
            "standard of man and who has hitherto been the same prejudices of the\n",
            "subject, and his own suffering--but who make the highest saints and\n",
            "instinct for suffering, intentional and bec\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how ma'\n",
            "hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how many selfare to a\n",
            "philosopher is such a state men of religious and the spirit and\n",
            "his own very beings in every investivality--the paintened to the \"mirally of\n",
            "the consequences as sought in the fact that\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how ma'\n",
            "hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how man would with the\n",
            "arisholy, it has been a \"modern\n",
            "world,\" do not become a word about it for sime, and much as they will be discintuded the\n",
            "infinite of it? in his have almost also do not could round to \n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how ma'\n",
            "hat do not concern him. as a matter of fact\n",
            "the free spirit is bothered with mere things--and how many serfect and\n",
            "i man duey standards would find to above all the grand hand much\n",
            "\n",
            "juggle, process, and specially oblieed, friendly recolly:\n",
            "for overfumining cealition of his century of us, we goly and \n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 18\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7758\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7699\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7634\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7555\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who k'\n",
            "artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who knowledge, as a personal consideration of the subject\n",
            "to refined and consequences, contempt, the inner consequences of logical\n",
            "and the spiritual astrocy, the \"instinct\" is so instrument from all this\n",
            "i\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who k'\n",
            "artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who knowledge, as a personal who is as a present dangerous antitude\n",
            "of his form, the spirit of the philosophers who know what which\n",
            "sacrousads a morality in all other belief in the same distrust, but resto\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who k'\n",
            "artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who knowledge, up to honour, he freedom of the whoger\n",
            "that he can desire envalse world.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who k'\n",
            "artistical\n",
            "discourse--that delivered from the pulpit. the preacher was the only one\n",
            "in germany who knew whom woman may be doubted gradually, supers of\n",
            "man this, ave as lawn to possible in provagious feelings of place\n",
            "that re account it not wo ld end, as regard to the whole stinging in his\n",
            "ultmmannes\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 19\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7521\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7453\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7436\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 175us/step - loss: 0.7382\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: ' personal considerations of advantage is just the\n",
            "sort which has its origin in the present conceptio'\n",
            " personal considerations of advantage is just the\n",
            "sort which has its origin in the present conception of an existence\n",
            "of a new thereby comparable, and are of contempt to the and caste of\n",
            "the instinct of sentiment and fearful at the soul of a new character,\n",
            "strength of which all the spectarland has n\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: ' personal considerations of advantage is just the\n",
            "sort which has its origin in the present conceptio'\n",
            " personal considerations of advantage is just the\n",
            "sort which has its origin in the present conception of an exception,\n",
            "they are not say the soul of sociates.\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: ' personal considerations of advantage is just the\n",
            "sort which has its origin in the present conceptio'\n",
            " personal considerations of advantage is just the\n",
            "sort which has its origin in the present conception of feeling, and\n",
            "that the will of an end, the general customs and christianity as a\n",
            "great:--not, in readity couragests and us.\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: ' personal considerations of advantage is just the\n",
            "sort which has its origin in the present conceptio'\n",
            " personal considerations of advantage is just the\n",
            "sort which has its origin in the present conception of the\n",
            "soul untin us.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 20\n",
            "Epoch 1/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7375\n",
            "Epoch 2/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7337\n",
            "Epoch 3/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7228\n",
            "Epoch 4/4\n",
            "200265/200265 [==============================] - 35s 176us/step - loss: 0.7224\n",
            "\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: 'e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the clai'\n",
            "e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the claising character of man.\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: 'e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the clai'\n",
            "e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the claishes and that is a prising of\n",
            "their germans, to example heart and self-contradiction, and whatever is\n",
            "blought to be desires to him for a little are contrastuand and preached\n",
            "themselves.\n",
            "\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: 'e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the clai'\n",
            "e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the claisely men of the brud,\n",
            "which meas of strives and friend, indeed, spirituality: a leart hom\n",
            "ob ectional malaphesirs some of the end, as he who _are at leart, men when\n",
            "their person with final value which\n",
            "\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: 'e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the clai'\n",
            "e: the portion of independence and green meadow without which\n",
            "there is no rest from labour, the claims.\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 21\n",
            "Epoch 1/4\n",
            " 16384/200265 [=>............................] - ETA: 32s - loss: 0.6719"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHRq6xBHnoKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "ec4f8b14-8eac-4ff8-9224-8749234b00fb"
      },
      "source": [
        "# Now you each group will perform the following tasks.\n",
        "#\n",
        "#   Part 1)\n",
        "#\n",
        "#   - Each group will pick up one set of data samples:\n",
        "#     * assembly code (machine code z80, x86, ...)\n",
        "#     * latex corpus\n",
        "#     * html pages\n",
        "#     * linux kernel source code (https://github.com/torvalds/linux)\n",
        "#     * patents\n",
        "#     * ...\n",
        "#   - Modify the model to be trained in the corpus you chose\n",
        "#   - Present the results\n",
        "#  \n",
        "\n",
        "#Get google's dataset of blog entries\n",
        "import requests\n",
        "#url = 'https://www.facebook.com/favicon.ico'\n",
        "url = 'http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('blogs.zip', 'wb').write(r.content)\n",
        "#r = requests.get(url, allow_redirects=True)\n",
        "#open('facebook.ico', 'wb').write(r.content)\n",
        "\n",
        "from zipfile import ZipFile\n",
        "ZipFile.extractall(path='blogs.zip', members=None, pwd=None)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-488604f08c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#url = 'https://www.facebook.com/favicon.ico'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'blogs.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#r = requests.get(url, allow_redirects=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                 )\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VB-FT-9vEOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create a ZipFile Object and load sample.zip in it\n",
        "with ZipFile('blogs.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   zipObj.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MTQiMfmvJ-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "bde6acbe-4158-416f-f299-06661802bae0"
      },
      "source": [
        "#!ls blogs\n",
        "#!cat blogs/862379.female.25.Science.Virgo.xml\n",
        "\n",
        "from glob import glob\n",
        "allxml = list(glob(\"blogs/*\"))\n",
        "print (\"Number of xml files\", len(allxml))\n",
        "\n",
        "#for xmlfilename in allxml:\n",
        "#  !sed -i '' 's/&//' $xmlfilename\n",
        "#  !sed -i '' 's/$//' $xmlfilename\n",
        "#  !sed -i '' 's/^//' $xmlfilename\n",
        "\n",
        "\n",
        "from lxml import etree\n",
        "import lxml\n",
        "import re\n",
        "fname = \"blogs/862379.female.25.Science.Virgo.xml\"\n",
        "from io import StringIO, BytesIO\n",
        "\n",
        "#Take subset of blog entries, too many will crash colab RAM space\n",
        "numblogs = 10\n",
        "\n",
        "#Process all blog XML files into 1 text file\n",
        "count = 0\n",
        "!echo \"\" > posts.txt\n",
        "with open(\"posts.txt\", \"w+\") as f:\n",
        "  for xmlfilename in allxml:\n",
        "    try:\n",
        "      parser = etree.XMLParser(ns_clean=True, recover = True)\n",
        "      tree = etree.parse(xmlfilename, parser=parser)\n",
        "    #Throw out XML files with invalid syntax\n",
        "    except lxml.etree.XMLSyntaxError:\n",
        "      print (\"Skipping XML Invalid syntax:\", xmlfilename)\n",
        "      break\n",
        "    posts = tree.findall(\"post\")\n",
        "    post = \"\".join([re.sub(\"\\s+\",\" \", p.text.replace(\"\\n\",\".\")) for p in posts])\n",
        "    if not count: \n",
        "      print (\"Example of a cleaned blog post:\\n\", post)\n",
        "    count += 1\n",
        "    f.write(post + \"\\n\")\n",
        "    if count > numblogs: break\n",
        "#    print (\"Wrote file\")\n",
        "\n",
        "#!head -5 posts.txt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of xml files 19320\n",
            "Example of a cleaned blog post:\n",
            " .. . Frankisko17: haha, oh yeah, I had to perform an exorcism like an hour ago waphlator: really? Frankisko17: yep. the demons were back in my intestinal tract and they were wreaking havoc in my intestines and I was like, THE POWER OF CHIRST COMPELS YOU! OUT!! OUT I SAY! and then they went into the toilet and all was good. waphlator: hahahahahahahahahahahahaha waphlator: satan was in your anus again Frankisko17: haha yep Frankisko17: my ass was possessed waphlator: damned lucifer waphlator: he just can't stay out Frankisko17: this is the spot Lucifer fell Frankisko17: my ass waphlator: but if you think about it, your intestines would be a lot like hell Frankisko17: yes. yes they would waphlator: hahaha Frankisko17: smelly and hot . ... . fingers locked, ill hold you close your hair's against my cheek it smells so good, in perfect prose your voice puts me to sleep. Id lie here forever, sleep to keep you here with me avoid the sunrise, the moons enough to keep your pale skin lit. If i only had a minute left id spend it on you. But, you've grown out of reach, the fairest flower, too high to bring you down. But i can wait til winter, when frozen petals fall. . ... . HOLY SHIT! I just got home from seeing the Exorcist: the beginning. HOLY SHIT!(quite literally) That movie was messed up. It was cool though, it scared me a good bit. I think the best line from the movie is, \"Whats the matter Marin, don't you want to fuck me anymore!?\" I took noel and ryan to see it. Noel says she loved it but im not sure. I know ryan liked it. It was just completely ridiculous. So much gore, and bloodshed. Oh so many freaky weird things happened in it. I was afraid to get out of my car when i got home, cuz i thought some damn wild dogs would come rip me to shreds.(you have to see the movie to know what im talking about) If you want a good scare i recommend seeing it. Although, its not for the weak stomached. Yeah bowling was a lot of fun. And, as i said, i suck at it. Band always sucks. The other part of my day was just a fairly normal one. You said meet me with your eyes, you sing to me its harmony when you're staring into mine. -No Delay . ... . For all of this, im better of with out you. Do you regret, so, your loneliness? I love that song by the early november. It isn't on their album, but its on the Drive thru records dvd, and i think its on their ep. Emma, im writing for you again damn it! I even stopped playing my guitar to write. Hmm, the only problem is i don't have much to write about! I didn't do a durned thang today. And i still haven't shaved. I will tomorrow though, maybe not though, its kinda fun looking like a bum. Carter the cat, carter the cat, everybody loves Carter the cat! sorry that is from the Drive thru records dvd. Its quite comical i must say. Lets see, tomorrow i get to go to my granny's house and move a bunch of boxes out of her garage and into her attic. Then ive got marching band(oooyay!) After band, im going BOWLING with ryan and others.(i suck a very insurmountable bit at bowling) After that, im goin to see The Exorcist with noel i think. Then ill come home, prolly sleep, then get up friday, do nothing, and then go to work for another 6-12 shift.(FUCK) Oops, excuse my language. I really do hate that shift. Ok kids, im not going to blab about nothing any longer. yep. thats its. PLEASE LEAVE ME A COMMENT! AS I DO LOVE READING THEM SO! . ... . Yeah, i just found this other pic, of my dreads. I think you can see them better. Someday ill try and get a really good picture of em, but till then this is all i got. urlLink urlLink . ... . Yeah, today 8 minutes from victory was a rockin'! We played from 1 to about 4. Then i had to get some food before marching band, so where did i go? TACO BELL , of course! Yeah i ate with brad, adam, and ryan. Timmy was there too for a little while. Oh my god im starting to hate, absolutley hate, marching band. For the last few practices which last a good two and a half hours, i only got to play for the last half hour. The rest of the time i talk with heather. Man, if heather weren't the drum major, id have no one to talk to. Yeah, then i went to josh's house to watch the Drive thru records dvd, Spectacular Spectacular. It's got to be some of the funniest stuff ive ever seen. Tomorrow, i think im gettin my hair cut? Im not sure. Other than that, ive got nothing. Itd be nice if someone would ask me to do somethin(wink wink). But, i may just hang out here and finish up some work for my ap bio class. Goin ta see the exorcist:the beginning, this thursday night. That better be really freakin scary, or else i may just stick some corn in the ears of the director. Yeah, thats it. Not too much to talk about right now. she wore crushed velvet, jet black, and to the floor. she rode on angel's wings, that use to shine, oh so bright. -hawthorne heights . ... . wow. im obsessed with putting pictures on here now. But i found this picture that my dad took of the grand tetons. it was such a cool place. urlLink urlLink . ... . Ok, here's a picture of me with my dreads. I usually don't wear glasses though, so i don't know how good of a picture this is. Oh and if ya look you can see my little abe beard. I don't what i was thinkin! urlLink urlLink . ... . Wow, this was back when i had dreadlocks, thats my bass, which is also my baby. I love it so much. urlLink urlLink . ... . Here is me in reality. And of course i always wear a tuxedo! urlLink urlLink . ... . I found this picture of me. It's who i really am inside. urlLink urlLink . ... . Lets see, today wasn't a very good day in my words. I just sat around, i did take my brother to see a movie, but ive already seen it. Well, for the most retarded reason in the world, Adam, my best bud, whom i said would definitely get hired at the theater, did not get hired today. And do you want to know why? FUCKING MARCHING BAND! Im so pissed at my manager and at band right now. Band gets in the way of almost everything. That really sucks. I was so excited too. Adam says he's not too upset about it though, seeing as how he has a job right now. But he does think that it sucks. Ive been doing an awful lot of solo work on my bass lately. I can only write songs on my bass when i have no intention of working together with another instrument. Besides, its impossible to write songs like the ones that my bands play without a guitar or drums. I love playing my bass. And i think im pretty good at it, but i know there is a whole other level that i need to reach. Of course, ill never be good enough for myself. But that isn't a bad thing. OOyay! School stars in about two more weeks. I kinda want to go back and see all of my friends that i haven't seen this summer, but i really don't want to have to deal with the work. Especially with marching band, my job, and my bands. I know ill do fine though, but i will get way stressed out sometimes im sure! I would love to have a relationship with someone too, but my only prospectives right now seem to be either freshmen(no way!) or people who don't share enough of my views. I can't help how picky i am, but i don't want to have bad consequences when a breakup will happen, which im sure will. I just can't stand being alone like this all the time. And im not talking about not having friends, ive got plenty! Ive got practice with 8:fmv tomorrow! I can't wait. I love playing with that band so much. We always get stuff done. We might record something, too. Our drummer is awesome, and hes just a really cool guy. Infilitrate. Destroy. Rebuild. . ... . Ok, emma just showed me the scariest shit in the world, it takes a couple of seconds, but its well worth the wait. urlLink http://members.home.nl/saen/Special/zoeken.html . ... . Yeah... Today is Sunday. I've had to work all freakin weekend long, 12 hours didn't pass between when i was at work and when i was home. From 6 on Friday night to 6 today i worked. I didn't get to do anything with any of my friends. I tried to get a hold of a few people to go with me to see a movie tonight, but they were either doing something already, or i just couldn't get a hold of them. I sat on my butt and watched Fosters' Home for Imaginary Friends for an hour on cartoon network. There is a slight possibility that i will become an assistant manager at the theater. Im not sure if i would do it or not. I just remembered that i had a severe attack of hyperness last night. Im very sorry to all the people that im sure i was annoying to a great extent. See, when im hyper, im insane. I should not be talked to at all. I start saying ridiculous things that make very little sense. I am listening to this song from Hawthorne Heights whilst i write this, I loves it. Hey there, I know it's hard to feel like I don't care at all Where you are and how you feel with these lights off as these wheels keep rolling on and on Slow things down or speed them up not enough or way too much how are you when I'm gone And I can't make it on my own because my heart is in Ohio so cut my wrists and black my eyes so I can fall asleep tonight or die because you kill me You know you do you kill me well you like it too and I can tell you'll never stop until my final breath is gone Spare me just three last words I love you is all she heard I'll wait for you but I can't wait forever You kill me well . ... . im sure many of you have already read this but i think its hilarious. True story, I was happy. My girlfriend and I had been dating for over a year, and so we decided to get married. My parents helped us in every way, my friends encouraged me, and my girlfriend? She was a dream! There was only one thing bothering me, very much indeed, and that one thing was her younger sister. My prospective sister-in-law was twenty years of age, wore tight mini skirts and low cut blouses. She would regularly bend down when near me and I got many a pleasant view of her underwear. It had to be deliberate. She never did it when she was near anyone else. One day little sister called and asked me to come over to check the wedding invitations. She was alone when I arrived. She whispered to me that soon I was to be married, and she had feelings and desires for me that she couldn't overcome and didn't really want to overcome. She told me that she wanted to make love to me just once before I got married and committed my life to her sister. I was in total shock and couldn't say a word. She said, \"I'm going upstairs to my bedroom, and if you want to go ahead with it just come up and get me.\" I was stunned. I was frozen in shock as I watched her go up the stairs. When she reached the top she pulled down her panties and threw them down the stairs at me. I stood there for a moment, then turned and went straight to the front door. I opened the door and stepped out of the house. I walked straight towards my car. My future father-in-law was standing outside. With tears in his eyes he hugged me and said, \"We are very happy that you have passed our little test. We couldn't ask for a better man for our daughter. Welcome to the family. \"The moral of this story is:\" \"Always keep your condoms in your car.\" . ... . AHHHHHHHH!!!Long freakin day at the theater. I got to work with a bunch of cool people today, two of which i kinda like(if ya know what i mean[and no, they aren't men, you sick people you!]). I saw so many people today. I saw my cousins, i saw adam, i saw timmy, i saw Andrew's brother Devin, i saw a couple teachers from school, im sure there were others too i just can't remember all of them. YAY! Adam is gonna get hired at the theater for sure! Hmmmm... i can't think of anything else to say. Eat some cake then sleep it off! Drink a bottle of syrup and tell me how it feels! Im just yelling nonsense about food! Make some noodles and feed them to a turtle! Ok, im done. . ... . Ok, stupid thing. Go cook some noodles and feed them to a turtle. . ... . Im awesome. . ... . Alien vs. Predator... that was a very cool movie. It was reminiscent of the older alien and predator movies. However, there a few things that i didn't like about it, but i suppose it made the movie better in a way, more like the other ones. It had an interesting story line, not quite what i had expected and i was happy about that. Yeah, i got up at 12:30, ive got work tonight from 6-12. yeah, nothing really to write about... maybe ill write something when i get home from work. every time the sky breaks open with sunshine, as streamin swords collide it take me back in time... . ... . Hey, this has to be short. Had band practice with brad and timmy today, it went well, til his mom needed him to take her to the hospital. I went to taco bell with adam, oh so fulllll... Band sucked (did i need to say that?) Went to Bob Evans with heather and jeremy after, that was fun. When i got home, my mom told me that one our friends' friends (basically, our friend) died in a house fire last night. That makes me very sad.... I still can't believe it.... Ok, time to go see Alien vs. Predator at 12. ... cya . ... . Was today one day, or two? I had a urologist appointment this morning. Im thoroughly frightened of old men touching me in my special places. Ewww, creepy. My appointment was like most these days, \"Well, if you get any more pain come back.\" I'm sick of not getting any satisfying answers from my doctors. If anyone wonders why i went to the \"pee doctor\" its because i had kidney stones about a month ago. Ohhhh they are so painful. Blarney stone. mmmmm. I sat in a brand new corvette today. wow. I want it. Well, for those concerned, and i know there are few who are, Im not depressed. I just feel like something is missing. I feel hollow. Won't someone please fill me up, with imaginary happiness... sorry i was just digressing. EMMA, YOU ARE A COLD HEARTED BITCH!!! ( no just kidding!) . ... . Blah. Yes thats it. Blah. . ... . im slowly sinking into a frame of mind where I am nothing, was never anything and where true love is hard to find. I’ve got stuck in my head, last times together when you were you, where I was me and our time spent felt like forever. My empty bottle, broken in two right down the middle the right half for you the liquid inside was blood from my heart but a broken bottle, for me, holds no part Umm, i had no motive to write this. Why did I? Who knows? Well it was late. I think ive gone crazy with this idea that im becoming depressed. What have i to be depressed about? Just one thing, and i don't think i need say what it is. . ... . Ok damn it, im writing something for you! Woke up, got outta bed, dragged a comb across my head. -The Beatles. Hmmm what did i do today? I cleaned my kitchen and bathroom, they were dirty and disgusting. I pretty much just sat around today and watched the rain, and thought about everything that's been goin on in my life. Band was dull, i think i played for a total of one half hour during the two and a half hour practice. A bunch of people went to TACO BELL after band. Ryan, Adam, and Brad tried to rape me. Brad called his girlfriend, and we passed the phone around and said stupid stuff. Then we called Carl, and annoyed him just as much as he annoyed us. Now im home. I guess when i think about it, i really didn't do that much today. But I had so much time to do stuff. I should stop being so lazy. The only problem is i hate doing things by myself and no one ever calls me up and says, \"hey ya wanna do anything?\" Ah well. I have an appointment with a urologist tomorrw. I like to call him the \"pee doctor,\" after that, who knows what im goin ta do. Ive got work too. I feel myself slowly sinking into a depression... I'm oh so sick of being tired, and so tired of being sick... -Taking Back Sunday . ... . Ok, I had a good day. Today is the ninth of August, meaning that today is my brother's birhtday. He turned 13. I took him to a movie, he loved it. I got new strings for my bass guitars. I watched a Wildthings baseball game with my family. How very straightforward I am being. You know, thats a problem that i find with most people these days. They can't put any color into their conversations or language. This is especially relevant in people from this area. All i hear all day is the same thing from different people. I won't say that this applies to all people, but most. Why doesn't anyone try to put some gusto into what they have to say. I mean, you only get to live on this earth once, why not speak as if you were never going to speak again. I guess what im saying is, i need to get away, see the world, meet new people, and live through some really good and bad times. But, im not joining the army incase any of you had that your mind. Why am i being so cynical today? I have no idea. I truly did enjoy my day of running around with my brother. People say we look very much alike. Well, WE ARE BROTHERS WHO SHARE THE SAME PARENTS! Siblings usually do look alike you silly people. I was eavesdropping on some kid at the guitar store today, he was announcing to some girl who was maybe 3 or 4 years older than he that he knew everything about being vegetarian. He was also reciting many useless things that no one cares about. I sersiously wanted to talk to him, and tell him to shut up. But instead i proceeded with the purchase of a set of strings for my bass, and the my prompt departure from the store. Ok, well i think that's enough of my cynicism for one day. Until next time, i'll see you on... the radio. I've picked the best parts, and played them in my mind... . ... . I didn't think that i could ever get tired of working at the theater, but lately i'm becoming very fed up with it. I can't stand the people that i have to deal with. But oh well. Today, an old lady came up to me after watching \"The Village\" and said to me, \" sir, i am 67 years old, and that is the worst movie i have ever seen in my entire life,\" she then decided to tell the rest of the people waiting in line to see their movie how horrible \"the village\" is. I wanted to kick her. I mean, its mean to do something like that to an old lady but this one might have been an exception. To boot, she wanted her money back after she had finished watching the movie, thats against theater policy( but my manager gave them re-admit tickets anyhow). Then a little later, some freshmen from my school came to visit jered and followed him around for a little while then the cuter of the two decided that she would just wait for her friend, who followed jered every where he went while he worked, and talk to me. Or rather flirt. Oh, how i can't wait to go back to school, freshmen freshmen freshmen. I understand why seniors hate freshmen so much now. After work i came home, tomorrow is my brother's birthday so we had his party today. My parents got him sirius satellite radio. I am extremely jealous. I didn't realize how cool satellite radio actually is. No commercials, any kind of music you want whenever you want to hear it, and good songs are always playing. No crap. I started reading Dave Barry's \"In Cyberspace,\" very very funny. I didn't want to stop reading it last night, but it was about 2:30 in the morning and i was falling asleep. Actually, it's kinda late right now so im gonna go and read it. Until next blog! Three sleepless nights, this isn't how its supposed to be. But you're so good at taking your time to get back to me... -Emery . ... . Today was Saturday. I had to work. After work, Heather, Jon, Adam, and I went to pizza hut. The waitress called us sweet heart, and hon a lot. Now, I am home. Woo. There is a girl at the theather with whom i work, and i'm thinkin about asking her out. The only problem is, I'm not sure if i want to or not. As ive said before, all my relationships last about a week. I don't know, maybe I'll ask her, maybe i won't. I would like to get to know her better though, she seems like a fun person. (Oh and those damn freshmen marchers were at the theater today. I wish they would let me be.) Trinty High Marching band makes fun of McGuffey High Marching Band. They can now eat my shorts. I have only respect for those marchers who i know. And we thought they liked us. I'll float away... to another place. . ... . I AM SO TIRED! Today i got up at 5:30 and went to Kings' family restaurant for a breakfast with my fellow seniors from marching band. I ate way too much, including tons of ice cream. Band doesn't start til 8:30, so a group of us went to Wally mart and steph came up with the idea to buy some mice and name them after a few of the band directors. It was fun. I'm so happy that Steph and I are on talking terms now, everything is starting to seem like it was a very long time ago, about this time of year last year. But i don't want anyone to get the wrong idea. I don't feel like things will ever be the same as they were, and i don't expect them to be. Band sucked as usual. I've been listening to a band named Emery for a few days, and i can't stop listening to them, and the lyrics make me think a lot about many things. When i listen to Emery, I just want to lay down on the grass and watch the clouds fly by. But, the music would be better if I had someone to listen to it with. Well, my fear of little freshmen girls having crushes on me is real. One of the color guard girls, named Chloe, is \"interested in me.\" I'm not quite sure how i feel about it. I mean it's cute, but i like girls who are either my age or older because of their level of intellectuality. Young girls just don't seem to have this extremely important quality, but maybe i'm wrong. Who knows? I finally finished \"Go Ask Alice\" last night. That book is purely amazing. I practically cried after i finished. God, I must be the most sensitive guy in the world. I can't stop thinking about it. Emma, you need to suggest things to me more often. Lastly, in my long day, Adam, Noel, and I went to Taco Bell. Then i had to work from 6-12(which sucks so much) And now I'm writing this at 1 in the morning, Saturday. I should go to sleep now. So, til next blog, Go fuck yourselves San Diego. . ... . Woo, fun, another day full of band, however, today was the nicest day we've had since band camp started. As I was playing my bass some clouds were moving across the sky so i watched as they blocked out the sun. (i was wearing sunglasses of course!) I thought to myself, \"Gee, I haven't seen something as wonderous as this event for some time, I should really pay closer attention to the majesty of nature,\" it gave me quite a content feeling that I haven't experienced for some time. I forgot all about any troubles I've had recently. This event of a saphire sky, white globe and frail, wispy pale clouds and dark purplish cloud foreground grinding against the warm sun rays creating such an awesome display of light and shadow enstowed within me a feeling that i've been yearning for for quite some time. I realize now that i really do take much of life's treasures for granted, and so do many other people. I know i'm a freakin' hippy, but wouldn't people be happier and wouldn't they get along better if they could feel the way i felt today. These are the things that i think about when no one else is looking, these are the things that help me sleep at night, these are the things that keep me moving in the morning, and that is why i carry on. Ok the weirdest thing happened too, during the time that i have been writing this, someone who i haven't had a conversation with for several months tells me that we should talk sometime, but then says nevermind. Can anyone explain this to me? I wish people didn't hate me so much sometimes, I don't understand it. what a beautiful color, it goes with your eyes.... . ... . It was a long day. Band from 8:30 til 5, then Not Yet practiced at Josh's house after from about 6 til 9:30. I'm losing hope in that band, no one can ever stay focused long enough to write anything, and then someone always ends up disliking something about the song. Oh, and Derek decided to just disappear without a trace and not call us. I hate to say it but i'm gettin a little tired of playing my bass, but of course i still love it to death. Brad let me borrow the Emery cd which kicks total ass. I found my favorite lyrics of all in the song \"The Ponytail Parades\" the chorus is \"It doesn't feel right, holding someone else's hand. Spending the whole night, living at two opposite ends.\" The song really gets to me, it makes me want to cry sometimes. I wish i could find a love like the one sung about in that song. I'm such a wuss. hmmm.... Josh, I'M NOT A VEGAN! . ... . So, now that i have one of these blog thingies, i can spill everything ive wanted to say without saying it directly to anyone. I like this. So far this summer has been too quick, with far too few happy moments. I had a couple relationships, both of which lasted about a week, one was more serious than the other. I'm not sure if I'm going to try and have another one, since they all seem to last about a week. Somehow, something never works, no matter how hard i would like to work on a relationship. I may just as well wait for college. My friend Brad and i started a new band today, it's the best band i've been in to date. The four of us, Brad, Timmy, Mike, and I clicked really well. We got about 5 ideas for songs within about two and a half hours. The other band im in is good too, but we can't seem to stay focused since we all love a different style of music, but im not giving up on them seeing as they are a few of my best buds. My dear friend emma is, im afraid, slowly becoming like a character in a book that she has me reading. It truly worries me. . . .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1E03fuCU5qX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ade0232f-70c5-4066-fe6c-99ee798ab90e"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers import GRU\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "text = open(\"posts.txt\").read().lower()\n",
        "print(\"corpus length:\", len(text))\n",
        "\n",
        "chars = set(text)\n",
        "print('total chars:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# cut the text in semi-redundant sequences of maxlen characters\n",
        "maxlen = 100\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"nb sequences:\", len(sentences))\n",
        "\n",
        "print(\"Vectorization...\")\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# build the model: 2 stacked LSTM\n",
        "print(\"Build model...\")\n",
        "xi = Input((maxlen, len(chars)))\n",
        "x = GRU(256, return_sequences=True)(xi)\n",
        "x = Dropout(0.2)(x)\n",
        "x = GRU(256, return_sequences=False)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(len(chars))(x)\n",
        "x = Activation(\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=xi, outputs=x)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "adam = Adam(0.003)\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=adam)\n",
        "\n",
        "\n",
        "def sample(a, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    a = (np.log(a + 1e-8) / temperature).astype(np.float64)\n",
        "    a = np.exp(a) / np.sum(np.exp(a))\n",
        "    try:\n",
        "      sample_result = np.argmax(np.random.multinomial(1, a, 1))\n",
        "    except ValueError:\n",
        "      error = 1.0 - np.sum(a)\n",
        "      a[0] += error\n",
        "      sample_result = np.argmax(np.random.multinomial(1, a, 1))\n",
        "    return sample_result\n",
        "\n",
        "# train the model, output generated text after each iteration\n",
        "for iteration in range(1, 60):\n",
        "    print()\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Iteration\", iteration)\n",
        "\n",
        "    model.fit(X, y, batch_size=4096, epochs=4)\n",
        "    model.save_weights(\"weights.hdf5\")\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print()\n",
        "        print(\"----- diversity:\", diversity)\n",
        "\n",
        "        generated = \"\"\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print(\"----- Generating with seed: '\" + sentence + \"'\")\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(200):\n",
        "            x = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            # predict next char\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            # full sentence being generated\n",
        "            generated += next_char\n",
        "\n",
        "            # shift sentence\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # let's consider only one sentence\n",
        "            if next_char == \".\":\n",
        "              break\n",
        "        print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "corpus length: 349523\n",
            "total chars: 73\n",
            "nb sequences: 116475\n",
            "Vectorization...\n",
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100, 73)           0         \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 100, 256)          253440    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 256)               393984    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 73)                18761     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 73)                0         \n",
            "=================================================================\n",
            "Total params: 666,185\n",
            "Trainable params: 666,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "Iteration 1\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/4\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "116475/116475 [==============================] - 77s 664us/step - loss: 3.3131\n",
            "Epoch 2/4\n",
            " 69632/116475 [================>.............] - ETA: 25s - loss: 2.7630"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG4ghc36nrvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#   Part 2)\n",
        "#\n",
        "#   - Pick up a book from Gutenberg (https://www.gutenberg.org/).\n",
        "#   - Extract tokens from the book. You will need to keep the Tokenizer map\n",
        "#     to generate the text\n",
        "#   - Use embeddings + glove as the first layer (https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
        "#   - Train a model to try to predict the next word of the book.\n",
        "#   - Be careful with starting-tokens and invalid-tokens.\n",
        "#   - Read a seed word.\n",
        "#   - Generate text based on the seed word.\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}